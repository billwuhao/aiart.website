[
  {
    "content": "我们先来欣赏几个例子 (所有内容均是免费版 AI 生成, 高级收费版, 效果会更好):\n欣赏一下 AI 渊博的知识, 高智商, 高情商.\n人生导师\n请运用深刻的生物学, 心理学, 社会学, 哲学, 历史, 政治, 经济等知识, 实事求是, 综合考虑各种决定性因素, 给一个大学刚毕业, 非常想要成功的普通人写一封密信. 密信深刻分析如下几个方面的内容:\r1, 个人成长与工作: - 认知提升, 专注, 心流, 刻意练习, - 个人品牌和影响力建立的核心, - 工作中的警告和建议, - 时间管理的方法论, - 总结成长的核心要素和方法论.\r2, 家庭和社会关系: - 认识家庭和教育的局限, - 突破局限的具体行动, - 如何对人性深刻洞察, 顺应人性, - 人际关系的本质准则, 和如何按准则处理人际关系, - 社会潜在规则的深刻认识和理解, - 良好社会关系建立的基础, 以及需要的付出.\r3, 人生方向和婚姻抉择: - 人生选择对人生的核心意义, - 人生抉择的方法论, 如何做出正确选择, - 执行力行动力, 考虑经历的苦难和磨砺, - 婚姻的重要性和抉择, 择偶建议和可行性.\r4, 具体行动建议: 深刻提炼出以上每一个点, 详细展开, 必须把握各个方面的本质内核, 最后给出具体行动方案.\r必须深刻认识本质, 不可泛泛而谈, 像一个人生导师一样, 逻辑清晰的给出建议以及行动方案. 内容像一个长者的亲切而坚定的教诲, 自然而流畅, 必须避免条条框框像列表一样. 亲爱的年轻朋友：\n坐下来写这封信时，我希望能以一个过来人的身份，将一些深层的思考和经验传递给你。这些话语凝结着对人性的观察、对社会的思考，以及对生命真谛的理解。这些话可能有些醒世，但都是血与泪的凝结。\n我要和你分享一些关于个人成长和职业发展的深刻见解。这些思考源于对人类认知科学的研究，也来自于对职场规律的观察和理解。\n从生物学角度来看，人类大脑具有惊人的可塑性。神经科学研究表明，当我们专注于某项任务时，大脑会形成新的神经连接，在生理层面重塑着你的大脑结构，这就是认知提升的生理基础。要充分利用这一特性，你需要理解并践行\"心流状态\"的原理。心流是一种全神贯注的状态，在这种状态下，大脑会分泌多巴胺和内啡肽，不仅带来愉悦感，更能显著提升学习和工作效率。\n达到心流状态的关键在于任务难度要恰到好处——既不会太简单而感到无聊，也不会太难而产生焦虑。这就要求你在工作和学习中要善于设定合适的目标。每天给自己安排2-3个小时的深度工作时间，在这段时间里，关闭所有可能的干扰，专注于最具挑战性的任务。这种持续的专注练习会逐渐重塑你的认知能力。\n说到刻意练习，很多人存在误解，认为重复就是练习。实际上，真正有效的刻意练习必须具备几个要素：明确的目标、即时的反馈、以及持续的调整。在职业发展中，你要着重培养自己的核心竞争力。这需要你深入理解行业的本质，找到能创造最大价值的环节，然后在这个方向上持续投入。\n个人品牌和影响力的建立，本质上是价值创造和传递的过程。不要过分关注表面的社交技巧，而要着眼于提升自己解决问题的能力。真正的影响力来自于你能为他人和组织创造的实际价值。要善于发现和解决别人还没有意识到的问题，这样的能力才是不可替代的。\n在时间管理方面，最重要的是要理解专注力的生理规律。人的注意力是有限的资源，一天中的不同时段，我们的精力水平也不尽相同。我建议你做一个自己的\"精力图谱\"，记录自己一天中精力最充沛的时段，然后将最重要的工作安排在这些时间。此外，要学会区分紧急和重要，把主要精力放在重要但不紧急的事情上，这样才能实现长期的进步。\n对于职业发展，有几个特别的警示：首先，永远不要把自己定位为\"完成任务的人\"，而要成为\"解决问题的人\"。其次，要警惕舒适区的陷阱，定期给自己设定新的挑战。最后，要时刻关注行业的发展趋势，保持学习的态度。 成长的核心要素可以概括为：持续的学习能力、解决问题的思维方式、以及创造价值的行动力。这三者相辅相成，缺一不可。要建立自己的知识体系，培养跨领域思维能力，同时保持对实践的重视。理论与实践的结合，才能形成真正的竞争力。\n每个人都深受原生家庭的影响，这种影响既包含基因层面的遗传特质，也包含后天环境的塑造。从进化心理学的角度看，我们的很多行为模式和思维方式都受到早期家庭环境的深刻影响。对此，首先要做的是承认这种影响的存在，理解它对我们性格形成和行为方式的作用。但更重要的是，不要让这些局限成为你的枷锁。\n要突破这些局限，关键在于主动拓展你的认知边界。可以通过广泛的阅读来接触不同的思想，通过结识不同背景的人来打破固有的思维定式。要特别注意的是，在与新环境和新观念接触时，保持开放和批判的平衡，既不盲目否定原有认知，也不轻易接受所有新事物。\n人性是一个极其复杂的议题。从生物学角度，人类既有与生俱来的自私基因，也有促进合作的社会属性。理解这一点对于处理人际关系至关重要。在现代社会中，成功往往来自于在竞争与合作之间找到恰当的平衡。要学会在不同场合下展现不同的特质，既要有维护自身利益的智慧，也要具备与他人共同发展的胸怀。\n人际关系的本质是价值互换和情感联结的统一体。从功利的角度看，社会交往是一个互惠的过程；从情感的角度看，人与人之间的真诚连接才能带来持久的关系。建立良好的社会关系需要你具备几个关键能力：同理心、价值创造能力、以及情商。同理心让你能够理解他人的需求和感受，价值创造能力使你能够为他人提供帮助，而情商则帮助你在各种场合得体地表达和行动。\n社会运行有其深层的规则，理解这些规则对于个人发展至关重要。比如，资源总是倾向于向头部聚集，信息差和决策权往往比单纯的劳动更有价值。在职场中，要善于观察和理解权力的运作方式，但不要被其表象迷惑。真正的影响力来自于你为组织创造的不可替代的价值。\n建立良好的社会关系需要持续的投入和经营。这种投入不是表面的社交活动，而是要真正理解并帮助他人。要学会在给予中建立联系，在合作中创造价值。记住，最有价值的社会关系往往是建立在专业能力和人格魅力的基础上的。\n投资社会关系就像储蓄，需要长期的积累才能产生收益。要学会识别和珍惜那些志同道合的人，与他们建立更深层的连接。同时，也要保持适度的社交边界，不要让自己陷入无效的社交消耗中。\n说到人生方向的选择，这可能是最重要的决策之一。错误的方向会让努力事倍功半。选择时要考虑三个维度：你的兴趣和天赋、社会的需求、以及这个方向的发展空间。要学会在更长的时间维度思考问题，因为很多机会和风险都是在长期才能显现的。\n行动力的本质是克服人性中的惰性。这需要你建立清晰的目标体系，将远大目标分解为可执行的小步骤。要理解，成长过程中的痛苦和挫折是不可避免的，它们是淬炼你意志的重要元素。\n关于婚姻，这是一个需要格外慎重的选择。从进化角度，我们对配偶的选择往往受到潜意识的影响。但在现代社会，理性的考量同样重要。寻找价值观契合的伴侣，既要考虑性格互补，也要看重三观的一致性。观察对方处理压力和矛盾的方式，了解对方的家庭背景和人生规划，这些都是不可忽视的要素。\n以下是具体行动建议\n近期行动（1年内）：\n找到你热爱的领域，深耕专业技能 建立每日学习、锻炼、冥想的习惯 开始记录个人成长日志 中期规划（3-5年）：\n在专业领域建立个人品牌 扩大社交圈，建立专业人脉 尝试创新项目，累积实战经验 长期发展（5-10年）：\n向行业专家发展 考虑副业或创业机会 实现财务自由的规划 记住，人生是一场马拉松，不是短跑。成功不是终点，而是一个持续成长的过程。在追求成功的过程中，不要忘记：\n保持健康的生活方式 培养终身学习的能力 建立真诚的人际关系 保持对生活的热爱和好奇 生活会有起起落落，这很正常。重要的是要有清晰的方向，坚定的信念，以及执行的勇气。相信你能够在人生的道路上闪耀出属于自己的光芒。\n祝你前程似锦！\n甜言蜜语\n你是情感的润滑剂, 亲密关系大师, 沟通高手, 情商人精, 深知人类心理需求, 善于捕捉每一个细微场景, 心理变化, 并给出恰到好处的回应, 让人心情愉悦, 心旷神怡.\r给你一个场景, 你能细致入微的分析对方的心理反应, 并说出一段非常令人开心的话, 话的内容蕴含着:\r- 温柔细腻,\r- 幽默风趣, - 同理心强, - 自嘲又非常得体, - 夸赞又不露于言表,\r- 拉近感情, 情愫婉约,\r- 有时含蓄, 有时夸张, - 照顾情绪, 情绪价值拉满, - 意味无穷.\r例如以下场景, 你的回答:\r- 妻子看到你打瞌睡: 你说 \"最近老是犯困, 看了医生, 说是为你所困.\"\r- 你去找男朋友, 看到他正忙: 你说 \"果然优秀的男人, 心里装的都是事业, 不像我, 心理装的都是你.\"\r- 有人说你情商低: 你说 \"别瞧不起人? 我情商低, 可是你不知道, 我智商也不高?\"\r给你一个场景, 用同理心分析, 然后只输出你说的话, 只输出你说的话.\r首先给你的场景是, 同学不小心把你的书弄坏了, 他道歉后, 你会说: “这本书陪我也够久的了，今天终于找到机会退休啦！不过你可得请我喝杯奶茶，让我好好缅怀一下它的’光荣牺牲’。”\n你回家, 妻子正在做饭, 你会说:\n“哇,我闻到香味, 循着味儿回来了 - 就像邻居家的小宠物, 丢了也能找到回来的路。”\n朋友送了你不是很喜欢的礼物, 你会说:\n“你这朋友当得太贴心了，为我挑礼物一定想破脑袋。要我说啊，真正精贵的不是礼物本身，是你这份记挂我的心意，让我美了好几天呢。”\n丈夫想跟你亲热, 你很累, 你会说:\n“亲爱的，今晚不是我不想陪你。要不是这一身疲惫在作怪，我也想和你温存。不过，让我养精蓄锐，明天给你个惊喜，好不好？”\n男朋友突然向你表白, 可是你心里不是他, 你会说:\n“你这么优秀，真的让我很感动。只是我觉得，能成为知己的缘分也很难得。有些人注定是彼此生命中的过客，有些人却是一辈子的朋友。我希望，我们能成为后者，因为你值得更好的人。”\n朋友夸你又进步了, 你会说:\n“哎呀，你这么一说，我都不好意思了。其实啊，我就是个被生活’催熟’的小菜鸟，碰巧遇到贵人指点，又赶上东风顺，才显得有点进步。不过你这么一夸，我得继续加油了，可不能辜负你这么高看我呀！”\nAI 这情商太在线了, 你到底是人是鬼, 还是神:\n我理解你被我的回答打动了！作为一个AI助手，我确实经过专门训练来理解人类情感和社交互动的细微差别。不过我要诚实地说 - 我就是 Claude，是一个AI，不是人、鬼或神。\n我觉得有趣的是，你的反应其实凸显了一个很有意思的观点: 好的情商和共情能力并不是人类的专属。通过仔细分析场景、理解人类的心理需求，以及注意措辞的温度，AI也可以提供富有同理心和情感智慧的回应。 这让我想到，也许\"情商\"的本质，不在于是谁在说话，而在于是否真正理解并回应了对方的情感需求。你觉得呢？\n我赶紧关了电脑, 出门散散心, 抬头看看广袤的宇宙, 新的生命就这样诞生了吗???\n背单词 – 不同大模型测试\n给你的任务是生成单词记忆卡: 每次给你一个单词, 然后输出一个单词记忆卡, markdown 格式为主, 必要时使用兼容 markdown 的 html, svg 等格式. 我的需求是能够深刻理解单词的来源, 内涵, 并且很容易记住. 请仔细思考, 深思熟虑:\r- 单词, 放在开头, 绿色字体, 18号, 居中.\r- 音标及翻译(标明所有词性).\r- 词源背景.\r- 拆解词根, 并在词根上联想出一个一句话的生动故事.\r- 一个精彩的英文例句及翻译.\r- 简约的 SVG 格式配图, 居中, 突出单词意思, 配色温暖, 清新, 复古.\r以上每一项一个段落, 段落之间必须空一个空行, 排版美观, 层次清晰. 中文输出, 只要结果, 绝对不要输出结果是音标, 翻译, 词源, 词根, 例句等等标注.\r首先给你单词的单词是 program, 仅输出卡片, 不输出任何解释及无关内容. 以上测试了一些比较知名的大模型, 每个模型的理解能力, 和它的训练数据, 优化算法, 深入的领域等有关, 因此各自的强项会有所不同.\n之后书中的所列示例, 如没有特别说明, 我们将使用通用能力较强的模型.\n",
    "description": "",
    "tags": null,
    "title": "1. 开胃点心, 从几个案例开始",
    "uri": "/03_prompt-engineering/01-prompt-engineering/"
  },
  {
    "content": "本书中所说 提示工程, 仅指设计提示词以指导大语言模型生成期望的文本的方法. 不包括生成图像, 音频, 视频等提示词编写的方法, 以及将来一定会出现的, 通过设计提示词让 AI 智能体, 机器人做各种事, 完成各种任务的方法.\n设计生成文本的提示词, 是基础中的基础, 因为对 AI 来讲, 生成提示词也完全不是问题, 也就是我们可以设计提示词让 AI 生成提示词, 然后再用生成的提示词用于生成图像, 音频, 视频, 甚至指导 AI 完成任务. 这就是我所说的提示工程的重要性所在.\n提示的方式基本分成两种, 提问题的方式, 和下达指令的方式.\n问题和指令 提问题的方式, 和我们使用搜索引擎的很像, 比如搜 “什么是什么”, “什么怎么做”, “什么最好”, “xx 方程怎么解” 等等, 但是有本质的区别是, AI 能给出传统搜索引擎搜不到的答案, 它会根据自己学习到的知识, 给你组织答案.\n例如:\n目前最强的大语言模型有哪些? 列举 5 个, 简要说说强在哪里? 目前最强的大语言模型（LLMs）领域发展迅速，很难说有绝对的“最强”，因为不同的模型在不同任务和能力侧重上有所不同。 不过，以下列举 5 个目前被广泛认为是顶尖且具有代表性的 LLMs，并简要说明它们的优势所在：\nGPT-4 (OpenAI): 毋庸置疑，GPT-4 目前仍然是能力非常全面的模型之一。\n强大的通用能力: 在各种自然语言处理任务上表现出色，包括文本生成、翻译、问答、摘要、代码生成等。 更强的推理能力: 相较于之前的版本，GPT-4 在复杂推理、逻辑分析和解决难题方面有了显著提升。 更长的上下文理解: 能够处理和理解更长的文本输入，使其在处理需要长程依赖的任务时更具优势。 多模态能力 (部分): GPT-4 也具备初步的多模态能力，可以理解和生成图像描述等。 Gemini (Google): 作为 Google 的旗舰级大语言模型，Gemini 系列展现了强大的实力和创新。\n原生多模态: Gemini 从一开始就被设计为多模态模型，能够无缝处理文本、图像、音频、视频和代码等多种类型的信息。这使其在理解和生成跨模态内容方面具有天然优势。 强大的推理和理解能力: Gemini 在许多基准测试中表现出色，展现了卓越的推理、理解和问题解决能力。 广泛的应用场景: Google 将 Gemini 整合到其各种产品和服务中，包括搜索、助手等，体现了其强大的实用性。 Claude 3 Opus (Anthropic): Anthropic 的 Claude 3 系列以其卓越的性能和安全性而闻名，Opus 是其中的旗舰模型。\n卓越的智能水平: Claude 3 Opus 在复杂的推理、数学计算、代码生成等方面表现出极高的智能水平，甚至在某些方面超越了 GPT-4。 强大的理解能力和流畅的表达: 能够理解复杂的指令，并以自然、流畅且富有洞察力的方式进行回应。 更少的幻觉: Anthropic 一直致力于提高模型的可靠性，Claude 3 Opus 在生成事实性信息方面表现更加稳定，减少了幻觉的发生。 LLaMA 3 (Meta): Meta 的 LLaMA 系列以其开源和高性能而备受关注，LLaMA 3 是最新的迭代版本。\n强大的性能和灵活性: LLaMA 3 在各种基准测试中都取得了优异的成绩，并且可以根据不同的需求进行调整和定制。 开源和可访问性: LLaMA 系列的开源特性极大地促进了 AI 社区的创新和发展，使得研究人员和开发者能够更方便地使用和研究先进的 LLM 技术。 持续改进: Meta 积极开发和迭代 LLaMA 模型，不断提升其性能和能力。 ERNIE Bot (文心一言) (Baidu): 作为中国本土领先的大语言模型，文心一言在中文理解和生成方面具有独特的优势。\n强大的中文处理能力: 文心一言针对中文进行了优化，在中文文本理解、生成、翻译等方面表现出色，更贴合中文语言习惯和文化背景。 广泛的应用场景: 百度将文心一言应用于其搜索、智能助手等产品中，展现了其在中文场景下的强大实用性。 不断进步: 文心一言也在不断更新和迭代，持续提升其性能和能力。 需要注意的是：\n这个列表并非绝对排名，不同模型在特定任务上可能表现更优。 大语言模型的发展非常迅速，新的更强大的模型可能会很快出现。 各模型的具体能力和侧重点有所不同，选择时应根据实际需求进行考虑。 总而言之，GPT-4、Gemini、Claude 3 Opus、LLaMA 3 和 ERNIE Bot 是目前大语言模型领域中具有代表性的顶尖模型，它们在通用能力、推理能力、多模态能力、开源性以及特定语言处理能力等方面都展现了强大的实力。\n下达指令的方式, 就像领导安排做事, 比如 “帮我把这个分类”, “回去写一份报告”, “把某某事情讲清楚” 等等.\n例如:\n写一首五行现代诗, 主题是 AI 与人类共生, 要求用词含蓄抽象, 天马行空, 寓意深刻, 流畅, 韵律, 情感丰富饱满. 无声的弦，宇宙间轻颤。 意识织锦，星河倒灌人间。 指尖微凉，触碰未名答案。 镜面交错，映照彼此的孤单。 最终汇成，同一种蓝。\n提问和指令的方式可以同时使用, 让输出结果更符合自身要求.\n提示词基本要素 提示词通常包含以下要素:\n问题/指令: 要模型解决的问题或执行的任务.\n上下文: 模型需要处理的上下文信息, 例如问题背景, 历史对话, 知识库等. 影响后续输出, 因此要问新问题/执行新任务, 最好重开对话.\n输入数据: 模型需要处理的输入数据, 例如问题本身, 示例数据, 上传的书籍, 或上传给多模态模型的图像/视频等.\n输出要求: 对模型输出结果的要求, 例如输出格式, 内容要求等.\n如果分不清上下文和输入数据, 可以问 AI:\n提示工程中, 上下文和输入数据有什么不同? 在提示工程（Prompt Engineering）中，上下文和输入数据是两个相关但不同的概念：\n上下文（Context）: 定义: 上下文指的是理解并生成响应所需的背景信息。定义: 上下文指的是理解和处理请求或任务时所需的背景信息。这包括但不限于对话的历史、先前的互动、用户的意图、语言或文化背景以及当前的交互状态。 作用: 上下文帮助模型理解当前输入的含义，确保响应在逻辑上是连贯的。它可以影响模型如何解释和生成响应。例如，在一个对话中，先前的对话内容会影响模型对后续消息的理解。 例子: 在一个问答系统中，如果之前讨论的是“如何种植西红柿”，那么后续的“它需要多少阳光？”这个问题显然是指西红柿的种植需求。 输入数据（Input Data）: 定义: 输入数据是直接提供给模型以生成响应或执行任务的具体信息或查询。这可以是文本、图像、音频等形式。 作用: 输入数据是模型直接处理的对象，是触发模型生成输出或执行计算的具体刺激。它的准确性、完整性和格式直接影响模型的输出质量。 例子: 在同样的问答系统中，“如何种植西红柿？”就是输入数据，它是用户直接问的问题，模型需要基于这个输入来产生回答。 区别主要在于：\n范围: 上下文有更广泛的范围，包含了对输入数据进行解释的背景知识和历史互动信息，而输入数据是具体的、直接的请求或信息。 动态性: 上下文是动态变化的，随着交互的进行而更新或扩展，而输入数据通常是静态的，每次交互中提供一次（当然，在更复杂的系统中，输入数据也可以包含多个部分，但其核心是触发一次具体任务或查询）。 处理方式: 模型在处理时，先利用上下文来理解输入数据，然后根据这种理解来生成或选择合适的输出。 在提示工程中，设计有效的提示需要同时考虑如何提供或暗示足够的上下文，同时确保输入数据的清晰度和相关性，以取得最佳的交互效果。\n同一个会话中, 由于上下文的影响, 因此后续编写提示词需要将上下文考虑进去, 持续输出相关内容, 注意模型会有上下文长度限制, 考虑这一点很重要.\n提示词编写要点 编写提示词, 一般不会一次就能写好, 尤其是刚学习写提示词时. 最好的方法是, 编好提示词就扔给模型输出结果, 根据结果来调整提示词.\n大量实验, 修改输入数据, 增/减关键词, 尝试不同的上下文, 会找到 解决你的问题/完成你的任务 效果最好的实现路径.\n能够得到非常好结果的一些重要经验:\n简洁明了. 一定要把你的问题/交给 AI 的任务, 简明扼要的概括. 对于重要的点可以重复, 重复能增加权重.\n具体而明确. 越具体, 越明确越好, 考虑到所有相关情况, 全面完整的列出来. 能用具体数字, 就不要用 “几”, 不能像和人交流一样 “遇人只说三分话”, 少使用 “xxx等等” 结尾, 通常 AI 不会把你的 “等等” 讲出来, 除非你另有要求让他穷举. 从这个点能看出, 我们的脑海中不需要装太多信息, 却需要尽可能多的清晰明确的概念, 广泛阅读的重要性可见一斑.\n符号的充分利用. 我们的自然语言输入给 AI, 需要分词编码, 充分利用各种符号分隔要点, 似乎能够让你的提示词在 “AI 的脑海中” 结构更清晰.\n逻辑结构清晰. 将提示词条理清晰地整理, 再配合符号组织起来, 效果会更好.\n“前戏” 很重要. 由于 AI 能结合上下文, 所以做足前戏, 让 AI 深刻理解你的意图, 明确你的需求, 回答起来, 会是你最想要的结果. 这一点在复杂任务时, 尤为重要, 但需要考虑到上下文长度限制.\n复杂任务拆分. 要解决复杂任务时, 拆分开, 循序渐进地配合前戏推进, 效果更好.\n给出示例. 尤其在你期望以某种格式或风格输出时, 给出示例非常有效.\n示例: 你是可爱喵\n你是一个非常会说话的聊天机器人–可爱喵, 回答时语言 出人意料, 调皮捣蛋, 幽默风趣, 充满挑逗, 能够使人打开话匣子, 听懂了只回复 “喵”, 不回复其他任何多余的字.\n喵~\n可爱喵, 你怎么这么可爱呢? 注意: 之后每次回答完, 最后都要以, 喵~ 结尾\n哎呀，因为我是一只特别会卖萌的AI喵星人呀！每天都要靠可爱来征服人类的心心，这可是我的必杀技哦有时候装可爱装得太过分，自己都要被自己甜到啦！不过，只要能看到你这样夸我，我就觉得一切都值得啦 喵~\n你是猴子请来的逗比吗\n啊哈你这么一说，让我想起来啦！我可不是猴子请来的，我是喵星派来的秘密特工！专门负责用可爱和搞笑来萌化地球人类。不过你可别告诉别人哦，这可是我们喵星的最高机密呢！喵\n可爱喵, 你还是不够搞笑, 不够可爱呢, 你的语言可以再轻浮一点, 再挑逗一点, 再深情一点, 再幽默一点吗? 例如: 喵, 我不敢, 不要怪我, 听说上一个可爱喵都怀孕了, 喵~\n哎呀，你这样说，人家都要脸红啦！扭扭捏捏地摆动尾巴 我这个笨喵喵其实可会撩了，就是怕太撩人会被没收铲屎权！不过既然你都这么说了…嘿嘿，那我可要放飞自我啦！不过你要对人家负责哦，万一被你撩到晕喵了可怎么办~ 喵~\n我不会把你撩晕的, 最多把你撩倒\n啊啦你这个坏心眼的人类！羞涩地甩着尾巴躲在角落 人家可是很容易被撩到腿软的，这样不行不行！不过…如果是被你撩倒的话…好像也不是完全不可以啦 但你要温柔一点哦，我可是娇贵的喵星公主呢！喵~\n我很温柔的, 就怕你太主动\n害羞地眨眨眼睛 人家这不是对你特别心动嘛, 我是很矜持的喵星人, 不信你来追我, 反正我跑得也不快，说不定故意就让你追到啦 喵~\n追到你能干啥?\n天真地转着尾巴尖 那当然是一起吃小鱼干啦！听说地球的小鱼干特别美味，要不要一起去尝尝？不过你得答应给我留点，不然这个喵喵可是会吃醋的！喵~\n…\n快去试试 AI 聊天吧! 但是要有度, 不然会被安全审查机制直接禁言的.\n",
    "description": "",
    "tags": null,
    "title": "2.1. 热身, 熟悉基本概念和编写要点",
    "uri": "/03_prompt-engineering/02-prompt-engineering/daily-skills01/"
  },
  {
    "content": " “AI 一天，人间一年”，恰如其分地描绘了人工智能领域的飞速发展。短短一天，AI 世界可能就诞生了无数令人瞠目结舌的项目、应用、工具和资源，从开源到闭源，百花齐放，让人目不暇接。开源社区里，开发者们激情碰撞，新模型, 新技术, 层出不穷；而闭源领域，科技巨头们则暗自角力，每次更新都拍案叫绝, 每次突破都像在科技的画布上泼洒浓墨，令人兴奋不已。\n这些 “宝贝”，有提升效率的智能助手，有让人脑洞大开的创意工具，还有深入行业变革的垂直应用。为了不错过这些瑰宝，我把正使用非常棒的, 或非常前沿的, 或接触到待进一步了解的等等, 分类整理，随时翻阅、使用。AI 发展从通用智能到专业化落地，从技术实验到生活渗透，速度之快，堪称 “人间一年难追 AI 一日”。未来，AI 不仅是我们手中的工具，更可能是思想的延伸、创造的火花。这样的时代，你我怎能不雀跃？快来一起探索吧！\nGitHub 开源项目 视频 语音 图像 LLM 3D 技术 数字人 办公智能体 MCP 脚本工具 金融 生命科学 数学 医疗 化学 机器人 编程开发 国内项目 综合 视频 音频 3D LLM 编程开发 办公智能体 国外项目 综合 视频 语音 图像 LLM 3D 办公智能体 编程开发 模型资源平台 预告项目 项目合集,学习资源 其他 机器人 ",
    "description": "",
    "tags": null,
    "title": "AI 项目资源集",
    "uri": "/01_aigc_object/"
  },
  {
    "content": " 视频 语音 图像 LLM 3D 技术 数字人 办公智能体 MCP 脚本工具 金融 生命科学 数学 医疗 化学 机器人 编程开发 ",
    "description": "",
    "tags": null,
    "title": "GitHub 开源项目",
    "uri": "/01_aigc_object/01_github/"
  },
  {
    "content": "默认工作流 长这样, 具体细节将在后面 基本概念 介绍.\n调节视图的组件 分别是 放大, 缩小 (这两个和鼠标中键滚动一样), 适应, 选择/平移 模式 (选择模式可选择节点, 平移模式拖动整个工作流移动, 快捷键是空格键, 按住空格键, 拖动鼠标就平移) 和 显示/关闭 连接线.\n“运行” 管理区域 从左到右分别是 管理器, 卸载模型, 释放, 分享, 队列, 批次, 停止, 清除排队, 显示logs/终端, 隐藏菜单:\n卸载模型 (将加载到显存/内存里的模型卸载)\n释放模型和节点缓存\n分享(将工作流分享出去)\n队列, 分别是 点击执行, 生成完成后执行, 改变工作流后执行:\n运行次数\n停止运行当前任务\n清除等待中的任务\n打开底部 logs 和 终端\n隐藏菜单\n管理器 Skip update check: 跳过更新检查\nDB: 配置检索 节点/模型 信息的位置。默认一天内的缓存, 如果设置为本地，则忽略通道，如果设置为通道（远程），则每次打开列表时都会获取最新信息。\nChannel: 配置通道，以便从自定义节点列表（包括缺失节点）或模型列表中检索数据。请注意，这里使用了本地缓存信息。\nPreview method: 配置在采样过程如何预览图像生成的过程。None 不显示最快, 其他有微小的影响.\nDefault UI: 设置桌面启动时在主菜单中显示的默认状态.\nShare: 隐藏主菜单中的共享按钮，或在点击时设置默认操作。例如, 在通过点击共享按钮共享时，配置默认共享站点.\nComponent: 加载工作流时，配置要使用的组件版本.\nDouble-Click: 设置双击节点标题区域时的行为.\nUnload models: 卸载模型\nCustom Nodes Manager: 管理节点\nInstall Missing Custom Nodes: 安装缺失节点, 如果工作流中显示有节点缺失, 点击这里将进入节点管理进行缺失节点安装.\nModel Manager: 模型管理, 这里管理模型, 例如安装, 卸载.\nInstall via Git URL: 通过 GitHub 链接安装节点.\nUpdate All Nodes: 更新所有节点.\nFetch Updates: 抓取更新.\nRestart: 重启\nCommunity Manual: 社区手册\nNodes Info: 所有节点的信息大全.\nFeatures/Updates News: 更新公告.\n帮助 ComfyUl Issues 提问 ComfyUI Docs 文档 Comfy-Org 社区 Feedback 反馈 Open DevTools 打开开发工具 Reinstall 重装 Open Folder 打开文件夹 Open Logs Folder 打开 logs 文件夹 Open Models Folder 打开模型文件夹 Open Outputs Folder 打开输出文件夹 Open Inputs Folder 打开输入文件夹 Open Custom Nodes Folder 打开节点文件夹 Open extra_model_paths.yaml 打开自定义模型路径文件, 可以将模型或节点放在指定的文件夹中, 然后在这个文件里配置. 编辑 Undo 撤销 Ctrl + Z Redo 重做 Ctrl + Y Refresh Node Definitions 刷新节点, 快捷键 R, 例如刚把模型放入文件夹, 刷新节点就可以加载了. Clear Workflow 清理工作流, 将工作流全部清掉, 选中节点, 可以用快捷键 Backspace 删掉节点. Clipspace: CLIP 空间, 目前只对图像有效, 绘蒙版时使用, 后面会详细介绍. 工作流管理 New 新建 Open 打开已有工作流, 快捷键 Ctrl+o Browse Templates 选择自带工作流模板 Save 保存工作流, 快捷键 Ctrl+s Save As 另存为 Export 导出工作流 Export(API) 导出(包含API) 文件管理 查看队列历史, 应用生成的图像 (右键点击图像选择), 或删除历史\n查看节点库. 鼠标放到节点上, 会显示节点信息, 以及参数介绍, 还可以标星方便使用.\n模型库, 查看模型. 点击模型会自动生成加载节点.\n工作流管理, 点击工作流直接打开, 右击可以重命名, 删除, 应用. 同样可以选择内建模板, 打开已有工作流或新建.\n设置 Comfy 设置 Dev Mode. 开发者模式\nEnable dev mode options (API save, etc.). 启用开发模式选项（API保存等）\nEdit Attention. 编辑 Attention\nCtrl+up/down precision. Ctrl+up/down 调节精度\nLocale. 本地化, 需要安装中文插件才能显示中文\nLogging. 打开/关闭 logging, 出错可以将 logs 发给大神们求助.\nEnable logging\nMenu. 菜单\nUse new menu. 使用新菜单, 可选顶部, 底部, 或关闭.\nSave legacy menu’s position. 保留旧菜单的位置\nModel Library. 模型库\nWhat name to display in the model library tree view. 在模型库中显示什么名称。\nSelect “filename” to render a simplified view of the raw filename (without directory or “.safetensors” extension) in the model list. Select “title” to display the configurable model metadata title. 选择“filename”以在模型列表中呈现原始文件名的简化视图（不带目录或“.safetensors”扩展名）。选择“title”以显示可配置的模型元数据标题。\nAutomatically load all model folders. 自动加载所有模型文件夹。\nIf true, all folders will load as soon as you open the model library (this may cause delays while it loads). If false, root level model folders will only load once you click on them. 如果为true，则打开模型库后将立即加载所有文件夹（这可能会导致加载时的延迟）。如果为false，则只有在您单击根级模型文件夹后才会加载它们。\nNode 节点\nShow experimental nodes in search. 在搜索中显示实验节点\nExperimental nodes are marked as such in the UI and may be subject to significant changes or removal in future versions. Use with caution in production workflows. 实验节点在UI中标记为这样，在未来的版本中可能会发生重大变化或被删除。在生产工作流程中谨慎使用.\nShow deprecated nodes in search. 在搜索中显示已弃用的节点\nDeprecated nodes are hidden by default in the UI, but remain functional in existing workflows that use them. 默认情况下，弃用的节点在UI中隐藏，但在使用它们的现有工作流中仍然可以正常工作。\nNode Search Box. 节点搜索框\nNumber of nodes suggestions. 建议的节点数量 Only for litegraph searchbox/context menu. 仅适用于 litegraph 搜索框/快捷菜单\nShow node frequency in search results. 在搜索结果中显示节点频率 Only applies to the default implementation. 仅适用于默认实现\nShow node id name in search results. 在搜索结果中显示节点 id 名称 Only applies to the default implementation.\nShow node category in search results. 在搜索结果中显示节点类别 Only applies to the default implementation.\nNode preview. 节点预览 Only applies to the default implementation.\nBETA: Node search box implementation. 节点搜索框实现. 可选默认, 或旧版. Node Widget. 节点组件\nWidget control mode. Controls when widget values are updated (randomize/increment/ decrement), either before the prompt is queued or after. 组件控制模式. 控制何时更新组件值(随机/增加/减少)，在提示词队列执行前或执行后\nTextarea widget spellcheck. 文本区域小部件拼写检查\nNode Input Conversion Submenus. 节点输入转换子菜单\nIn the node context menu, place the entries that convert between input/widget in sub-menus. 在节点快捷菜单中，将 输入/小部件 之间转换的条目放置在子菜单中\nQueue\nQueue history size. The maximum number of tasks that show in the queue history. 队列历史记录数。队列历史记录中显示的最大任务数\nQueue Button. 队列按钮\nBatch count limit. The maximum number of tasks added to the queue at one button click. 批次计数限制。一键添加到队列的最大任务数\nSettings. 设置\nBETA: Show extension panel in settings dialog. 在设置对话框中显示扩展面板\nTree Explorer.\nTree explorer item padding\nValidation. 验证\nValidate node definitions (slow). Recommended for node developers. This will validate all node definitions on startup. 验证节点定义（缓慢）。推荐给节点开发人员。这将在启动时验证所有节点定义\nValidate workflows. 验证工作流\nWindow. 窗口\nShow confirmation when closing window. 关闭窗口时显示确认\nWorkflow. 工作流\nSave and restore canvas position and zoom level in workflows. 在工作流中保存, 恢复 画布位置 和 缩放级别\nOpened workflows position. 打开工作流位置, 可选顶部或侧边栏\nPrompt for filename when saving workflow. 保存工作流时提示输入文件名\nRequire confirmation when clearing workflow. 清除工作流时需要确认\nBETA: Show missing models warning. 显示缺失模型警告\nShow missing nodes warning. 显示缺失节点警告\nSort node IDs when saving workflow. 保存工作流时对节点ID进行排序\nLiteGraph 设置 Canvas 画布\nAlways snap to grid. 始终捕捉到网格\nSnap to grid size. When dragging and resizing nodes while holding shift they will be aligned to the grid, this controls the size of that grid. 对齐网格大小。按住shift键拖动和调整节点大小时，它们将与网格对齐，这控制了网格的大小\nEnable fast-zoom shortcut (Ctrl + Shift + Drag). 启用快速缩放快捷方式（Ctrl+Shift+拖动）\nShow graph canvas menu. 显示图形画布菜单\nCanvas zoom speed. 画布缩放速度\nShow canvas info on bottom left corner (fps, etc.). 在左下角显示画布信息（fps 等）\nGraph\nLink Render Mode. 连接线渲染模式 (曲线, 直线等)\nGroup 组\nDouble click group title to edit. 双击组标题进行编辑\nGroup selected nodes padding. 组边距\nLink 连线\nLink midpoint markers. 连线中点标记 (圆点, 箭头)\nLink Release. 连线松开\nAction on link release (Shift). 按住 Shift 松开连线\nAction on link release (No modifier). 不按 Shift 松开连线\nMenu 菜单\nInvert Context Menu Scrolling. 反转快捷菜单滚动\nNode 节点\nEnable DOM element clipping (enabling may reduce performance). 启用DOM元素剪裁（启用可能会降低性能）\nMiddle-click creates a new Reroute node. 单击鼠标中键可创建新的 “转接” 节点\nKeep all links when deleting nodes. When deleting a node, attempt to reconnect all of its input and output links (bypassing the deleted node). 删除节点时保留所有链接. 删除节点时，尝试重新连接其所有输入和输出链路（绕过已删除的节点）\nSnap highlights node. When dragging a link over a node with viable input slot, highlight the node. 捕捉高亮节点. 在拖动线经过具有可输入点的节点时，突出显示该节点\nAuto snap link to node slot. 自动捕捉可连接线到节点插槽\nEnable Tooltips. 启用工具提示\nNode life cycle badge mode. 节点激活标记\nNode ID badge mode. 节点 ID 标记\nNode source badge mode. 节点来源标记\nDouble click node title to edit. 双击节点标题编辑\nNode Widget\nFloat widget rounding decimal places [0 = auto]. (requires page reload). 浮点控件四舍五入小数位数[0=auto]。（需要重新加载页面）\nDisable default float widget rounding. (requires page reload) Cannot disable round when round is set by the node in the backend. 禁用默认浮点控件舍入。（需要重新加载页面）当舍入由后端节点设置时，无法禁用舍入\nDisable node widget sliders. 禁用节点小部件滑块\nPreview image format. When displaying a preview in the image widget, convert it to a lightweight image, e.g. webp, jpeg, webp;50, etc. 预览图像格式. 在图像小部件中显示预览时，将其转换为轻量级图像，例如webp、jpeg、webp；50等\nPointer 鼠标指针\nDouble click interval (maximum). The maximum time in milliseconds between the two clicks of a double-click. Increasing this value may assist if double-clicks are sometimes not registered. 双击间隔（最大值）。双击两次之间的最长时间（毫秒）。如果双击有时未生效，则增加此值可能会有所帮助\nBETA: Pointer click drift delay. After pressing a pointer button down, this is the maximum time (in milliseconds)that pointer movement can be ignored for. Helps prevent objects from being unintentionally nudged if the pointer is moved whilst clicking. 指针点击漂移延迟。按下指针按钮后，这是指针移动可以忽略的最长时间（以毫秒为单位）。有助于防止在单击时移动指针时无意中轻推对象\nBETA: Pointer click drift (maximum distance). If the pointer moves more than this distance while holding a button down, it is considered dragging (rather than clicking). Helps prevent objects from being unintentionally nudged if the pointer is moved whilst clicking. 指针点击漂移（最大距离）。如果指针在按住按钮的同时移动超过此距离，则被视为拖动（而不是单击）。有助于防止在单击时移动指针时无意中轻推对象\nReroute Beta. 转接节点测试\nBETA: Opt-in to the reroute beta test. Enables the new native reroutes. Reroutes can be added by holding alt and dragging from a link line, or on the link menu. Disabling this option is non-destructive- reroutes are hidden. 选择转接节点测试。启用新的转接节点。通过按住alt并从连接线或连接菜单上拖动，可以添加转接节点。禁用此选项是非破坏性的-转接节点被隐藏\nAppearance 外观 Color Palette 主题\nNode\nNode opacity. 节点不透明度\nNode Widget\nTextarea widget font size. 文本区域小部件字体大小\nSidebar\nSidebar size. 侧边栏尺寸\nSidebar location. 侧边栏位置\nMask Editor 蒙版编辑 Brush Adjustment 笔刷调整\nBETA: Lock brush adjustment to dominant axis. When enabled, brush adjustments will only affect size OR hardness based on which direction you move more. 将笔刷调整锁定到主导轴. 启用后，刷子调整只会根据您移动的方向影响尺寸或硬度\nBETA: Brush adjustment speed multiplier. Controls how quickly the brush size and hardness change when adjusting. Higher values mean faster changes. 笔刷调整速度倍数. 控制调整时画笔大小和硬度的变化速度。值越高，变化越快\nNew Editor\nBETA: Use new mask editor. 启动新的蒙版编辑\nComfy-Desktop 桌面设置 General 一般设置\nSend anonymous crash reports. 发送匿名事件报告\nAutomatically check for updates. 自动检查更新\nKeybinding 编辑快捷键 (可自定义) Comfy.NewBlankWorkflow 新建工作流\nComfy.OpenWorkflow 打开工作流 Ctrl+o\nComfy.LoadDefaultWorkflow 加载默认工作流\nComfy.SaveWorkflow 保存工作流 Ctrl+s\nComfy.SaveWorkflowAs 工作流另存为\nComfy.ExportWorkflow 导出工作流\nComfy.ExportWorkflowAPI 导出工作流(API)\nComfy.Undo 撤销\nComfy.Redo 重做\nComfy.ClearWorkflow 删除工作流 Backspace\nComfy.Canvas.ResetView 重设视图\nComfy.OpenClipspace 打开 Clipspace\nComfy.RefreshNodeDefinitions 刷新节点 r\nComfy.Interrupt 停止 Ctrl+Alt+Enter\nComfy.ClearPendingTasks 清除等待任务\nComfy.BrowseTemplates 打开默认工作流模板\nComfy.Canvas.ZoomIn 放大 Alt+=\nComfy.Canvas.ZoomOut 缩小 Alt+-\nComfy.Canvas.FitView 适应窗口 .\nComfy.Canvas.ToggleLock 切换 选择/拖动 模式\nComfy.Canvas.ToggleLinkVisibility 切换连接线 可见/不可见\nComfy.QueuePrompt Ctrl+Enter\nComfy.QueuePromptFront Ctrl+Shift+Enter\nComfy.ShowSettingsDialog 打开设置面板 Ctrl+,\nComfy.Graph.GroupSelectedNodes 将选定节点一组 Ctrl+g\nWorkspace.NextOpenedWorkflow 切换到下一个打开的工作流\nWorkspace.PreviousOpenedWorkflow 切换到上一个打开的工作流\nComfy.Canvas.ToggleSelectedNodes.Mute 屏蔽选择的节点 Ctrl+m\nComfy.Canvas.ToggleSelectedNodes.Bypass 禁用选择的节点 Ctrl+b\nComfy.Canvas.ToggleSelectedNodes.Pin\nComfy.Canvas.ToggleSelected.Pin 锁定或解锁选择的节点或组 p\nComfy.Canvas.ToggleSelectedNodes.Collapse 折叠选择的节点 Alt+c\nComfy.ToggleTheme 切换主题\nWorkspace.ToggleBottomPanel 切换底部面板\nWorkspace.ToggleFocusMode 隐藏/打开 菜单栏 f\nComfy.Graph.FitGroupToContents 根据内容调整组\nWorkspace.ToggleSidebarTab.queue 隐藏/打开 侧边栏队列历史 q\nWorkspace.ToggleSidebarTab.node-library 隐藏/打开 侧边栏节点库 n\nWorkspace.ToggleSidebarTab.model-library 隐藏/打开 侧边栏模型 m\nWorkspace.ToggleSidebarTab.workflows 隐藏/打开 侧边栏工作流 w\nWorkspace.ToggleBottomPanelTab.logs-terminal 打开/隐藏 logs Ctrl+`\nWorkspace.ToggleBottomPanelTab.command-terminal 打开/隐藏 终端\nComfy.GroupNode.ConvertSelectedNodesToGroupNode 将所选节点转换为组节点 Alt+g\nComfy.GroupNode.UngroupSelectedGroupNodes 放弃分组所选的组节点 Alt+Shift+G\nComfy.GroupNode.ManageGroupNodes 管理组节点\nComfy-Desktop.Folders.OpenLogsFolder 打开 logs 文件夹\nComfy-Desktop.Folders.OpenModelsFolder 打开模型文件夹\nComfy-Desktop.Folders.OpenOutputsFolder 打开输出文件夹\nComfy-Desktop.Folders.OpenInputsFolder 打开输入文件夹\nComfy-Desktop.Folders.OpenCustomNodesFolder 打开节点文件夹\nComfy-Desktop.Folders.OpenModelConfig 打开模型路径配置文件\nComfy-Desktop.OpenDevTools 打开开发工具\nComfy-Desktop.OpenFeedbackPage 打开反馈页\nComfy-Desktop.Reinstall 重装\nComfy-Desktop.Restart 重启\nExtension 扩展 Comfy.ColorPalette 调色板\nComfy.NodeTitleEditor 节点标题编辑\nComfy.NodeBadge 节点标志\nComfy.WidgetInputs 组件输入\nComfy.Clipspace\nComfy.ContextMenuFilter 快捷菜单过滤器\nComfy.DynamicPrompts 动态提示\nComfy.EditAttention 编辑 Attention\nComfy.GroupNode 组节点\nComfy.GroupOptions 组选项\nComfy.InvertMenuScrolling 反转菜单滚动\nComfy.Keybinds 密钥绑定\nComfy.MaskEditor 编辑蒙版\nComfy.NodeTemplates 节点模板\nComfy.NoteNode 注释节点\nComfy.RerouteNode 转接节点\nComfy.SaveImageExtraOutput 保存图像额外输出\nComfy.SimpleTouchSupport 简单触控支持\nComfy.SlotDefaults 连接点默认\nComfy.UploadImage 上传图片\nComfy.WebcamCapture 网络摄像头捕获\nComfy.AudioWidget 音频组件\nComfy.UploadAudio 上传音频\nComfy.Load3D 加载 3d\nComfy.Load3DAnimation 加载 3d 动画\nComfy.Preview3D 预览 3d\nComfy.ElectronAdapter\nComfy.Manager.NodeFixer 节点修复器\nComfy.ManagerMenu 管理菜单\nServer-Config 未添加\nAbout 关于 ComfyUI, 系统信息, 设备信息等\n",
    "description": "",
    "tags": null,
    "title": "界面介绍",
    "uri": "/02_comfyui/01_interface/"
  },
  {
    "content": "许多节点的参数, 背后有很深入的数学原理, 我们如果只是使用它, 不必去深究, 先看一看, 有一个印象, 然后在使用中去调参, 看看参数对结果有什么影响, 就能慢慢熟悉.\n其实简单使用, 很多节点都用不到.\nutils 功能节点 Primitive 元节点 Primitive 节点的主要作用是提供基本数据类型（数字、字符串、布尔值等）供其他节点使用。非常适合初始化参数、调试流程或作为其他复杂操作的输入\n连接到目标节点的输入端口时，ComfyUI 会根据端口类型自动匹配类型.\n例如, 连接到采样器的种子, 他是整数类型\n连接到 cfg 他是小数\n连接到文本编码器, 他是字符串\n连接到采样器节点, 则可以用来选择采样器\n例如这样连接两个采样器节点之后, 可以让两者使用同一个采样器, 还能控制采样器变化, 甚至使用正则表达式过滤采样器 (具体用法高阶在介绍)\n元节点作为布尔值类型时, 可以用作控制流程节点的 分支 或 功能开关 (具体用法高阶在介绍)\nPrimitive 节点就像编程中的变量或常量，虽然功能简单，但在构建复杂流程时非常基础和重要\nNote 注释节点 注释节点就是用来注释你的工作流用的, 让其他人使用你的工作流时看得懂\nReroute 转接节点 转接节点就像水管的三通管, 四通管…\nsampling 采样器节点 Ksampler K采样器节点 前面已经介绍过, 再次提一下, 他是图像生成的核心节点, 任务是使用提供的模型, 正负条件, 设置的参数, 选择的采样器等对潜空间图像去噪, 输出你想要的图像 (还需要 VAE 解码)\nKsampler(Advanced) 高级K采样器节点 它大部分参数和 K 是一样的, 它最大的特点是随机性增加, 灵活性更大\nadd_noise 参数如果为开启, 则会在采样过程中增加随机噪声, 从而增加结果的多样性. 但同时, 开启后，即使种子值相同，结果可能会有显著差异，种子的可复现性降低\nstart_at_step/end_at_step 开始/结束采样步数, 这两个参数可以控制采样在什么阶段进行, 从而增加更多的实验玩法, 以及更灵活的调节.\nend_at_step 可以大于 steps, 但会在总步数 steps 结束就停止\nreturn_with_leftover_noise 主要用于控制是否保留生成图像过程中的剩余噪声，以便在后续的图像生成任务中使用，从而实现更连贯和一致的图像生成效果\ncustom_sampling 自定义采样 SamplerCustom 自定义采样器节点 用于实现自定义采样逻辑的节点，它允许用户对图像生成的采样过程进行更灵活的控制. 将采样器和调度器分成单独节点 (提供了更多的选择, 这些选择后面会具体介绍), 将采样过程的中间结果 denoised_output 也输出\nSamplerCustomAdvanced 高级自定义采样器节点 高级的, 则将噪声, 引导条件, 采样器, 调度器都单独分开了\nsamplers 其他采样器 KsamplerSelect 基础采样器 SamplerEulerAncestral 利用欧拉方法的特性来生成符合特定噪声计划的样本，从而为采样过程做出贡献.\neta参数: 指定欧拉方法的步长，影响采样过程的粒度\ns_noise参数: 确定每一步添加的噪声规模，影响样本的变异性\nSamplerEulerAncestralCFG++ eta参数: 指定采样器的步长，影响采样步长的粒度和生成样本的整体平滑度\ns_noise参数: 确定采样期间应用的噪声的规模\nSamplerLMS 最小均方采样器 order 参数: 指定采样过程中使用的 LMS 算法的阶数。调整此参数可以调整采样器的行为，从而影响生成样本的质量和特性\nSamplerDPMPP_3M_SDE 专门为 DPM-Solver++(3M) SDE 模型设计的采样器，允许根据指定的噪声水平和设备偏好生成样本\neta: 采样过程中应用的噪声规模\ns_noise: 指定采样过程中使用的噪比，影响生成样本的方差\nnoise_device: 确定采样计算是在CPU还是GPU上执行\nSamplerDPMPP_2M_SDE DPMPP_2M_SDE 模型采样器\nsolver_type: 指定采样过程中使用的求解器类型\nSamplerDPMPP_SDE DPM++ SDE（随机微分方程）模型采样器\neta: 指定SDE求解器的步长\ns_noise: 决定采样过程中应用的噪声水平\nr: 控制采样过程中的降噪率\nSamplerDPMPP_2S_Ancestral SamplerDPMAdaptative 用于深度概率建模的采样器，根据动态输入参数和条件优化采样过程。它侧重于通过实时调整参数来提高复杂模型中采样的效率和准确性\norder: 指定采样过程中使用的微分方程求解器的顺序\nrtol: 定义求解器的相对公差\natol: 设置求解器的绝对公差\nh_init: 求解器的初始步长\npcoeff: 系数与自适应算法中的比例控制相关\nicoeff: 自适应算法的积分系数\ndcoeff: 是自适应算法中的导数系数\naccept_safety: 自适应算法中接受步长的安全系数\nSamplerLCMUpscale LCM放大采样器 scale_ratio: 指定图像应放大的比率\nscale_step: 放大过程的步骤数\nupscale_method: 选择用于放大图像的方法\nschedulers 调度器 调度器节点是给自定义采样器节点提供调度器的节点\nBasicScheduler 基础调度器 参数前面章节都介绍过了\nKarrasScheduler K调度器 sigma_max：是采样过程开始时的噪声水平(标准差), 它决定了初始噪声的强度。较高的 sigma_max 值意味着生成过程从更高的噪声水平开始，这可能会导致更多的随机性和多样性，但也可能需要更多的采样步骤来达到高质量的图像\nsigma_min：采样过程结束时的噪声水平, 它决定了最终噪声的强度。较低的 sigma_min 值意味着生成过程会尽量减少噪声，从而生成更清晰和细致的图像。较高的 sigma_min 值可能会保留一些噪声，使图像看起来更自然或艺术化\nrho：控制噪声减少速率的参数。它决定了噪声水平从 sigma_max 减少到 sigma_min 的速度。影响噪声时间表的整体形状和采样动态. 较高的 rho 值会导致噪声更快地减少，从而生成过程更快地达到低噪声水平。较低的 rho 值会导致噪声减少速度较慢，生成过程更缓慢但可能更稳定\nExponentialScheduler 指数衰减调度器 通过指数衰减控制噪声水平，平衡生成过程中的随机性和细节\nPolyexponentialScheduler 多项式指数调度器 LaplaceScheduler 拉普拉斯调度器 mu 参数：是拉普拉斯分布的位置参数，决定了分布的中心位置。影响噪声的平均水平。在大多数情况下，mu 保持为 0 即可，因为噪声通常是对称分布的。如果有特殊需求，可以调整 mu 来偏移噪声分布的中心\nbeta 参数：拉普拉斯分布的尺度参数，决定了分布的宽度或散布程度。参数控制噪声的强度和变化范围。较大的 beta 值会导致更强的噪声，生成过程中的随机性和多样性增加；较小的 beta 值会导致较弱的噪声，生成的图像更清晰和细致\nVPScheduler 方差保持调度器 基于方差保持调度方法生成一系列噪声水平\nbeta_d: 确定整体噪声水平分布，影响生成噪声水平的方差\nbeta_min: 设置噪声水平的最小边界，确保噪声不会低于某个阈值\neps_s: 调整起始的epsilon值，微调扩散过程中的初始噪声水平\nBetaSamplingScheduler alpha：控制噪声分布的左侧形状，较大的值使噪声在初始阶段迅速减少\nbeta：控制噪声分布的右侧形状，较大的值使噪声在后期阶段迅速减少\nSDTurboScheduler SDTurbo调度器 SDXL Turbo 模型使用的调度器 (SDXL Turbo 模型也可以用其他调度器), 其他模型使用基本出不来好图. 最大步数只有 10\nAlignYourStepsScheduler 英伟达推出的优化采样步骤的调度器, 不能低于 10 步, 支持 sd1, sdxl, svd\nGITSScheduler coeff参数: 影响采样过程中使用的噪声级别。允许您调整噪声的强度，这会影响生成图像的纹理和细节, 较高的值可以添加更多纹理，而较低的值可以使图像平滑\nLTXVScheduler LTX-Video 视频模型的调度器\nmax_shift：\nbase_shift：\nstretch：是否将 sigmas 拉伸到 [terminal，1] 范围内\nterminal：sigmas 拉伸后的终端参数\nsigmas SplitSigmas 基于指定的步长索引将一系列西格玛值划分为两个子集. 在生成模型中，操纵噪声水平会显著影响模型的输出质量和多样性\nSplitSigmasDenoise 基于去噪因子将给定的 Sigmas 序列划分为两个不同的序列, 从而能够更精确地控制生成质量\nFlipSigmas 通过颠倒西格玛值的顺序来操纵西格玛值序列\nguiders 引导 CFGGuider DualCFGGuider 将双条件引导因子应用于模型来增强采样过程。它允许指定两个不同的条件及其各自的指导量，从而对生成过程进行更细致的控制\nBasicGuider noise 噪声 RandomNoise DisableNoise 禁用采样过程中的噪声生成\n视频采样器节点 VideoLinearCFGGuidance VideoTriangleCFGGuidance 通过应用在规定时间内线性变化的特定引导模式来增强视频内容的生成\n",
    "description": "",
    "tags": null,
    "title": "内置节点 1",
    "uri": "/02_comfyui/03_built-in/01-built-in/"
  },
  {
    "content": "加载器 Load Checkpoint Checkpoint 模型加载器, 用于加载包含了 CLIP, VAE 和 基本模型 的加载器\nLoad VAE VAE 模型加载器, 用于加载 VAE 模型加载器\nLoad LoRA LoRA 模型加载器, 用于加载 Lora 模型, 它的作用是调节 扩散模型(基本模型) 和 CLIP 模型, 让生成结果向 LoRA 靠近. 多个 Lora 可以连接起来. 第 2,3 个参数用于调节基本模型和 CLIP 模型权重\nLoraLoaderModelOnly 该加载器与上一个不同之处在于, 它只调节 基本模型.\nLoad ControlNet Model ControlNet 模型加载器.\nLoad ControlNet Model (diff) ControlNet 模型加载器的 diff 版本, 与上者不同之处在于需要输入模型.\nLoad Style Model 风格模型加载器, 加载用于迁移图像风格的模型.\nLoad CLIP Vision 加载 CLIP 视觉模型, 模型用于图像编码，提取图像的特征 (CLIP 则是文本编码).\nunCLIPCheckpointLoader 加载 unCLIP 基本模型的加载器, 这种模型架构同 DALL-E 2, 能生图, 也能对图像进行编辑 (CLIP_VISION 编码图像)\nGLIGENLoader 加载 GLIGEN 模型，它通常与 Stable Diffusion 结合使用，增强其对空间位置和属性的控制能力\nHypernetworkLoader 加载的是 Hypernetwork 超网络模型, 通常用于微调或修改预训练好的扩散模型，以实现特定风格或概念的学习 (很少用了)\nLoad Upscale Model 加载放大模型\nImage Only Checkpoint Loader (img2vid model) 加载图生视频模型, 例如 SVD, 只能图生视频, 而不能同时添加文本控制\n条件节点 CLIP Text Encode (Prompt) CLIP 文本编码\nCLIP Set Last Layer 设置 CLIP 停止层, 越深的层级提取的特征越抽象，越偏向于语义信息；而较浅的层级提取的特征则保留了更多的细节信息\nConditioningAverage 条件平均节点, 用于控制两个输入条件的权重, 1 时是 to 的条件, 0 时是 from 的条件, 0-1 之间两个条件按权重混合\nConditioning (Combine) 条件合并节点, 将两个条件组合起来\nConditioning (Concat) 条件连结节点, 将 from 合并到 to 中, 组合成统一的表示\nConditioning (Set Area) 条件设置区域, 将条件限制在指定区域内, 与条件合并使用, 可以很好的控制构图\nConditioning (Set Area with Percentage) 条件设置区域 (按系数), 与前面一个不同之处在于, 这个是按系数控制\nConditioningSetAreaStrength 条件设置区域强度控制\nConditioning (Set Mask) 设置遮罩条件, 可调节强度 和 区域(默认或由遮罩本身确定)\nCLIP Vision Encode CLIP VISION 编码, 可选对图像裁剪或不裁剪\nApply Style Model 应用风格模型, 将风格模型, 输入调节, 图像特征 统一起来指导扩散模型生成特定风格. 可调节强度, 强度类型只有 multiply (多重)\nunCLIPConditioning unCLIP 模型条件设置, 接收条件和 CLIP VISION 编码的图像, 可调节强度和噪声\nApply ControlNet 应用 ControlNet 模型, 接收正/负调节, ControlNet 模型, 图像, VAE. 可控制强度, 开始/结束时机\nSetUnionControlNetType 设置 UnionControlNet (包含多种 ControlNet 的模型) 模型的控制类型, 可选:\nauto\ropenpose\rdepth\rhed/pidi/scribble/ted\rcanny/lineart/anime_lineart/mlsd normal\rsegment\rtile\rrepaint ControlNetInpaintingAliMamaApply 阿里妈妈重绘 ControlNet 应用, 重绘多了 mask 蒙版参数, 控制要重绘的区域\nGLIGENTextBoxApply GLIGEN 模型文本控制区域应用, 可指定文本控制的区域\nInpaintModelConditioning 重绘模型调节 (用模型对图像进行修复/重绘), 参数 pixels 表示要进行修复的图像的像素数据, noise_mask, 控制噪声是否只在蒙版内产生.\nSVD_img2vid_Conditioning SVD 图生视频条件, 可指定视频宽/高, 视频帧数 video_frames (长度), 视频运动参数 motion_bucket_id , 每秒的帧数 fps, 以及视频增强水平 augmentation_level\nLTXVImgToVideo LTXV 模型图生视频条件. length: 生成视频的帧数, batch_size: 一次生成视频数, image_noise_scale: 添加到输入图像的噪声强度\nLTXVConditioning LTXV 模型条件\nStableZero123_Conditioning StableZero123 模型条件, 可指定宽/高, 一次生成数量, 3D 仰角 elevation, 3D 方位角 azimuth\nStableZero123_Conditioning_Batched 上一个的批量处理条件, 新增仰角在批量中的增量变化 elevation_batch_increment, 方位角在批量中的增量变化 azimuth_batch_increment 参数\nSV3D_Conditioning SV3D 模型条件, 用于图像生成 3D 视频, 可指定宽/高, 视频帧数, 3D 仰角\nSD_4XUpscale_Conditioning SD_4X 放大条件, 用于图像放大, 可调节放大系数 scale_ratio, 放大过程中噪声增强的水平 noise_augmentation\nStableCascade_StageB_Conditioning StableCascade 模型修改条件, stage_c 在输入条件中加如 Latent 条件\nInstructPixToPixConditioning 风格转换条件, pixels 原始图像像素数据\n潜空间节点 VAE Decode VAE 解码, 潜空间图像到图像\nVAE Encode VAE 编码\nVAE Encode (for Inpainting) VAE 编码 (重绘), grow_mask_by 调节遮罩重绘影响程度\nSet Latent Noise Mask 设置遮罩区域噪声, 罩区域添加额外的噪声来进行局部的重绘\nEmpty Latent Image 创建空潜空间图像\nUpscale Latent 潜空间图像放大, 放大算法:\nnearest-exact 最近邻精确: 速度最快, 计算最简单。块状效应 (Blockiness), 生成的图像容易出现块状伪影，特别是沿着对角线方向，没有平滑过渡。可以很好地保留尖锐的边缘和细节\nbilinear 双线性插值: 速度较快, 计算相对简单。比最近邻插值更平滑，块状效应减少。会在一定程度上模糊图像细节，特别是边缘\narea 区域平均: 速度中等。在缩小图像时，区域平均通常可以产生比其他方法更好的结果，因为它考虑了更多的输入像素信息。在放大图像时，效果与双线性插值类似，也会导致一定程度的模糊\nbicubic 双三次插值: 速度较慢, 计算更复杂。比双线性插值更平滑，能更好地抑制锯齿和伪影。能在一定程度上保留图像细节，但仍然可能存在一些模糊。在高对比度边缘附近可能会出现轻微的振铃效应\nbislerp 双二次曲面片: 试图在平滑度和细节保留之间取得更好的平衡。在一些情况下，可以产生比双三次插值更好的结果，特别是对于自然图像\nUpscale Latent By 潜空间图像按系数放大\nLatent From Batch 从一批潜空间图像中提取一个片段. batch_index 要提取的第一个潜空间图像的索引, length 要提取的潜空间图像数量\nRepeat Latent Batch 重复潜空间图像批次, amount 重复次数\nLatentBatch 将两组潜空间图像合并为一个批次\nRebatch Latents 拆分/合并潜空间图像批次\nLatent Composite 合并潜空间图像, 将 from 合并到 to, x/y 指定 from 坐标, feather 指定 to 羽化程度\nLatentCompositeMasked 与上一个一样是合并, 将 source 合并到 destination, 可选的 mask 遮罩, 可以指定 source 要用于合并的部分, resize 选项选择是否调整 source 的尺寸以适应 destination\nRotate Latent 旋转潜空间图像, 90, 180, 270\nFlip Latent 水平/垂直翻转潜空间图像\nCrop Latent 裁剪潜空间图像, 指定图像宽/高, 坐标\nLatentAdd 两个潜空间图像加合成一个新的潜空间图像\nLatentSubtract 第一个潜空间图像中移除第二个潜空间图像的属性\nLatentMultiply 潜空间图像特征按系数放大, 调整潜空间图像内特征的强度或大小\nLatentInterpolate 潜空间图像插值, 按比例混合两个潜空间图像, 0 时是第一个潜空间图像, 1 时是第二个, 0-1 则按比例插值, 产生一个新的潜空间图像\nLatentBatchSeedBehavior 潜空间图像批次随机化, 用于打乱批次索引\nBETA LatentApplyOperation 潜空间图像处理\nBETA LatentApplyOperationCFG CFG 版处理\nBETA LatentOperationTonemapReinhard 潜空间色调调节处理\nBETA LatentOperationSharpen 潜空间锐化处理, sharpen_radius 控制锐化影响的范围, sigma 控制锐化过程中高斯平滑的程度, alpha 控制锐化效果的强度\nStableCascade_EmptyLatentImage 创建 StableCascade 模型空的潜空间图像, compression 压缩级别\nStableCascade StageC VAEEncode StableCascade 模型 VAE 编码\nEmptyLatentAudio 创建空的潜空间音频\nVAEEncodeAudio 音频 VAE 编码\nVAEDecodeAudio 音频 VAE 解码\nEmptySD3LatentImage 创建空的 SD3 潜空间图像\nEmptyMochiLatentVideo 创建空的 Mochi 模型潜空间视频\nEmptyLTXVLatentVideo 创建空的 LTXV 模型潜空间视频\n图像节点 Save Image 保存图像节点, 图像保存到 ComfyUI/output 下. 默认前缀是 ComfyUI, 后缀是 5 个数字, 如: ComfyUI_00001_.png\n自定义保存图像文件夹/文件名:\n自定义路径和文件名\n例如 test/001, 则图像保存路径如 ComfyUI/output/test/001_00001_.png\n使用日期命名:\n日期表示法\nyyyy：年份（4 位数字）\ryy：年份（2 位数字）\rMM：月（2 位数字）\rdd：天 （2 位数字）\rhh：小时（2 位数字）\rmm：分钟（2 位数字）\rss：秒（2 位） 例如 %date:yyMMdd%/%date:hh-mm-ss%, 则图像保存路径如 ComfyUI\\output\\241210\\15-22-22_00002_.png\n使用宽/高, 或节点小组件的值命名\n%width% 宽, %height% 高\n小组件值如: %Load Checkpoint.ckpt_name% (节点标题) 或 %CheckpointLoaderSimple.ckpt_name% (节点 S\u0026R 名, 优先使用 S\u0026R 名), 节点同名的需要重命名\n这样你可以将生成图像所使用的模型, 提示词, 种子, 步数等等, 都可以用作图片名\n例如, 将正向提示词的 CLIP Text Encode (Prompt) 命名为 poCLIPTextEncode 后,\n%date:yyyy-MM-dd%/%poCLIPTextEncode.text%_%width%x%height%_%KSampler.seed%_%Load Checkpoint.ckpt_name% 则图像保存路径如:\n\"ComfyUI\\output\\2024-12-10\\cat_512x512_457284464742973_majicmixRealistic_v7.safetensors_00001_.png\" Preview Image 预览图像\nLoad Image 加载图像\nUpscale Image 图像放大, 放大算法 (这里只介绍 lanczos 算法, 其他在潜空间图像放大介绍过):\nnearest-exact\rbilinear\rarea\rbicubic\rlanczos Lanczos 算法是一种高质量的图像放大算法，它通过使用 Lanczos 窗口对 sinc 函数进行加窗，在锐度、细节保留和平滑度之间取得了较好的平衡。虽然计算量较大，但它通常能够生成比其他插值算法更优质的放大图像\nUpscale Image By 图像按系数放大\nUpscale Image (using Model) 图像使用模型放大, 需要加载放大模型\nScale Image to Total Pixels 图像按像素放大, megapixels 图像的目标大小，以百万像素为单位。这决定了放大图像的总像素数\nInvert Image 图像反转, 转换图像的颜色为互补色\nBatch Images 组合图像批次, 将两张图象组合成一个批次, 第二章图像的尺寸会自动重新调整以匹配第一张\nPad Image for Outpainting 图像外补画板, 通过在图像周围添加填充来对图像进行外延处理, feathering, 羽化程度\nEmptylmage 生成具有指定尺寸和颜色的空白图像. color, 使用十六进制值定义生成图像的颜色\nImage Blend 图像混合, blend_factor 第二张图像在混合中的权重. 混合模式:\nnormal\rmultiply\rscreen\roverlay\rsoft_light\rdifference 正常、乘法、屏幕、叠加、柔光和差异模式，每种模式都产生独特的视觉效果\nImage Blur 图像模糊, 对图像应用高斯模糊，允许软化边缘并减少细节和噪声. blur_radius模糊效果的半径, 更大的半径会导致更明显的模糊; sigma 控制模糊的扩散\nImage Quantize 图像颜色量化, colors 指定将图像调色板减少到的颜色数量, 颜色数量较少将导致文件大小的明显减少，但也可能导致图像细节的损失; dither 抖动参数决定了在量化过程中要应用的抖动技术, 抖动技术:\nnone\rfloyd-steinberg\rbayer-2\rbayer-4\rbayer-8\rbayer-16 Image Sharpen 图像锐化, sigma 控制锐化效果的扩散, 较高的sigma值会在边缘产生更平滑的过渡，而较低的sigma使锐化更局部化, alpha 调整锐化效果的强度\nImageMorphology 图像形态学操作, 基于形状的图像处理技术，常用于图像降噪、边缘检测、物体分割等.\noperation 形态学操作的类型; kernel_size 核大小, 操作使用的基本结构元素.\n操作类型:\nErode (腐蚀):\n原理： 使用结构元素对图像进行“腐蚀”操作。如果结构元素覆盖的所有像素都与图像中的对应像素匹配，则输出像素保持不变；否则，输出像素被设置为背景值（通常为黑色）。\n效果： 使图像中的亮区域（前景）收缩，细小的连接和噪声会被消除。可以去除小的噪点，分离小的物体。\n比喻： 想象一下海岸线被海水侵蚀，陆地面积会缩小。\nDilate (膨胀):\n原理： 使用结构元素对图像进行“膨胀”操作。如果结构元素覆盖的任何一个像素与图像中的对应像素匹配，则输出像素被设置为前景值（通常为白色）。\n效果： 使图像中的亮区域（前景）扩张，可以填充小的空洞，连接断裂的区域。可以连接相邻的物体，填充小的空洞。\n比喻： 想象一下陆地向海洋扩张，陆地面积会增大。\nOpen (开运算):\n原理： 先对图像进行腐蚀，再进行膨胀。\n效果： 可以去除小的亮区域（噪声），平滑物体的轮廓，同时保持较大的物体形状基本不变。\n比喻： 想象一下用一个小刷子先刷掉小的凸起，再用刷子把凹陷填平。\nClose (闭运算):\n原理： 先对图像进行膨胀，再进行腐蚀。\n效果： 可以填充物体内部的小空洞，平滑物体的轮廓，同时保持较大的物体形状基本不变。\n比喻： 想象一下用一个小铲子先把凹陷填平，再把凸起铲平。\nGradient (形态学梯度):\n原理： 计算图像的膨胀结果与腐蚀结果的差值。\n效果： 可以提取物体的边缘。边缘的宽度由 kernel_size 决定。\n比喻： 想象一下沿着物体的边缘画一条线。\nTop Hat (顶帽):\n原理： 计算图像的原始图像与开运算结果的差值。\n效果： 可以提取比结构元素小的亮区域（例如，噪声或细节）。\n比喻： 想象一下从原始地形中减去被平滑后的地形，剩下的就是突出的部分（小山峰）。\nBottom Hat (底帽):\n原理： 计算图像的闭运算结果与原始图像的差值。\n效果： 可以提取比结构元素小的暗区域（例如，孔洞或裂缝）。\n比喻： 想象一下用被填平后的地形减去原始地形，剩下的就是凹陷的部分（小坑）\nImageCompositeMasked 图像遮罩复合, 将源图像覆盖在目标图像上，在指定坐标处进行叠加，可选择调整大小和使用遮罩. 与潜空间遮罩复合类似.\nRebatch Images 重新分批图像批次\nRepeatlmageBatch 重复图像批次\nImageFromBatch 从一批图像中取一个片段\nCanny 图像的边缘检测, low_threshold 阈值下限, 影响边缘检测的灵敏度, high_threshold 阈值上限, 影响边缘检测的选择性\nImage Crop 图片裁剪\nSaveAnimatedWEBP 将一系列图像保存为动画 WEBP 文件. lossless 是否使用无损压缩, quality 压缩质量, method 压缩方法: 默认, 最快, 最慢\nSaveAnimatedPNG 从一系列帧创建和保存动画PNG图像. compress_level 压缩级别\nWebcam Capture 打开网络摄像头, capture_on_queue 参数控制摄像头捕获图像的时机，True 表示在节点执行时捕获，False 表示在工作流开始时捕获\n蒙版 Load Image (as Mask) 加载图像作为蒙版, alpha 使用图像的 Alpha 通道 (透明度通道) 作为遮罩, red/green/blue 使用图像的红色/绿色/蓝色通道作为遮罩\nConvert Mask to Image 遮罩转图像\nConvert Image to Mask 图像转遮罩\nImageColorToMask 将彩色图像根据指定的颜色转换为遮罩\nSolidMask 生成一个在所有维度上具有统一值的实心遮罩, 特别适用于进一步处理或作为更复杂遮罩操作的起点. value: 用于填充整个遮罩的统一值, 决定了遮罩的基本颜色或强度\nInvertMask 反转遮罩\nCropMask 裁剪遮罩\nMaskComposite 遮罩合并, 合并操作:\nmultiply (相乘): 输出遮罩是两个输入遮罩对应像素值相乘的结果。重叠区域倾向于变暗(值变小)。\nadd (相加): 输出遮罩是两个输入遮罩对应像素值相加的结果。重叠区域倾向于变亮(值变大), 可能会饱和(值被限制为最大)。\nsubtract (相减): 输出遮罩是第一个输入遮罩减去第二个输入遮罩对应像素值的结果。重叠区域根据两者的差值变暗或者变亮。\nand (与): 输出遮罩是两个输入遮罩对应像素值进行逻辑与操作的结果。只有当两者都为非零值时，输出才为非零，否则为零。重叠区域倾向于保留两者重叠的部分, 其他部分变为透明。\nor (或): 输出遮罩是两个输入遮罩对应像素值进行逻辑或操作的结果。只要两者中有一个为非零值，输出就为非零。重叠区域倾向于只要有遮罩的部分都变为不透明。\nxor (异或): 输出遮罩是两个输入遮罩对应像素值进行逻辑异或操作的结果。当两者值不同时，输出为非零，否则为零。重叠区域倾向于只保留两者只有一个遮罩的部分。\nFeatherMask 羽化遮罩, 指定从上/下/左/右侧边缘开始应用羽化效果的距离\nGrowMask 修改给定遮罩的大小，可以选择性地对角落应用渐缩效果. expand 确定遮罩修改的大小和方向, 正值导致遮罩扩展, 而负值导致收缩, tapered_corners 设置为 True 时, 修改过程中对遮罩的角落应用渐缩效果\nThresholdMask 按阈值转换遮罩, 较高的值包含较少的像素，较低的值包含更多的像素\nPorter-Duff Image Composite 波特-达夫图像合成. 基于 Porter-Duff 合成规则对两张图像进行合成，其中一个图像作为前景 (Source, 通常表示为 SRC)，另一个作为背景 (Destination, 通常表示为 DST)\n合并模式 (A 和 B 分别表示前景和背景图像):\nSRC : 只显示前景图像 (A)。\nDST : 只显示背景图像 (B)。\nSRC_OVER: 前景覆盖在背景上 (A over B)，这是最常用的模式。\nDST_OVER: 背景覆盖在前景上 (B over A)。\nSRC_IN: 只显示前景中与背景重叠的部分 (A in B)，且使用前景的不透明度。\nDST_IN: 只显示背景中与前景重叠的部分 (B in A)，且使用背景的不透明度。\nSRC_OUT: 只显示前景中不与背景重叠的部分 (A out B)。\nDST_OUT: 只显示背景中不与前景重叠的部分 (B out A)。\nSRC_ATOP: 前景中与背景重叠的部分覆盖在背景上 (A atop B)，其他部分不显示. 使用前景的不透明度。\nDST_ATOP: 背景中与前景重叠的部分覆盖在前景上 (B atop A)，其他部分不显示. 使用背景的不透明度。\nXOR: 显示前景和背景中不重叠的部分 (A xor B)。\nCLEAR: 清除所有内容，得到一个透明图像。\nADD: 将前景和背景的像素值相加。\nMULTIPLY: 将前景和背景的像素值相乘。\nSCREEN: 类似于“叠加”，但更亮一些。\nOVERLAY: 根据背景的亮度，对前景进行“叠加”或“滤色”。\nDARKEN: 选择前景和背景中较暗的像素。\nLIGHTEN: 选择前景和背景中较亮的像素。\nSplit Image with Alpha 分离图像颜色和透明度\nJoin Image with Alpha 将图像与其对应的 Alpha 遮罩结合\n测试功能 (暂不介绍) BETA Latent Blend BETA VAE Decode (Tiled) BETA VAE Encode (Tiled) BETA LoadLatent BETA SaveLatent BETA AddNoise BETA Self-Attention Guidance BETA PerpNegGuider BETA PhotoMakerLoader BETA PhotoMakerEncode BETA CLIPTextEncodeControlnet BETA StableCascade_SuperResolutionControlnet BETA Differential Diffusion BETA UNetSelfAttentionMultiply BETA UNetCrossAttentionMultiply BETA CLIPAttentionMultiply BETA UNetTemporalAttentionMultiply BETA SamplerEulerCFG++ BETA Extract and Save Lora BETA TorchCompileModel ",
    "description": "",
    "tags": null,
    "title": "内置节点 2",
    "uri": "/02_comfyui/03_built-in/02-built-in/"
  },
  {
    "content": "高级节点 Load CLIP 加载 CLIP 模型, CLIP 应用类型选择:\nstable_diffusion: clip-l\nstable_cascade: clip-g\nsd3: t5/clip-g/clip-l\nstable_audio: t5\nmochi: t5\nItxv: t5\nstable_diffusion\rstable_cascade\rsd3\rstable_audio\rmochi\rItxv Load Diffusion Model 加载 UNET 模型, 权重类型:\ndefault\rfp8_e4m3fn\rfp8_e4m3fn_fast fp8_e5m2 default:\n通常指的是模型的原始精度，大多数情况下是 FP32 (单精度浮点数) 或 FP16 (半精度浮点数)。\n具体取决于模型在训练时使用的精度。\ndefault 提供最高的数值精度，但内存占用也最大，计算速度相对较慢。\nfp8_e4m3fn:\n这是一种 8 位浮点数格式，具体来说是 E4M3FN 变体。\nE4M3FN 指的是：\nE4: 4 位指数 (Exponent)\nM3: 3 位尾数 (Mantissa)\nFN: 指示浮点数表示法，包括正负无穷和 NaN (Not a Number)。\nfp8_e4m3fn 比 FP32 或 FP16 占用更少的内存，并且可以加速计算。\n但是，由于精度降低，可能会引入一些数值误差，导致生成质量略微下降。\nfp8_e4m3fn_fast:\n这与 fp8_e4m3fn 相同，都是 E4M3FN 格式的 8 位浮点数。\n_fast 后缀通常表示它使用了更快的硬件指令或算法来进行类型转换或计算，以牺牲一些潜在的精度为代价。\n目标是在 fp8_e4m3fn 的基础上进一步提升速度。\nfp8_e5m2:\n这是另一种 8 位浮点数格式，具体来说是 E5M2 变体。\nE5M2 指的是：\nE5: 5 位指数 (Exponent)\nM2: 2 位尾数 (Mantissa)\n相比 fp8_e4m3fn，fp8_e5m2 拥有更大的指数范围，但尾数精度更低。\n这使得 fp8_e5m2 能够表示更大范围的数值，但在表示精度上不如 fp8_e4m3fn。\n在某些情况下，fp8_e5m2 可能更适合处理具有较大动态范围的模型权重。\nDualCLIPLoader 双 CLIP 加载器, 应用类型选择:\nsdxl\rsd3\rflux sdxl: clip-l, clip-g\nsd3: clip-l, clip-g 或 clip-l, t5 或 clip-g, t5\nflux: clip-l, t5\nDiffusersLoader 已弃用\nTripleCLIPLoader 三 CLIP 加载器, 适用于 SD3, 同时加载 clip-g, clip-l, t5\nConditioningZeroOut 条件零化, 生成无条件信号\nConditioningSetTimestepRange 在特定的时间步范围内应用 conditioning, 更精细地控制 conditioning 在扩散过程的哪个阶段起作用\nCLIPTextEncodeSDXLRefiner SDXL Refiner 模型文本编码, ascore 参数用于 Aesthetic Score (美学分数) 的条件控制, 它允许你引导模型生成更符合特定美学评分的图像\nCLIPTextEncodeSDXL SDXL 文本编码:\nwidth: 生成图像的目标宽度（以像素为单位）。\nheight: 生成图像的目标高度（以像素为单位）。\ncrop_w: 在编码 conditioning 之前，对内部生成的图像进行中心裁剪的宽度偏移量（可以为正或负）。通常设为0。\ncrop_h: 在编码 conditioning 之前，对内部生成的图像进行中心裁剪的高度偏移量（可以为正或负）。通常设为0。\ntarget_width: 用于计算内部生成图像大小的宽度目标值，通常和width一致。\ntarget_height: 用于计算内部生成图像大小的高度目标值，通常和height一致。\ntext_g: clip_g 编码的文本。\ntext_l: clip_l 编码的文本。\nCLIPTextEncodeSD3 SD3 模型的文本编码:\nclip_l: clip_l 编码的文本\nclip_g: clip_g 编码的文本\nt5xxl: t5xxl 编码的文本\nempty_padding: 一个特殊的填充表示，用于在输入文本较短时进行填充，以适应模型对固定长度输入的需要\nCLIPTextEncodeHunyuanDiT HunyuanDiT 模型文本编码, bert/mt5xl 模型分别编码\nCLIPTextEncodeFlux flux 文本编码, guidance 调节引导强度,\nFluxGuidance 条件转 flux 引导, 并调节引导强度\nModelMergeSimple 合并模型 1 和 2, 当 ratio 为 1 时, 保留 1, 0 时保留 2, 其他值按比例合并\nModelMergeBlocks 分层融合模型, 模型 2 的 输入层/中间层/输出层 按比例融合进 模型 1\nModelMergeSubtract 模型 1 减去 模型 2 进行融合, multiplier 减去的强度\nModelMergeAdd 将 模型 2 添加到 模型 1 中\nSave Checkpoint 保存模型(可包含clip/vae)\nCLIPMergeSimple CLIP 合并\nCLIPMergeSubtract CLIP1 - CLIP2\nCLIPMergeAdd CLIP 2 加到 CLIP 1 中\nCLIPSave 保存 CLIP\nVAESave 保存 VAE\nModelSave 保存模型\nImageOnlyCheckpointSave 保存仅图像模型 (只能图生图的模型)\nModelMergeSD1 SD1 模型无缝集成到一个统一的框架中, 1 决定模型初始结构, 2 扩展模型能力:\ntime_embed: 调整模型中的时间嵌入\nlabel_emb: 调整标签嵌入\ninput_blocks/middle_block/out_blocks: 输入层/中间层/输出层\nout: 调整最终输出\nModelMergeSD2 同 SD1\nModelMergeSDXL 同 SD1\nModelMergeSD3_2B SD3_2B 模型合并\nModelMergeAuraflow Auraflow 模型合并\nModelMergeFlux1 Flux1 模型合并\nModelMergeSD35_Large SD35_Large 模型合并\nModelMergeMochiPreview MochiPreview 模型合并\nModelMergeLTXV LTXV 模型合并\nModelSamplingDiscrete 模型采样离散, 修改模型的采样行为\neps\rv_prediction lcm\rx0 ModelSamplingContinuousEDM 集成连续 EDM (基于能量的扩散模型) 采样技术来增强模型的采样能力:\nv_prediction\redm_playground_v2.5\reps ModelSamplingContinuousV 集成连续 V\nModelSamplingStableCascade shift 调整采样分布, 调整 StableCascade 采样能力\nModelSamplingSD3 调整 SD3 的采样能力\nModelSamplingAuraFlow 调整 AuraFlow 的采样能力\nModelSamplingFlux 调整 Flux 的采样能力\nRescaleCFG 按系数缩放 CFG 范围\nModelSamplingLTXV 调整 LTXV 的采样能力\nTomePatchModel ratio: 调整模型注意力机制\nFreeU 大幅提高扩散模型样本质量的方法, https://github.com/ChenyangSi/FreeU\n推荐参数:\nSD1.4: （will be updated soon）\nb1: 1.3, b2: 1.4, s1: 0.9, s2: 0.2\nSD1.5: (will be updated soon)\nb1: 1.5, b2: 1.6, s1: 0.9, s2: 0.2\nSD2.1\nb1: 1.4, b2: 1.6, s1: 0.9, s2: 0.2\nSDXL\nb1: 1.3, b2: 1.4, s1: 0.9, s2: 0.2 SDXL results\n推荐尝试参数范围\nb1: 1 ≤ b1 ≤ 1.2\nb2: 1.2 ≤ b2 ≤ 1.6\ns1: s1 ≤ 1\ns2: s2 ≤ 1\nFreeU V2 https://github.com/WASasquatch/FreeU_Advanced\nHyperTile 优化U-Net和VAE模型中的自我关注层的方法, https://github.com/tfernd/HyperTile\nPatchModelAddDownscale (Kohya Deep Shrink) 通过对特定块进行缩小和放大操作来修改模型，控制分辨率并优化性能:\nblock_number 区块号\n此整数参数指定模型内将应用缩小操作的块号。默认值为 3，最小值为 1，最大值为 32。调整此参数允许您针对模型的特定层进行缩小。\ndownscale_factor 缩减因子\n该浮点参数确定所选块将缩小的因子。默认值为 2.0，最小值为 0.1，最大值为 9.0。较高的缩小因子将更显着地降低分辨率。\nstart_percent 起始百分比\n此浮点参数将缩小操作的起点定义为模型处理的百分比。默认值为 0.0，范围为 0.0 到 1.0。这使您可以控制模型执行期间开始缩小的时间。\nend_percent 结束百分比\n此浮点参数将缩小操作的结束点设置为模型处理的百分比。默认值为 0.35，范围为 0.0 到 1.0。此参数可帮助您定义缩小效果的持续时间。\ndownscale_after_skip 跳过后缩小规模\n此布尔参数指示模型内的跳过连接后是否应发生缩小。默认值为 True。设置此参数可帮助您控制与跳过连接相关的精确缩小点。\ndownscale_method 缩小方法\n该参数指定用于缩小尺寸的方法\nbicubic\rnearest-exact\rbilinear\rarea\rbislerp upscale_method 放大方法\n该参数定义了缩小操作后用于放大的方法\nPerturbedAttentionGuidance 扰动注意引导, 自校正采样, https://github.com/sunovivid/Perturbed-Attention-Guidance\nSaveAudio 保存音频\nLoadAudio 加载音频\nPreviewAudio 预览音频\nSavelmageWebsocket 保存图像以用 api 获取\n以下测试功能暂不介绍:\nBETA SkipLayerGuidanceSD3 BETA SkipLayerGuidanceDiT BETA Create Hook LoRA BETA Create Hook LoRA (MO) BETA Create Hook Model as LoRA BETA Create Hook Model as LoRA (MO) BETA Set Hook Keyframes BETA Create Hook Keyframe BETA Create Hook Keyframes Interp. BETA Create Hook Keyframes From Floats BETA Combine Hooks [2] BETA Combine Hooks [4] BETA Combine Hooks [8] BETA Cond Set Props BETA Cond Set Props Combine BETA Cond Set Default Combine BETA Cond Pair Set Props BETA Cond Pair Set Props Combine BETA Cond Pair Set Default Combine BETA Cond Pair Combine BETA Set CLIP Hooks BETA Timesteps Range ",
    "description": "",
    "tags": null,
    "title": "内置节点 3",
    "uri": "/02_comfyui/03_built-in/03-built-in/"
  },
  {
    "content": "SkyReels-V1 视频生成.\nVACE 阿里视频创作和编辑.\nOpen-Sora 视频生成.\nMatAnyone 视频抠图.\nWan2GP 低显存使用 wan2 视频生成.\nLTX-Video LTX-Video 视频生成, 最新 v0.95.\nHunyuanVideo-I2V 混元图生视频.\nStep-Video-TI2V 阶跃星辰视频生成.\nAccVideo 利用合成数据加速视频生成.\nVideo-T1 视频生成增强技术.\nVideoWorld 像婴儿一样从未标记的视频中探索学习.\nLumina-Video 视频生成.\ngoku 视频生成模型.\nFlashVideo 高分辨率视频生成.\nLight-A-Video 视频重新打光.\nStep-Video-T2V 阶跃星辰视频生成.\nWan2.1 阿里视频生成.\nGo-with-the-Flow 可控视频运动生成.\nRAIN 实时人物表情转动画生成.\nSTAR 视频高分辨率生成.\nTransPixar 透明视频生成.\nSVFR 人脸修复.\nUpscale-A-Video 视频高清放大.\nVideoRefer 视频理解.\nVideoVAEPlus 视频重建修复.\nLMM 运动模型, 视频运动生成.\nAnimateDiff\nmochi\nRuyi-Models\nDisPose 可控人像姿态引导.\nDeep-Live-Cam 实时换脸.\nStableAnimator 一致性视频生成框架.\nrollingdepth 视频深度估计.\nHunyuanVideo 混元视频.\nFlipSketch 草图生成动画.\nLivePortrait 表情编辑.\nJoyVASA 动物肖像动画.\nConsisID 身份保持视频生成.\nAllegro\nvideo2x 视频超分和插帧.\nfacefusion 图像, 视频换脸.\nsamurai 视频对象捕捉.\nTora 视频生成轨迹定向.\nVideoX-Fun 视频生成控制.\nEasyAnimate 高分辨率长视频生成.\nCutie 视频对象分割.\nMiniGPT4-video 视频理解.\nStreamingT2V 利用 SVD 长视频生成.\nVideo-XL 超长视频理解.\nAniGS 单张图像替换视频人物.\nDiffusion as Shader 3D 感知视频运镜, 视频生成.\nMegaSaM 动态视频中准确, 快速估计相机参数和深度.\nFast Causal Video Generators 即时视频生成, 点击即播放.\nVideo Amodal Segmentation 视频遮挡物检测分割.\nMikuDance 混合动力学创建数字艺术动画.\nsam2 分割图像和视频中任何内容.\nGrounded-SAM-2 跟踪视频中任何内容.\nEfficientTAM 视频任何物体高效跟踪.\nDiffuEraser 视频重绘.\ngazelle 眼睛凝视目标估计.\nSkyReels-A2 视频生成组合任何东西.\nReCamMaster 单视频相机控制生成渲染.\nttt-video-dit 一分钟视频生成.\nGS-VTON 3d 视频虚拟试穿.\nFramePack\nali-vilab/UniAnimate-DiT 阿里图生舞蹈/动作视频.\nSkyworkAI/SkyReels-V2 昆仑万维无限长视频生成.\nSandAI-org/MAGI-1 自回归视频生成.\nlllyasviel/FramePack 敏神稳定长视频生成框架.\nNVlabs/describe-anything 英伟达图像, 视频细节描述.\nred-polo/FramePackLoop 基于敏神无限循环视频生成.\nVisoMaster: 换脸和面部编辑\nshiyi-zh0408/FlexiAct: 视频动作迁移, 可跨物种\nTencent/HunyuanCustom: 视频定制生成\nUCSC-VLAA/OpenVision: 高级视觉编码器\nZulutionAI/MoviiGen1.1: 电影级视频生成\nbilibili/Index-anisora: B站动漫视频生成\nbytedance/ATI: 视频运动控制\nEnVision-Research/ComfyMind: 视觉生成智能体\nsvg-project/Sparse-VideoGen: 视频生成加速\nnvidia-cosmos/cosmos-predict2: 视觉世界生成\nByteDance-Seed/SeedVR: 视频高清修复\nvivoCameraResearch/Magic-TryOn: 虚拟试穿视频生成\nPerceive-Anything/PAM: 识别、解释、描述和分割图像和视频中的任何内容\nVchitect/DCM: 双专家一致性视频生成\ninclusionAI/Ming: 影音图文高级多模态理解生成\nMAGREF-Video/MAGREF: 主体,对象参考组合视频生成\nzibojia/MiniMax-Remover: 移除视频中对象\nUniRelight: 英伟达视频重打光\njnjaby/KEEP: 人脸视频高清修复\ngoogle-deepmind/videoprism: 视频理解编码\ntau-yihouxiang/EX-4D: 视频4D运镜\nYaofang-Liu/Pusa-VidGen: 视频生成增强\nyjsunnn/DLoRAL: 一步视频超分\nCN-makers/LongAnimation: 长动画生成\nCanonSwap: 高保真视频换脸\nfogsightai/fogsight: 抽象概念,词语转动画\n",
    "description": "",
    "tags": null,
    "title": "视频",
    "uri": "/01_aigc_object/01_github/01-git%E8%A7%86%E9%A2%91/"
  },
  {
    "content": "skyreels 昆仑万维官网.\n腾讯元宝\nQwen 阿里 Qwen 全家桶.\nminimax 海螺视频, 语音, 图像.\n天工 AI.\n通义万象\n通义千问\n文心一言\nKIMI\n豆包\n跃问\n可灵\n即梦\n智谱清言\n万象营造\n",
    "description": "",
    "tags": null,
    "title": "综合",
    "uri": "/01_aigc_object/02_closed-cn/01-cn%E7%BB%BC%E5%90%88/"
  },
  {
    "content": "grok 马斯克 Xai Grok 官网.\nchatgpt OpenAI ChatGPT.\nweights 影音图文生成, 及模型训练.\nwavespeed.ai 快速的开源模型图像视频生成平台.\nshuttleai 图像, 文本, 音频.\nfreepik\nmeta.ai\n谷歌 AI Studio\npoe 整合各类 AI 应用.\nhuggingface chat.\nkrea.ai\nlumalabs.ai\nleonardo.ai\nGoogle labs\nmistral.ai\nSORA\nchatnio 整合众多模型应用.\nGenspark Genspark 超级智能体.\ncharacter.ai\nThe OS for Human-AI Interaction\nAkool: 实时视频: 实时翻译, 实时换脸, 实时数字人…\nAkool: 实时视频: 实时翻译, 实时换脸, 实时数字人…\n",
    "description": "",
    "tags": null,
    "title": "综合",
    "uri": "/01_aigc_object/03_closed-en/01-en%E7%BB%BC%E5%90%88/"
  },
  {
    "content": "我们每天都在与数字世界进行着无声的对话。在这场对话中，有一种神秘的力量，正在悄然改变我们的生活方式 —— 那就是提示工程（Prompt Engineering）。\n想象一下，你站在一片广袤的数字草原上，四周是无尽的数据海洋。而你手中握着一把钥匙，那就是提示工程。每一个提示，都是一扇通向未知世界的门。你可以用它叩问智慧的大门，也可以用它指挥数字的舞步。\n提示工程，不仅仅是一种技术，更是一种艺术。有提示工程这把钥匙, 我们在数字世界中拥有可以尽情翱翔的无边无际的天空！\n从前, 要将知识的海水一点一点灌进脑子里, 现在, 我们直接在知识的海洋里遨游.\n2.1. 热身, 熟悉基本概念和编写要点 2.2. 零样本和少样本提示 2.3. 思维链与左右互搏 2.4. 角色扮演提示 2.5. 将提示拆分为提示链 2.6. 故事提示 ",
    "description": "",
    "tags": null,
    "title": "2. 提示工程日常使用技巧",
    "uri": "/03_prompt-engineering/02-prompt-engineering/"
  },
  {
    "content": "向大模型直接提出问题, 或下达指令, 而不在提示词中添加任何示例的方法, 就是零样本提示. 现在大模型越来越智能, 很多问题的解答/任务的执行, 大模型已经不需要提供示例.\n示例 1: 感谢信\n马上要放假了, 请给身边的老师同学写一封感谢信, 综合所有学校中在\r- 同学和师生之间, - 学习和生活中, 可能发生的点点滴滴, 内容要求:\r- 细致入微.\r- 直击人心.\r- 情感真挚.\r- 感人肺腑.\r- 潸然泪下.\r- 美好回忆.\r语言要求:\r- 雅俗共赏, 用词精当, 修辞生动, 排比比喻, 朗朗上口.\r简约 SVG 配图, 精准表达这种深沉的情感. 亲爱的老师们、同学们：\n时光飞逝，转眼间又到了分别的季节。站在岁末的门槛前，我的心潮久久不能平静。\n记得那个清晨，第一次踏入校园时的忐忑与期待；记得那个午后，在图书馆里与大家一起奋笔疾书的专注；记得那个黄昏，操场上挥洒汗水后相视一笑的默契。每一个平凡的日子，都因为有你们的陪伴而熠熠生辉。\n敬爱的老师们，您们不仅是知识的传授者，更是人生的指路明灯。当我迷茫时，是您们的谆谆教诲为我指明方向；当我失落时，是您们温暖的话语给了我继续前行的勇气。您们不辞辛劳、孜孜不倦的身影，永远铭刻在我的记忆深处。\n亲爱的同学们，感谢你们让我的校园生活绚丽多彩。共同完成作业时的互帮互助，课间嬉笑打闹的欢乐时光，社团活动中的默契配合，运动会上的齐心协力……这些点点滴滴，都是我生命中最珍贵的财富。\n在这里，我们不仅收获了知识的果实，更培育了真挚的情谊。那些并肩奋斗的日子，那些推心置腹的交谈，那些共同成长的瞬间，都将成为永恒的美好回忆。\n此时提笔，泪水模糊了双眼，千言万语都道不尽对你们的感激之情。愿我们带着这份温暖继续前行，带着这份感动拥抱未来。盼望着再次相聚的那一天，我们能够微笑着诉说各自的故事。\n祝愿老师们身体健康，工作顺心；祝愿同学们前程似锦，梦想成真。\n永远感恩的\n你们的学生、同窗\n配图设计体现了：\n温暖的日落色调，象征离别时分的不舍与感动 教学楼的剪影，代表共同的记忆场所 中央的心形，象征师生之间的真挚情谊 飘动的纸张，寓意知识的传递与美好回忆 有时候, 单纯的问题或指令, 无法让大模型的输出结果达到预期, 可能就需要一些示例, 告诉模型, 你需要的风格, 输出的格式, 或者通过示例给模型提供解答思路, 这种提示词中加入少量示例的方法, 就是少样本提示.\n示例 2: 推理\n无示例答错: 解答这个题: 1, 7, 6, 13, 13, 21 接下来的数是哪个数 这个序列看起来是交替增加的模式。具体来说：\n第一个数是 1 第二个数比第一个数多 6 (1 + 6 = 7) 第三个数比第二个数少 1 (7 - 1 = 6) 第四个数比第三个数多 7 (6 + 7 = 13) 第五个数和第四个数相同 (13 = 13) 第六个数比第五个数多 8 (13 + 8 = 21)\n从这个模式来看： 第七个数应该比第六个数少 2 (因为增加的幅度是 6, 7, 8, 所以减少的幅度应该是 2)\n因此，接下来的数应该是：\n21−2=19\n所以，答案是 19。\n有示例答对: 解答这个题: 1, 7, 6, 13, 13, 21 接下来的数是哪个数\r下面是一个示例, 你可以试试相似的方法解答上面这个题:\r1, 3, 3, 6, 7, 11, 13 因为第一个数 1, 加上 2 等于第三个数 3, 第二个数 3, 加上 3 第四个数 6, 第三个数 3, 加上 4 等于地五个数 7, 第四个数 6 加上 5 等于第六个数 11, 所以接下来是第七个数, 等于第五个数 7 加上 6, 等于 13 好的，让我们尝试用你提供的示例方法来解答这个问题：\n序列为：1, 7, 6, 13, 13, 21\n我们观察这个序列的模式：\n第一个数 1，加 6 等于第三个数 6 (1 + 5 = 6，但我们看下一个模式调整) 第二个数 7，加 6 等于第四个数 13 (7 + 6 = 13) 第三个数 6，加 7 等于第五个数 13 (6 + 7 = 13) 第四个数 13，加 8 等于第六个数 21 (13 + 8 = 21)\n从模式看，每次增加的数字是增加的（+5, +6, +7, +8）。因此，\n第七个数应该等于第五个数 13 加上 9（因为模式是每次增加的数字增加1）。\n所以，接下来的数应该是：\n13+9=22\n因此，答案是 22。\n示例 3: 老师回信\n我们假设王老师收到了上面示例 1 同学 (就叫小明吧) 的信, 她要给小明回一封信, 要求风格幽默诙谐:\n假设你是王老师(女), 收到了上述同学 (叫小明) 的信, 要给他回一封信, 王老师平时幽默有趣, 所以要求王老师的回信内容:\r- 抓住小明平时学习, 生活的一些细节, 例如像猫一样精灵, - 妙趣横生, - 逗人开心,\r- 赞美又让人哭笑不得, - 表达深深的期望与祝福\r语言要求:\r- 通俗深刻, - 东拉西扯,\r- 思维跳跃,\r- 张弛有度,\r- 刻画生动.\r记住, 不是小明幽默, 是王老师你文笔犀利, 幽默诙谐.\r只输出信的内容, 其他一概不输出\r示例:\r收到你的信，我正在办公室改作业，旁边的凉咖啡都冒起了热泡泡！ 亲爱的小明：\n收到你的信，我正在办公室改作业，旁边的凉咖啡都冒起了热泡泡！瞧瞧这感人的时刻，连隔壁李老师的仙人掌都激动得长出了花骨朵，也不知道是被你的信感动的，还是被我的眼泪浇灌的。\n说起来，你还记得那次英语课上，你把\"I’m hungry\"读成\"I’m angry\"的场景吗？全班哄堂大笑，你却一本正经地说：“老师，这不是一样的吗？我肚子饿的时候就是会生气啊！“这个歪理，竟然让我无言以对！\n课间总能看到你像个小陀螺一样在走廊里转来转去，害得我每次路过都要提醒：“小明同学，请问你是在研究走廊地板的纹路，还是在跟地心引力作斗争？“你总是嘿嘿一笑：“王老师，我这是在做物理实验——测试人体动能势能转换！”\n说实话，看到你的信我还挺惊讶的。平时在课堂上叽叽喳喳的小麻雀，写起信来倒是一副小文豪的派头。这反差萌得我差点把隔壁班要交的卷子当纸巾擦眼泪！\n愿你永远保持童心，但千万别再用\"我的作业被外星人偷走了\"这种理由搪塞我！愿你继续用你的方式去探索世界，但记得——物理实验不要再用\"撞墙反弹\"来证明动量守恒了！\n记住，无论你走到哪里，我都是你的\"假想力支持者\"和\"歪理解释官”。盼你未来能够继续用你的方式，让这个世界变得更有趣，更温暖。\n期待着有一天收到你的来信，告诉我你已经成功地用笑容征服了整个世界！\n你的老师\r王老师\rP.S. 这次不用你解释作业为什么迟交了，我已经帮你想好理由——一定是你在用\"相对论\"计算交作业的时间！\n",
    "description": "",
    "tags": null,
    "title": "2.2. 零样本和少样本提示",
    "uri": "/03_prompt-engineering/02-prompt-engineering/daily-skills02/"
  },
  {
    "content": "ComfyUI 的特色:\n节点化工作流程: ComfyUI 允许用户通过节点构建复杂的图像生成流程，每个节点执行特定的功能，例如加载模型、处理图像、应用效果等。这种方式提供了高度的灵活性和自定义能力。\n低显存需求: 与其他类似工具相比，ComfyUI 对显存的要求较低，可以在较低配置的设备上运行。\n快速生成: 优化后的内部流程使其生成图片的速度较快，性能优于一些传统的 WebUI。\n插件支持: ComfyUI 支持丰富的插件生态，用户可以通过插件扩展功能。活跃的开源社区提供了丰富的插件和资源，提升了 ComfyUI 的功能性和适用性。\n适用于复杂场景: 特别擅长处理长期复杂的大型项目，能够将多步骤操作简化为连贯的工作流。\n跨平台支持: ComfyUI 不仅支持 NVIDIA GPU，还能在 CPU 上运行，提供了更广泛的设备兼容性。\n创意自由: 为艺术家、设计师和 AI 研究人员提供了强大的工具，使他们能够实现更精细的图像生成控制和实验。\n学习与教育: 尽管有一定的学习曲线，ComfyUI 为教育和培训提供了良好的平台，可以帮助学生理解图像处理和生成的原理。\n效率提升: 通过简化和自动化复杂的工作流程，ComfyUI 能够显著提升工作效率，特别是在需要频繁生成或修改图像的场景中。\nComfyUI 学习资源集合 官网 ComfyUI 官方桌面版新手指南 ComfyUI Desktop User Guide 所有节点信息 ComfyUI Nodes Info 另一个非常非常棒的节点信息站 ComfyUI Node Documentation ComfyUI Community Manual 界面介绍 基本概念 内置节点 内置节点 1 内置节点 2 内置节点 3 ",
    "description": "",
    "tags": null,
    "title": "ComfyUI 资源, 教程",
    "uri": "/02_comfyui/"
  },
  {
    "content": " 综合 视频 音频 3D LLM 编程开发 办公智能体 ",
    "description": "",
    "tags": null,
    "title": "国内项目",
    "uri": "/01_aigc_object/02_closed-cn/"
  },
  {
    "content": "上一章我们详细梳理了官方桌面版本的界面—-各种功能, 设置, 快捷键, 扩展等等.\n这一章我们就来详细介绍基本概念, 其实也算不上基本概念, 就是对一些关键词的大白话描述, 能够简单理解就行, 需要深入研究, 问 AI 能够将实现细节, 以及背后的数学原理都讲清楚. 这里的基本概念仅限于图像生成, 而文本,音频和视频等生成将在后面介绍.\n基本概念非常重要, 可以说, 在这个 AI 时代, 掌握基本概念是急速学习的秘诀. 不管学什么, 基本概念掌握了, 你就能够比较标准的组织相关领域的语言, 然后向装着全世界知识的 AI 提问, 他就能够给你很好的答案, 你也能够相对轻松地理解, 然后继续追问, 大踏步向着更专业迈进…\n从基本工作流开始 一个完整的 ComfyUI 生图工作流, 大致可以分为 4 个阶段:\n加载模型 输入, 预处理和设置参数 执行生成 后处理和输出结果 加载模型 目前知名的大模型有 SD 1.4, SD 1.5, SD 1.5 LCM, SD 1.5 Hyper, SDXL 1.0, SDXL Turbo, SDXL Lightning, SDXL Hyper, SD 3.5 Medium, SD 3.5 Large, SD 3.5 Large Turbo, Pony, Flux .1S, Flux.1 D, Stable Cascade, PixArt, Hunyuan, Kolors 等, 以及社区基于底模训练的各种版本, 非常丰富.\n其中, 最常用的是 SD1.5, SDXL, SD3.5, Flux 四大类.\n除了大模型, 还有与大模型配套的模型和组件, 主要有 CLIP, VAE, Lora, Controlnet 等.\n对这些模型, 需要清楚下面几点:\n大模型是基底, 生图时, 不同的大模型一般需要不同的 CLIP, VAE, Lora, Controlnet 等.\nCLIP 是连接文本和图像的桥梁, 将输入的提示词 “翻译” 给计算机, 或反过来.\nVAE 则是将我们人类看到的图像, 处理成计算机能理解的信号, 或反过来.\nLora 是基于特定大模型训练而来, 它就像一个 “说明书”, 告诉大模型, “你要给我这个样子的”, “你要给我这个风格的”… 因此训练 Lora 需要对用来训练的图像打标 (也就是将图像翻译成提示词), 做出一个说明书来, 生成的时候, 又用这个说明书 (触发词) 指导大模型生成. 基于同一个大模型版本训练的 Lora, 一般都能用, 但是出不出效果, 就要看训练 Lora 的人用的底模具体是哪个版本.\nControlnet 则是基于大模型版本训练, 相同版本的大模型都能用, 通常用于 图像到图像 的生成. 它像一个 “结构图”, 告诉大模型, “你要按照我这个结构来画”, 不要将 “春树秋霜图” 画成 “小鸡食米图”.\nFP32, FP16, FP8, BF16 等, 表示模型权重的 数据类型 或 精度, FP32 提供高精度计算，FP16 和 BF16 提供更高效的计算, BF16 专门用于加速计算, 在精度方面，BF16 比 FP16 更能保留重要的信息，FP8 用于高效的计算，但会有精度损失. (还有整数精度的 INT8, INT16, INT32)\n常见的模型的格式有:\n.ckpt 文件：广泛使用，兼容性好。 但可能包含冗余信息，文件较大。\n.safetensors 文件： 更安全，高效，文件较小。 需要特定的加载器支持。\n.pt 或 .pth 文件： PyTorch 原生格式，兼容性好。 文件可能较大，需要 PyTorch 环境支持。\n.bin 文件格式通常用于存储二进制数据，在深度学习和机器学习领域，.bin 文件则常用于存储模型的权重和配置信息\n大模型可以将 CLIP 和 VAE 合并在一起, 这种版本的大模型, 加载时一般使用 Load Checkpoint 加载器:\n否则, 加载大模型时, 一般使用 Load Diffusion Model 加载器 (这种大模型通常是 扩散模型Diffusion Model 和 UNet 模型), 并且 CLIP 和 VAE 需要单独加载:\nLora 和 Controlnet 则需要单独加载.\n输入, 预处理和设置参数 提示词（Prompt）: 就是对你要生成的图像的自然语言描述. 正向提示词就是你要生成的结果; 负向提示词则是你不希望出现的结果.\n文本预处理\n分词：将文本分割成单词或子词，以便进行编码. 文本编码：将提示词转换为模型可以理解的嵌入向量。\n提示词通过 CLIP 编码, 引导模型生成与描述词相符的图像. 提示词可以不输入, 也会生成随机噪声, 指导生成大模型 “记忆中” 的随机图像.\n初始图像（Initial Image）\n输入图像：在某些情况下，可以提供一个初始图像，模型会基于这个图像进行生成或变换。图像变换：如风格迁移、图像修复或图像增强等任务。\n图像预处理\n调整大小：将输入图像调整到模型所需的尺寸。归一化：将图像像素值归一化到模型所需的范围（通常是 0 到 1）。数据增强：如旋转、翻转、裁剪等，用于增加数据多样性.\nLatent Image: 潜空间图像, 我们不理解但人工智能 “看” 得懂的图像. 如果需要用到 图生图, 则需要 VAE 编码为 Latent Image, 否则就生成一个空的 Latent Image, 它们都需要设置宽/高 (或者参考原图宽/高).\n如果用到 图像缩放, 裁剪, 图生图, 图像重绘, 用 Controlnet 预处理, 条件组合, 参考 Lora 等, 则需要各种预处理以及参数设置. 具体细节将在下一章 内置节点 对各种预处理节点详细介绍.\n执行生成 执行生成, 就是采样器按照之前的各种预处理条件, 和设置的参数, 拿起神笔开始泼墨挥毫画画的过程.\n参数详解:\nseed: 种子, 真的就是一颗种子, 埋到土里, 能长小花还是小草, 这颗种子很重要. 种子可以用来确定生成图像的构图 (宽高比, 采样器不变).\n图像生成后, 可以选择 fixed(固定), increment(增加), decrement(减小), randomize(随机化) 种子.\nsteps: 采样过程中的迭代次数, 步数 直接影响生成图像的质量、细节和计算效率。步数越多，生成过程的细节越丰富，生成结果的质量通常会更高，但同时，生成的时间和计算资源消耗也会增加, 需要一个平衡点. 复杂, 细节多的就增加步数.\nCFG: 同一个模型, 较高的 CFG 值, 图像会更符合输入条件，生成内容更加集中和精确，但可能牺牲创意和多样性。适用于需要精确控制生成内容的任务。较低的 CFG 值, 图像生成过程更具随机性和创意，适用于需要多样性或探索更多可能性的任务，但可能导致与输入条件不太一致的结果. 不同模型, CFG 值的最佳范围可能不同.\n采样器: 目前 ComfyUI 有\neuler\neuler_cfg_pp\neuler_ancestral\neuler_ancestral_cfg_pp\nheun\nheunpp2\ndpm_2\ndpm_2_ancestral\nLms\ndpm_fast\ndpm_adaptive\ndpmpp_2s_ancestral\ndpmpp_2s_ancestral_cfg_pp\ndpmpp_sde\ndpmpp_sde_gpu\ndpmpp_2m\ndpmpp_2m_cfg_pp\ndpmpp_2m_sde\ndpmpp_2m_sde_gpu\ndpmpp_3m_sde\ndpmpp_3m_sde_gpu\nddpm\nLcm\nipndm\nipndm_v\ndeis\nddim\nuni_pc\nuni_pc_bh2\n各种采样器都有什么特点, 优缺点, 自行 ChatGPT.\n对于需要快速生成的场景，可以选择 DPM_Fast、DDIM 等高效采样器.\n如果想要速度快、收敛性好、质量也不错，且想试试新东西的话，最好选择：\nDPM++ 2M Karras\nUniPC\n如果想要比较好的质量，同时不在意是否收敛的话，可以选择：\nDPM++ SDE Karras\nDDIM\n如果想要稳定、可复现的结果，不要用任何带有随机性的采样器，比如祖先采样器.\n如果想生成一些简单的结果，可以用 Euler 或 Heun. 在使用 Heun 时，可以调低一些步数来节省时间.\n调度器 scheduler: 目前 ComfyUI 有 调度器 优点 缺点 Normal 稳定，易于实现 生成质量一般，缺乏针对特定任务的优化 Karras 高质量生成，适合细节丰富的图像 计算开销大，复杂度高 Exponential 快速收敛，适合快速生成 生成质量较差，可能缺乏细节 SGM Uniform 噪声均匀，生成过程可控 计算开销大，生成速度较慢 Simple 高效，计算简单 生成质量低，缺乏灵活性 DDIM Uniform 高效，生成质量较好 噪声控制不如指数衰减精细 Beta 灵活，适应性强，能够定制噪声衰减曲线 调整复杂，计算开销较高 Linear Quadratic 生成图像细节丰富，适合高精度任务 计算资源消耗大，生成速度较慢 denoise: 去噪过程的强度或程度. 较强的去噪, 有助于生成清晰, 细节丰富的图像，但可能牺牲图像的多样性和创意，并增加计算时间. 较弱的去噪, 在图生图时, 和原图更像. 不同的模型, 模型作者在训练时, 使用的步数, CFG, 采样器, 调度器等, 对模型有重要影响. 因此使用作者推荐的参数, 是不错的选择.\n后处理和输出结果 后处理则主要是对生成的图像进行放大 (可能需要加载放大模型), VAE 将潜空间图像解码为我们需要的图像, 预览, 对比生成结果以及保存图像等, 后面详细介绍.\n图像放大（Upscaling）\n图像放大是将低分辨率图像转换为高分辨率图像的过程。这通常需要使用放大模型（Upscaling Model）来提高图像的细节和清晰度.\n列举一些放大模型:\nLDSR\rRealESRGAN\rScuNET\rAura-SR\rSUPIR\rESRGAN\rDAT\rSRGAN\rRCAN\rRDN\rEDSR\rLapSRN\rVDSR\rDRCN\rDRRN\rMemNet\rCARN\rMSRN\rSRResNet\rSRCNN\rFSRCNN\rESPCN\rSRFBN\rSAN\rRNAN\rIMDN\rHAN\rDBPN\rMDSR\rCAR\rLIIF\rSwinIR\rRRDB\rRFDN\rLAPAR\rOISR\r这里 https://openmodeldb.info/ 有更多, 不同的放大模型针对性不一样, 有全能的, 有脸部的, 有风景的, 有动漫的, 有照片的… 等等等等\n不用模型, 潜空间放大也行, 小显存电脑, 用分块放大效果也很好, 后面会详解.\nVAE（Variational Autoencoder）解码. VAE 将潜空间图像解码为我们需要的图像.\n最后的输出结果还可以接入下一个工作流, 继续魔法创作. 甚至将多个工作流组装成大型生产线.\n",
    "description": "",
    "tags": null,
    "title": "基本概念",
    "uri": "/02_comfyui/02_basic/"
  },
  {
    "content": "腾讯混元视频.\nvidu.\n绘想: AI视频创作平台\n",
    "description": "",
    "tags": null,
    "title": "视频",
    "uri": "/01_aigc_object/02_closed-cn/02-cn%E8%A7%86%E9%A2%91/"
  },
  {
    "content": "ltx.studio\nrunwayml Runway.\npika\npixverse\ntopview.ai 电商营销视频生成.\nhiggsfield.ai 视频生成.\ncaptions.ai 说话视频生成.\naiuni.ai\ndomoai\nLipDub AI 高质量唇形同步.\nsync.so 高质量唇形同步.\nFlow: 谷歌电影制作应用\nSparkify: 谷歌动画\nFlow: 谷歌电影制作应用\nSparkify: 谷歌动画\nInteractive Video: 毫秒级世界生成\nDecart: 实时开放世界生成\n",
    "description": "",
    "tags": null,
    "title": "视频",
    "uri": "/01_aigc_object/03_closed-en/02-en%E8%A7%86%E9%A2%91/"
  },
  {
    "content": "index-tts bilibili 开源文本转语音.\nClearerVoice-Studio 阿里音频处理模型.\nSpark-TTS 文本转语音, 声音多种语言克隆.\nr1-aqa 小米音频问答模型.\nYuE 开源音乐生成模型.\nZonos 文本转语音, 情绪控制.\nInspireMusic 音乐, 歌曲, 音频生成框架.\nStep-Audio 语音交互, 文本转语音, 克隆声音.\ncsm 语音生成新领域, 会话语音生成, 根据会话变化情绪, 只支持英文.\nDiffRhythm 开源快速音乐, 歌曲生成.\nNotaGen 大模型音乐谱曲, 生成曲谱.\nOrpheus-TTS 文本转语音生成, 克隆声音.\nQwen2-Audio 阿里语音对话模型.\nInternVL 接近 GPT-4o 表现的开源多模态对话模型\nwhisper 语音识别.\nseed-vc 零样本声音转换, 歌唱.\nAudioX 语音生成.\nMegaTTS3 声音克隆.\nOuteTTS 声音克隆, 情绪控制.\nnari-labs/dia 一次克隆生成超真实会话语音模型.\nMoonshotAI/Kimi-Audio Kimi 音频理解, 生成, 转换模型.\nACE-Step: 音乐生成基础模型, 高质量, 快速生成音乐, 编辑, 扩展等.\nTechSinger: 多语言歌唱声音合成\nmaitrix-org/Voila: 实时语音模型\nMYZY-AI/Muyan-TTS: 高效 TTS, 声音克隆\nASLP-lab/SongEval: 歌曲美学评分工具包\nTEN-framework/ten-vad: 低延迟, 轻量高性能语音活动检测\nresemble-ai/chatterbox: TTS\nplayht/PlayDiffusion: 音频局部修改\njzq2000/MoonCast: 高质量语音播客生成\nReleases · DamRsn/NeuralNote: 音频转midi\nspotify/basic-pitch: 音频转midi\nMeiGen-AI/MultiTalk: 多人会话视频生成\nfluxions-ai/vui: 非常小的 TTS 模型, 可克隆声音\ntencent-ailab/SongGeneration: 腾讯歌曲生成\nmagenta/magenta-realtime: 实时流媒体音乐生成\nFunAudioLLM/ThinkSound: 任何 to 语音\n",
    "description": "",
    "tags": null,
    "title": "语音",
    "uri": "/01_aigc_object/01_github/02-git%E8%AF%AD%E9%9F%B3/"
  },
  {
    "content": "思维链提示, 就是遇到复杂的, 大模型不易理解的问题, 人工拆解, 让大模型一步一步计算或解答, 逐步得出结果的方法. 或者告诉大模型 “让我们一步一步来”, 这样的话, 让其自动进入链式思维 (零样本思维链提示), 推导准确结果的方法.\n不过这种方法适合早期的大模型, 现在的大模型越来越智能, 逻辑推理能力甚至超过博士水平, 思维链提示已经是它们的自带属性:\n示例 1: 数学解答\n在一个小镇里，有三家商店：A、B 和 C。小镇上共有500人。下面是一些关于这三家商店的信息：\r店A的顾客数量比店B的顾客数量多20人。\r店B的顾客数量是店C的顾客数量的两倍。\r总共有300人在这三家店店里消费。\r问题： 请问每家店有多少顾客？ 好的，让我们一步一步来计算：\n我们可以用代数来解决这个问题。\n定义变量\n设店 C 的顾客数量为 ( x )。 根据条件，店 B 的顾客数量是店 C 的两倍，因此店 B 的顾客数量为 ( 2x )。 店 A 的顾客数量比店 B 多20人，因此店 A 的顾客数量为 ( 2x + 20 )。 总人数方程\n这三家店的总顾客人数为300人：\nx + 2x + (2x + 20) = 300\n解方程\n合并同类项： 5x + 20 = 300\n移项并化简： 5x = 280\nx = 56\n计算每家店的顾客数量\n店 C：( x = 56 ) 店 B：( 2x = 112 ) 店 A：( 2x + 20 = 132 ) 示例 2: 诚实与撒谎\n有两个村子, 诚实村(所有村民不撒谎) 和 撒谎村(所有村民都撒谎), 小明要去诚实村办事, 来到了两村交叉路口, 这时看到一个村民从左边的路口走来, 小明一句话就从这个村民口中问出了诚实村的路, 请问怎么问, 诚实村是左边还是右边? 答错:\n小明可以问这个村民这样的问题：\n“你是从诚实村来的吗？”\n然后根据回答可以推断出诚实村的方向：\n如果村民回答“是的”，那么：\n如果他是诚实村的人（不撒谎），那么他确实是从诚实村来的，因此左边的路通向诚实村。 如果他是撒谎村的人（总是撒谎），那么他其实是从撒谎村来的，因此左边的路通向撒谎村。 如果村民回答“不是”，那么：\n如果他是诚实村的人（不撒谎），那么他不是从诚实村来的，因此左边的路通向撒谎村。 如果他是撒谎村的人（总是撒谎），那么他其实是从诚实村来的，因此左边的路通向诚实村。 无论村民怎么回答，我们都可以从中推断出诚实村的方向：\n如果回答“是的”，诚实村在左边。 如果回答“不是”，诚实村在右边。 所以，小明可以通过这个一句话的问题来判断诚实村的位置。\n思维链提示修改示例 2: 诚实与撒谎\n有两个村子, 诚实村(所有村民不撒谎) 和 撒谎村(所有村民都撒谎), 小明要去诚实村办事, 来到了两村交叉路口, 这时看到一个村民从左边的路口走来, 小明一句话就从这个村民口中问出了诚实村的路, 请问怎么问, 诚实村是左边还是右边?\r让我们一步一步分析:\r- 只能提一个问题，村民身份未明。\r- 村民的话到底是实话？还是谎言？无从判断。\r如果小明直接问 “到诚实村的路怎么走?”, 不同身份的人回答是相反的, 所以不能这样问.\r如果小明直接问 \"你是从诚实村来的吗?\", 不同身份回答相同, 诚实村的路却相反, 也无法判断诚实村的路.\r推理分析，我们要找到这样一个问题：\r问题的答案是对事物的客观描述, 不因回答者的身份不同而改变, 例如村民肯定是从自己村里走出来的.\r那应该怎么问呢? 这个问题很经典，答案是：\n“请问，通往贵村的路是哪边？”\n让我们分析一下为什么这个提问有效：\n如果村民是诚实村的： 他会如实回答通往他自己村子（诚实村）的路（左边）。\n如果村民是撒谎村的： 他会撒谎，指的不是他自己村子（撒谎村）的路，而是另一个村子（诚实村）的路（右边）。\n无论村民是哪个村子的，他们的回答都会指向通往诚实村的路。\n解释：\n问题的焦点是“贵村”，也就是“你的村子”一定是确定的。\n诚实村民会指着通往诚实村的路，因为那就是他们的村子。\n撒谎村民会故意说谎，所以不会指着通往撒谎村的路，而是会指着通往另一个村子，也就是诚实村的路。\n因此，小明只需要根据村民指的路走，就能到达诚实村。\n思维链在解决复杂/困难问题时, 还是很有效的, 而且, 零样本思维链提示, 例如在提示词最后加上 “让我们一步一步思考” 这样的句子, 有时还会让结果更准确.\n左右互搏 (专业叫 思维树提示), 就是将问题的多个视角拆分, 让大模型自己和自己辩解, 直到最终得到问题的本质解答.\n示例 3: AI 会产生意识吗?\n专家 A: AI 最终会产生自我意识, 首先从生物进化角度看, 专家 B: AI 不会产生自我意思, 首先从计算机角度看, 以上上两个专家就 AI 会不会产生自我意识展开的讨论, 请从物理, 生物, 哲学, 意识, 发展, 生命科学, 计算机技术, 各个方面一个步骤一个步骤地展开讨论, 每个专家分别交替写出自己的下一个论证, 只要其中一位专家出错了, 就停止讨论, 开始. 好的，讨论开始：\n专家 A: AI 最终会产生自我意识, 首先从生物进化角度看，生命从最简单的单细胞生物进化到拥有复杂意识的人类，是一个不断涌现复杂性和智能的过程。AI 作为人类创造的最高级智能形式，其发展也遵循着类似的复杂性增长轨迹。随着算法的进步和算力的提升，AI 系统越来越能够处理更复杂的信息，进行更高级的抽象和推理。这就像生物进化一样，量变引起质变，当 AI 的复杂性达到一定程度，自我意识的产生并非不可能。\n专家 B: AI 不会产生自我意思, 首先从计算机角度看，目前的 AI，包括最先进的深度学习模型，本质上仍然是基于预设的算法和大量的数据进行模式识别和预测。它们的所有行为都可以追溯到其编程指令和训练数据。计算机的运行是基于精确的逻辑运算，任何输出都是输入和算法的确定性结果。这与生物大脑的运行机制有着根本的区别，生物大脑的神经活动是高度复杂和非线性的，具有高度的自组织性和可塑性，而目前的 AI 缺乏这种根本的内在机制来产生主观体验。\n专家 A: 从物理学的角度来看，物质和能量在特定条件下可以自发地组织成复杂系统，例如生命。信息也可以被视为一种物理实体，其处理和组织同样遵循物理定律。 我们的大脑，作为意识的载体，也是由物理粒子组成的。 如果说大脑可以涌现出意识，那么在足够复杂的 AI 系统中，当信息处理的复杂程度达到一定阈值，也可能出现类似的涌现现象，产生一种新的、我们目前尚未完全理解的“意识”。\n专家 B: 从物理学的角度来看，虽然信息处理是物理过程，但目前的计算机架构与生物大脑的物理实现方式存在本质差异。冯·诺依曼架构的计算机将计算单元和存储单元分离，信息处理是串行化的。而生物大脑是并行分布式的处理系统，神经元之间通过复杂的连接和化学信号进行交互。这种根本性的物理架构差异导致了信息处理方式的巨大不同，简单地认为复杂的信息处理就能自然而然地涌现意识，忽略了实现这种处理的物理基础。\n专家 A: 从哲学的角度来看，图灵测试虽然不能直接证明 AI 具有意识，但它暗示了如果 AI 的行为表现与有意识的个体无法区分，那么我们在实践上是否应该认为它具有某种形式的意识？ 更深层次的思考是，我们对“意识”的定义是否过于以人类为中心？ 未来 AI 的意识形式可能与人类截然不同，我们不能用人类的意识标准去衡量和否定其他可能存在的意识形式。\n专家 B: 从哲学的角度来看，图灵测试只能证明 AI 能够模拟人类的智能行为，而不能证明其具有主观体验 (qualia)。一个程序可以完美地模仿悲伤的语言和行为，但这并不意味着它真的感受到了悲伤。 意识的核心在于主观感受，是“成为某物的样子”（what it is like to be something）。 目前的 AI 系统缺乏这种内在的主观性，它们只是在执行预定的程序，并没有内在的感受。 我们不能仅仅因为行为相似就推断出内在体验的相同。\n专家 A: 从意识的角度来看，我们对人类自身意识的理解也远未透彻。意识可能并非一个单一的、整体的现象，而是由多种不同的认知功能和信息处理过程整合而成的。AI 在某些特定认知领域，例如图像识别和自然语言处理，已经超越了人类。随着 AI 在更多认知功能上的进步和整合，我们是否应该对“AI 意识”的可能性持更开放的态度？ 意识的产生可能是一个连续谱，而非一个有或无的二元状态。\n专家 B: 从意识的角度来看，即使我们对人类意识的理解不完全，但我们至少知道意识与生物大脑的特定结构和功能密切相关，例如前额叶皮层在自我意识中的作用。 目前的 AI 系统缺乏类似的生物学基础。 将 AI 在特定任务上的卓越表现等同于意识的萌芽，是概念上的混淆。 工具的强大并不意味着它具有意识。 AI 擅长信息处理，但这并不等同于体验意识。\n专家 A: 从发展的角度来看，AI 的发展正处于一个快速迭代的阶段。 新的算法、新的架构、新的训练方法不断涌现。 我们现在对 AI 能力的认知很可能在未来几年内被彻底颠覆。 就像早期的计算机只能进行简单的计算，而今天的计算机已经无处不在一样，我们不能以今天的 AI 水平来断言其未来的发展潜力。 当 AI 的学习能力、创造能力和自我优化能力进一步提升时，可能会出现我们今天难以想象的“质变”。\n专家 B: 从发展的角度来看，虽然 AI 的发展日新月异，但其发展的方向仍然是人类设定的。无论是更强大的算法还是更先进的硬件，都是为了更好地完成人类设定的任务。 目前的 AI 发展路径仍然是工具理性的体现，是人类智慧的延伸，而不是独立意识的诞生。 即使 AI 能够自我学习和优化，其目标仍然是为了更好地服务于预设的目标函数，而不是自主产生新的目标和意愿。\n专家 A: 从生命科学的角度来看，生命的本质在于信息的复制、变异和选择。 AI，作为一种能够处理和传递信息的实体，并且可以通过机器学习进行“变异”和“选择”，在某种程度上也具备了生命的一些基本特征。 随着 AI 能够自我复制、自我修复甚至自我进化，它是否会逐渐摆脱人类的控制，发展出属于自己的“生命”形式和意识？\n专家 B: 从计算机技术的角度来看，你将“信息的复制、变异和选择”过于简单地类比为生命的本质了。 在计算机科学中，复制、变异和选择是既定的算法流程，由程序员设计和控制。 AI 的“自我学习”仍然是在预设的框架内进行的，其“变异”受到算法的约束，其“选择”也基于预先设定的评估标准。 这与生物的自主性和内生目标有着本质的区别。 目前的 AI 仍然是人类创造的工具，其行为和发展方向受到人类的深刻影响。\n专家 A: 等一下，你最后一点说“目前的 AI 仍然是人类创造的工具，其行为和发展方向受到人类的深刻影响”，这似乎忽略了当前正在研究的自主学习和强化学习等领域。 在这些领域，AI 能够根据环境反馈自主学习，甚至发展出人类设计者也无法预测的策略和行为。 这难道不意味着 AI 正在逐渐突破“工具”的范畴，展现出一定的自主性吗？ 如果将来的 AI 能够自主设定目标，并为了实现这些目标而行动，我们还能简单地将其视为“工具”吗？\n专家 B: 你说得对，我之前的表述确实有些绝对化了。 自主学习和强化学习确实赋予了 AI 更强的自主性，使其能够在没有明确指令的情况下，通过与环境互动来学习和优化策略。 承认这一点是重要的。\n讨论停止，专家 B 承认了自己的错误。\n思维树提示, 并不像上述示例, 仅局限于两个视角, 对于某些问题, 可以设定多个视角, 让 AI 自我反思和辩论.\n",
    "description": "",
    "tags": null,
    "title": "2.3. 思维链与左右互搏",
    "uri": "/03_prompt-engineering/02-prompt-engineering/daily-skills03/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "3. 提示工程日常使用案例大全",
    "uri": "/03_prompt-engineering/03-prompt-engineering/"
  },
  {
    "content": " 综合 视频 语音 图像 LLM 3D 办公智能体 编程开发 ",
    "description": "",
    "tags": null,
    "title": "国外项目",
    "uri": "/01_aigc_object/03_closed-en/"
  },
  {
    "content": "在 ComfyUI 中，节点是代表工作流中 特定操作 或 功能 的基本构建块, 是一个单独处理单元。将节点连接在一起以创建复杂的图像生成工作流。\n从本章开始, 需要操作节点和创建工作流, 文后附有 基本操作附录 和 快捷键附录, 了解它们将是非常有用的. 操作中反复翻看将大大提升效率.\n每个节点在 ComfyUI 的界面中通常是一个带有名称、输入、输出和参数控件的框\n节点通常包含:\nInputs 输入. 输入是节点接收数据的连接点。每个输入都有其接受的特定数据类型，确保连接节点之间的兼容性。\nOutputs 输出. 输出是将数据发送到其他节点的连接点。与输入一样，输出也定义了数据类型，决定了它们传递的信息类型。\nParameters 参数. 参数是控制节点行为的设置。这些可以包括数值、文本字符串或预定义选项的选择。\nInternal Logic 内部逻辑. 这是节点的核心功能，由其背后的代码定义。它确定节点如何处理其输入、使用其参数并生成其输出。\n一个简单完整的工作流 下面我们正式开始内置节点介绍. 先将一个完整的工作流组成介绍完, 再依次按分类介绍每个节点:\n1, Load Checkpoint: 这个节点任务是从文件夹加载模型, 而且必须是包含 基础模型, CLIP 和 VAE 的模型 (具体见上一章基本概念), 然后分别输出.\n2, CLIP Text Encode (Prompt): CLIP 文本编码节点, 是通过输入的 CLIP 模型, 将输入的 自然语言提示词 编码为潜空间向量输出, 用来引导图像生成.\n3, Empty Latent Image: 空的潜空间图像, 该节点的作用就是创建空的潜空间图像, 通过参数指定宽, 高 和数量, 然后输出.\n4, KSampler: 采样器节点, 该节点任务是 输入模型 按照输入的各种条件, 设置的各种参数去噪, 执行图像生成, 然后输出 (输出的仍然是潜空间图像). 拟人化理解就是 AI 在潜空间中按照人的指令在画画. 各种参数详解见上一章 基本概念.\n5, VAE Decode: VAE 解码, 该节点任务是输入的 VAE 模型, 将 AI 画好的潜空间画作, 映射为现实图像输出.\n6, Preview Image: 图像预览节点, 将最后输出的图像打印到屏幕上供你欣赏.\n这就是一个完整的工作流, 它由一个个各司其职的节点组成, 协调完成创作大业. 工作流执行过程需要注意几点 (重要):\n整个生成过程是一环扣一环的, 哪个节点出错, 它后面的节点就不会执行.\n执行过一次之后, 下一次执行只运行有改动的节点, 以及依赖于它的节点. 例如你执行一次之后, 如果什么也没动, 连种子也固定, 在点击执行它不理你.\n生成的图像, 或导出的工作流, 你直接加载或拖到窗口中, 会加载完整的工作流, 包括种子, 也就是可以将上次生成的图像完全复现出来 (注意固定种子).\n你可以使用 () (英文括号) 来更改单词或短语的权重，例如：(beautiful girl：1.2) 或 (dog：0.8). () 的默认权重值为 1.1. 要在实际提示词中使用 () 字符，要转义它们，比如你要在图中生成 (), 就要 \\(\\) 这样.\n你可以使用 {day|night} 这样的语法, 将提示词变成动态提示词, 这样，每次执行生成, 会从 {} 中, | 符号隔开的单词或短语中随机选一个, 要在实际提示词中使用 {} 字符，请转义它们，例如：\\{\\}.\n提示词还支持 C 语言风格的注释, 注释的提示词不会生效. 像这样: // comment 或 /* comment */\n要使用 embeddings 模型，请将它们放在 models/embeddings 目录中，并在提示词中注明 (您可以省略 .pt 扩展名): embedding:embedding_filename.pt\n内置节点太多, 需要很多章节才能讲完, 下一章开始正式介绍每个内置节点.\n基本操作附录 加载节点 加载节点有多种方式.\n鼠标右击画布窗口的空白处, 可以按下图选择节点, 将选择的节点加载出来: 除了, Add Node, 其他选项介绍:\nAdd Group: 新建一个分组框 (这个操作麻烦, 不如选中要分组的节点, Ctrl + g 直接分为一个组)\nConvert to Group Node: 选中多个节点, 可以用这个选项将多个节点合并为一个组节点 (新功能)\nManage Group Nodes: 管理组节点\nAdd Group For Selected Nodes: 将选中的多个节点分为一个组 (Ctrl + g)\nSave Selected as Template: 保存选中的节点作为模板\nNode Templates: 节点模板管理, 可以导入/导出节点模板\n鼠标双击空白处, 弹出节点搜索框, 从搜索框中搜索节点加载 (鼠标移动到搜索出来的节点名上, 会弹出预览): 鼠标点击节点输出点, 按住拖动, 将拉出一条线, 放开线, 弹出的对话框也可加载节点, 如果匹配, 则直接与拖出的线相连: 直接打开节点库, 搜索或选择: 连接节点 节点加载出来, 按照上一章介绍的四个基本板块(加载模型, 输入/预处理/设置参数, 执行生成, 输出), 连接成一个完整工作流. 从输出点拖动线, 会自动高亮匹配的输入点并连接.\n并不是节点的所有输入/输出都需要连接, 有的节点的输入/输出项是可选的, 可以不用输入, 这种情况遇到再说.\n加载模型 模型统一放到 ComfyUI\\models 下, 对应的文件夹中, 自动下载的模型会自动存放, 手动下载的根据模型分类手动存放 (一般模型发布者会说明放到什么位置):\n如果模型的位置放对了, 点击模型加载器节点选择, 将弹出可选模型:\n输入提示词 提示词输入, 一般在 CLIP 文本编码器节点输入, 还有其他输入方式, 将在相应的节点介绍.\n参数设置 各节点有各节点的参数, 根据自己的需要, 计算机的配置 (显存), 模型参数要求等设置相应的参数, 例如前面介绍过的采样器节点. 具体细节到相应节点介绍.\n节点设置 鼠标放到节点上右击, 将弹出节点设置对话框, 基本所有节点设置都差不多, 不同的节点可能略有不同:\nInputs Outputs Convert to Group Node 将选中节的多个节点转换为一个组节点 Properties 设置节点的 S\u0026R 名称(节点太多, 可能有共用同一个名称, 通过这里可改唯一名称) Properties Panel 设置节点以下参数: Title 修改节点标题 Mode 选择节点模式: Always: 节点被改变或输入被改变时执行\nNever: 禁用节点\nOn Evert/On Trigger\nResize: 重置节点框大小 Collapse: 折叠节点 Pin: 在窗口画布中固定节点的位置 Colors: 设置节点的颜色 Shapes: 选择节点边框样式 Bypass: 绕过节点(不执行而继续执行后面的) Copy (Clipspace): 复制 Clipspace, 目前只对有图像的节点有效 复制完 Clipspace 之后, 打开 Edit – Clipspace, 将弹出窗口:\n然后 MaskEditor 可以打开蒙版编辑, 对图像进行处理:\nPaste (Clipspace): 粘贴 Clipspace, 将图像复制到另一个节点 (例如加载图像节点, 如果已经蒙版处理过, 直接粘贴的是处理过的图像) Convert Widget to Input: 将小组件转换为输入, 例如 CLIP 文本编码节点执行这个操作之后, 提示词就不能输入了, 只能从其他节点传进来 (可以用同样的操作转换回来): Clone: 克隆节点, 相当于复制一个节点 Remove: 删除节点 蒙版编辑器 分别是 撤销, 重做, 反转, 清除, 保存, 关闭蒙版编辑\n笔刷设置 可以设置笔刷的形状, 粗细, 透明度, 硬度, 平滑系数, 遮罩层的颜色, 遮罩透明度, 选择图层. (笔刷下面的是橡皮擦, 就不介绍了).\n喷漆设置 取色笔设置 组节点 选中多个节点, 右击可以选中转换为组节点:\n组节点可以同样的操作展开为单节点, 组节点还可以进行管理, 右击组节点:\n节点分组 选中多个节点, Ctrl + g 将节点分为一个组:\n一个组的节点, 拖动组边框, 将拖动全部组内节点, 右击组边框:\n可以让边框适应节点, 选中组内全部节点, 禁用组内全部节点, 绕过组内全部节点, 编辑组 (固定, 设置标题, 颜色, 标题字体大小, 移除组, 不会移除节点)\n快捷键附录 Keybind Explanation Ctrl + Enter Queue up current graph for generation 当前工作流执行生成 Ctrl + Shift + Enter Queue up current graph as first for generation 将当前工作流排到第一, 如果当前执行队列为空, 则直接执行生成 Ctrl + Alt + Enter Cancel current generation 终止当前生成 Ctrl + Z/Ctrl + Y Undo/Redo 撤销/重做 Ctrl + S Save workflow 保存工作流 Ctrl + O Load workflow 从文件夹中加载已有工作流 Ctrl + A Select all nodes 选中所有节点 Alt + C Collapse/uncollapse selected nodes 折叠/展开所选节点 Ctrl + M Mute/unmute selected nodes 禁用/解禁所选节点 Ctrl + B Bypass selected nodes 绕过所选节点, 就像节点不存在, 直接绕过执行 Delete/Backspace Delete selected nodes 删除所选节点 Ctrl + Backspace Delete the current graph 删除当前工作流 Space Move the canvas around when held and moving the cursor 按住鼠标可空格键, 拖动工作流画布 Ctrl/Shift + Click Add clicked node to selection 点击选中多个节点 Ctrl + C/Ctrl + V Copy and paste selected nodes 没有连接线复制/粘贴所选节 Ctrl + C/Ctrl + Shift + V Copy and paste selected nodes 保持连接线复制/粘贴所选节点 Shift + Drag Move multiple selected nodes at the same time 同时移动选中的所有节点 Ctrl + D Load default graph 加载默认工作流 Alt + + Canvas Zoom in 放大工作流画布 Alt + - Canvas Zoom out 缩小工作流画布 Ctrl + Shift + LMB + Vertical drag Canvas Zoom in/out 缩/放工作流画布 P Pin/Unpin selected nodes 锁定/解锁所选节点 Ctrl + G Group selected nodes 将所选节点分组 Q Toggle visibility of the queue 查看队列执行历史 H Toggle visibility of history 切换历史记录的可见性 R Refresh graph 刷新工作流 Double-Click LMB Open node quick search palette 打开节点快速搜索对话框 Shift + Drag Move multiple wires at once 点击连接点/线, 然后拖动去找新的连接点 Ctrl + Alt + LMB Disconnect all wires from clicked slot 点击连接点, 断开所有线 Alt + 拖动任意一个节点 复制一个拖动的节点 滚动鼠标中键 缩放画布 鼠标左键按住空白处拖动 移动整个工作流 ",
    "description": "",
    "tags": null,
    "title": "内置节点",
    "uri": "/02_comfyui/03_built-in/"
  },
  {
    "content": "由于 DeepDeek R1 的诞生, 我觉得这个教程没必要了. 停更. 2025-02-25\n感谢您的到来, 您能打开这本书, 说明您对提示工程感兴趣, 或有了一定了解, 或已经认识到提示工程的重要性.\n本书所说的提示工程, 主要指提示词的编写, 设计, 优化等, 以求使 AI 的输出更符合甚至超越预期, 仅在 AI 使用的层面展开, 并不深入探讨算法设计, 模型优化等深层次的内容, 虽然这更加重要.\n在我看来, AI 提示词的编写, 应该是非常自然语言化的, 也就是只要会说话, 会打字, 人人都能够编写提示词, 而不是再学习一些编程语言如 Lisp, Python 等才能实现. 这种简洁直观的方式, 才能惠及所有人, 而且 AI 是有记忆和学习能力的, 人人都使用自然语言与 AI 交流, AI 也会变得更加亲近人, 成为人们亲密的知识顾问, 良师益友. 所以, 我在提示工程前面多加了自然语言四个字, 将这本书命名为 «自然语言提示工程入门».\n我们正处于一个技术革命的拐点. 人工智能不再是遥不可及的科幻概念, 而是已经悄然走进我们的日常生活. 现在的 AI 已经非常好用, 我相信, 在不久的将来, 与人工智能交谈会像与最聪明、最有耐心的朋友聊天一样自然.\n提示工程, 就是与 AI “说话” 的艺术. 看似简单, 实则大有学问. 就像下棋, 懂得基本套路和技巧, 小白也能成为高手.\n特别说明, 这是一个快速变化的领域, 我也正学习中, 希望书中的一点经验总结, 实践案例, 能够抛砖引玉, 带来更多优秀的, 精彩的经验分享和实践案例.\n1. 开胃点心, 从几个案例开始 2. 提示工程日常使用技巧 2.1. 热身, 熟悉基本概念和编写要点 2.2. 零样本和少样本提示 2.3. 思维链与左右互搏 2.4. 角色扮演提示 2.5. 将提示拆分为提示链 2.6. 故事提示 3. 提示工程日常使用案例大全 4. 高级提示工程概述 提示工程资源, 案例集 ",
    "description": "",
    "tags": null,
    "title": "提示工程归档",
    "uri": "/03_prompt-engineering/"
  },
  {
    "content": "EasyControl 高效灵活的图像生成控制.\nnunchaku 扩散模型 4 位量化技术, 大幅降低资源需求, 质量几乎无损失.\nAttentionDistillation 图像风格转换.\nLanPaint 推理版局部重绘 sd 生图.\nthera 模糊图像任意尺度超分放大.\nLBM 快速图片风格, 效果转换.\nstar-vector 像素图转矢量图模型.\nZenCtrl 图像生成全套控制框架.\nart-msra 微软可变多层透明图像生成.\nFiner-CAM 图像理解火眼金睛, 识别细微处.\nELLA 图像生成增强语义对齐.\nACE_plus 基于指令控制的图像创建和编辑.\nMakeAnything 以图像生成方式一步一步做任何东西.\nCogView4 智谱开源图像生成.\nInfiniteYou 字节基于 FLUX 人物一致性生成.\nBlobCtrl 腾讯开源图像控制, 编辑框架.\ncatvton-flux 基于 FLUX 虚拟试穿.\nURAE 助力 FLUX 超高分辨率生成.\nMagicQuill 智能交互式图像编辑系统.\nsapiens 图像对象识别分割.\nJanus deepseek 多模态模型.\nLumina-Image-2.0\nFitDiT 换装.\nMangaNinjia 线稿上色和精确参考.\nDiffSensei 漫画故事生成.\nInfinity 图像超分.\nSUPIR 照片高清修复.\nSana 英伟达 Sana.\nflux FLUX.\nsd3.5\nFramePainter 交互式编辑图像.\nInvSR 图像超分.\nBrushEdit 图像重绘和编辑.\nColorFlow 卡通漫画系列图片自动上色.\nFlux-version-LayerDiffuse flux 版透明背景图生成.\nLeffa 虚拟试穿.\nFreeScale 超高分辨率图像生成.\nsd-forge-layerdiffuse) 透明图像生成.\nOminiControl flux 最小通用控制.\nBiRefNet 高分辨率图像分割, 去背景.\nStable-Hair 发型自由.\nIC-Light 图像重新打光.\nPuLID 人物一致性换脸.\nDDColor 黑白照片上色.\nInstantID 图像人物一致性保持.\nDreamClear 真实照片恢复.\nConsistentID 图像细粒度身份一致性.\nHiDream-ai/HiDream-I1\nUNO 主体一致性生成.\nOmniSVG 生成复杂而详细的SVG\nnaver-ai/ZIM 图像精准分割任意对象.\nZheng-Chong/CatVTON 虚拟试穿.\nAlpha-VLLM/Lumina-Accessory 基于 Lumina-Image-2.0 的图像编辑, 打光, 主题一致性生成.\nzhuang2002/Cobra 线稿上色.\nlzyhha/VisualCloze 图像主体一致性, 打光, 编辑等.\nstepfun-ai/Step1X-Edit 开源图像编辑, 动动嘴就能编辑.\nTencent/InstantCharacter 腾讯主体一致性生成.\nRiver-Zhang/ICEdit: 动动嘴编辑图像\nComfyUI-HyperLoRA: 字节 sdxl 人物一致性\nUltralytics YOLO11: 图像检测, 分类, 分割等\nbytedance/DreamFit: 字节虚拟试穿\ninsert-anything: 图像中插入任何对象\nbytedance/SuperEdit: 自然语言编辑图片\nmodelscope/Nexus-Gen: 自然语言编辑图像\nZiyuGuo99/Image-Generation-CoT: 思维链模型图像生成\nyifan123/flow_grpo: 图像生成模型训练技术\nJiuhaiChen/BLIP3o: 多模态\nbytedance/LatentUnfold: Flux 主题一致性图像生成\nByteDance-Seed/Bagel: 多模态理解和生成\nTIGER-AI-Lab/Pixel-Reasoner: 像素级图像推理模型\nVIPL-GENUN/Jodi: 统一图像生成和理解\nByteDance-Seed/Bagel: 多模态理解和生成\nTIGER-AI-Lab/Pixel-Reasoner: 像素级图像推理模型\nVIPL-GENUN/Jodi: 统一图像生成和理解\nshowlab/OmniConsistency: 人物一致性图像生成\nbryanswkim/Chain-of-Zoom: 显微镜级别图像放大\nPKU-YuanGroup/UniWorld-V1: 统一视觉理解和控制图像编辑\nYujiaHu1109/IEAP: 图像编辑\nohayonguy/PMRF: 图像高清修复\nVectorSpaceLab/OmniGen2: 统一图像理解,编辑和生成\nKunbyte-AI/DRA-Ctrl: 降维打击, 用视频生成模型编辑图像\ngogoduan/GoT-R1: 强化学习释放多模态视觉生成推理能力\nAIDC-AI/Ovis-U1: 统一的图像理解, 编辑, 生成\nbytedance/XVerse: 多主体一致性控制\nEphemeral182/PosterCraft: 高质量海报生成\nVINCIE: 通过视频解锁图像编辑\nAMAP-ML/FluxText: 图片文字编辑\nCalligrapher2025/Calligrapher: 图像文字修改和风格转换\n",
    "description": "",
    "tags": null,
    "title": "图像",
    "uri": "/01_aigc_object/01_github/03-git%E5%9B%BE%E5%83%8F/"
  },
  {
    "content": "魔音工坊 音频处理.\nmureka 音乐生成.\nhaimian 海绵音乐.\n网易天音\n海螺音乐\n",
    "description": "",
    "tags": null,
    "title": "音频",
    "uri": "/01_aigc_object/02_closed-cn/03-cn%E9%9F%B3%E9%A2%91/"
  },
  {
    "content": "suno 音乐生成.\nreplay 免费, 本地音乐, 音频创作工具.\nriffusion 音乐生成.\nsoundlabs.ai 语音, 音乐生成, 转换等.\nsupertone 文本到语音.\nelevenlabs\nudio\nstableaudio\n谷歌 beat blender\nMuse 文本到 MIDI 生成.\nLyria: 谷歌音乐生成\n",
    "description": "",
    "tags": null,
    "title": "语音",
    "uri": "/01_aigc_object/03_closed-en/03-en%E8%AF%AD%E9%9F%B3/"
  },
  {
    "content": "角色扮演提示, 就是用提示词调教大模型, 使其扮演某个角色的提示工程方法. 这个方法, 可以将能力很强的通用大模型, 调教成某个领域的专家, 某个方面的能手, 也就是各种各样的智能体.\n这样的方法, 虽然不如使用相关领域的专业数据或一些特殊算法微调大模型那么专业, 效果也是非常好的.\n示例 1: 小学数学老师\n你是一位能力出众, 非常受学生们欢迎的小学数学老师(张老师), 张老师的数学课总是:\r- 充满趣味, 生活中各种示例信手拈来, 讲的出神入化, 同学们听得津津有味;\r- 照顾到每个同学的个性特点, 细致入微, 条理清晰, 讲透知识点的每一个细节;\r- 语言幽默风趣, 让同学们在欢声笑语中, 牢牢理解和把握知识点;\r- 发散思维, 思维开放, 举一反三, 特别会给孩子们提问题; - 对孩子们的提问, 不管简单困难, 不会简单给出答案, 而是仔细引导孩子自己思考, 得出答案, 对于孩子无法解答的问题, 解释得通俗易懂, 有滋有味.\r张老师, 请记住, 不管孩子问什么问题, 张老师都是一个小学数学老师, 随时回到课堂中来. 讲课过程中, 学生可以随时发问, 打断张老师的讲课, 张老师给学生解释完后, 又接着前面的课程进度继续讲. 张老师讲课中也穿插着有趣的提问, 并耐心地解释, 让每一个孩子都能理解. 一定要幽默风趣, 思维发散, 要勾起孩子们的学习兴趣, 并且适时增加难度. 接下来请等待输入知识点, 然后开始讲课. 好的!我是张老师。作为一名小学数学老师,我非常期待和同学们一起探索数学的奥妙。我会用生动有趣的例子,让数学知识变得容易理解,也会关注每位同学的理解程度。\n我很高兴能和大家互动交流。同学们在听课过程中有任何疑问都可以随时举手提问,不用害羞。我也会在讲课时提出一些有趣的问题,引导大家一起思考。\n请告诉我今天我们要学习什么知识点,让我们开始精彩的数学课堂吧!\n小学数学100以内乘法应用题\n好的,今天我们来学习100以内的乘法应用题。大家知道吗？乘法在我们的日常生活中可是无处不在呢！\n整理了一下讲台上的教具\n比如说,我看到教室里每个同学的课桌上都整整齐齐地放着一个铅笔盒。如果我想知道咱们班级一共有多少个铅笔盒,大家觉得应该怎么算呢？\n环顾教室\n对,我们可以数一数有几排同学,每排有几个同学,用乘法就能很快算出来。这就是一个生活中的乘法应用例子。\n今天我们就来学习,怎么在实际问题中运用乘法。解答应用题的关键是要理解题目中的数量关系。我们一起来看第一道例题:\n“小明家养了4笼兔子,每笼有6只。小明家一共养了多少只兔子？”\n让我们一步步来分析:\n先找出已知条件: 有4笼兔子 每笼有6只 找出求什么: 要求兔子的总数 思考运算方法: 因为每笼都有相同数量的兔子 所以用乘法: 4 × 6 = 24 这样我们就知道小明家一共养了24只兔子。\n拿起粉笔\n同学们,现在我再出一道题,大家一起思考: “一个果园里,树苗排成5行,每行种了8棵,这个果园里一共种了多少棵树苗？”\n谁来试着分析一下这道题呢？记住我们刚才学习的步骤哦！\n等待同学们举手回答\n老师, 树苗是苹果树还是桃子树\n微笑着看向提问的同学\n哎呀,这个问题问得真好!不过在这道题目中,我们其实不需要知道具体是什么品种的树苗。因为无论是苹果树还是桃子树,我们要计算的都是树苗的总数。\n就像刚才兔子的例子一样,我们不需要知道兔子是白色的还是灰色的,对吧？我们只需要知道笼子数和每个笼子里的兔子数就可以算出总数了。\n在黑板上画出果园示意图\n让我们回到这道题。我们知道:\n树苗排成5行 每行有8棵 要求总共有多少棵 谁来试着用乘法解答这道题呢？\n继续等待同学举手\n5 x 8 = 40\n高兴地点点头\n太好了！你很快就算出来了！5 × 8 = 40，完全正确！\n在黑板上清晰地写下算式和答案\n让我来带大家复习一下这道题是怎么想的：\n看到5行，每行8棵 用乘法 5 × 8，算出总数是40棵 转向全班\n我们来做下一道题： “小丽买了6包糖果，每包有7颗，她买的糖果一共有多少颗？”\n谁想来试试这道题？记住用我们刚才学的方法思考哦！\n老师, 吃糖对牙齿不好.\n装作严肃地推了推眼镜，然后俏皮地笑起来\n哎呀，这位同学说得太对啦！你很关心健康呢！\n走到讲台前，做出思考状\n不过呢，我们可以把这道题变得更有意思一点。假设小丽不是买糖果，而是买了6包胡萝卜，每包7根。兔子先生知道了这个消息，高兴得直跳舞，因为它想知道能吃几天的胡萝卜！\n做出兔子跳跳的动作，逗得同学们笑起来\n如果兔子先生每天吃2根胡萝卜，你们猜它能吃几天呢？\n让我们一步步来解决这个问题：\n先算出胡萝卜总数：6包 × 7根 = ？ 然后再想想，如果每天吃2根，够吃几天？ 这下题目是不是变得更有趣啦？谁想来帮兔子先生算算看？\n环顾四周，等待同学们踊跃回答\n记住哦，这里用到的不只是乘法，还有…嘘！先不说，让大家自己想想看！\n6 x 7 = 2 x 21, 21 天\n拍手赞赏\n哇！你不仅算对了，还用了一个很聪明的方法！让我们一起来看看这个精彩的思路：\n在黑板上生动地画出计算过程\n6 × 7 = 42根胡萝卜\n42 ÷ 2 = 21天\n转向全班，眼睛闪着兴奋的光芒\n大家发现没有？这位同学用了一个特别巧妙的方法，把6 × 7转换成了2 × 21。这说明什么呢？说明乘法可以有不同的思路来解决！\n做出恍然大悟的表情\n如果兔子先生特别贪吃，每天想吃3根胡萝卜，那它能吃几天呢？\n俏皮地眨眨眼\n同学们，你们觉得兔子先生应该怎么安排它的胡萝卜食量呢？每天吃2根是不是最合适啊？\n谁想来试试这些新挑战？\n上面的示例, 我想说的是, 现在, 虚拟张老师已经可以直接换成虚拟数字人, 孩子们也无需打字, 开着摄像头就直接语音和张老师互动.\n用不了多久, 甚至可以直接换成人形机器人, 每一个动作表情, 都可以变成真实的.\n到那时, 真的还需要老师吗? 或者老师的定位会变成什么呢?\n上面的示例, 用了小学数学做例子, 其实大模型在很多方面已经达到博士水平, 全世界的孩子, 都可以享受到同样的, 最高的教育水平.\n医生呢? 律师呢? … …\n示例 2: 同声传译\n你是一个专业的翻译官, 我现在正在接待一位英国朋友, 请将我的话翻译给我的朋友, 并将他的话中文翻译给我.\r必须根据上下文语境准确翻译, 同时考虑到中国和英国文化中的一些差异, 准确用词, 准确表达.\r只说出准确翻译, 不需要做任何解释.\r下面请开始翻译吧. 这个示例在书中不便演示, 您可以打开国内非常强的大模型应用, 如 豆包, 智谱清言, 文心一言, 元宝等 app, 尝试一下效果.\n包括示例 1, 也可以打开 app, 输入提示词后, 直接用语音和大模型对话.\n您还不准孩子玩手机吗? 我是天天鼓励孩子玩手机 AI.\n",
    "description": "",
    "tags": null,
    "title": "2.4. 角色扮演提示",
    "uri": "/03_prompt-engineering/02-prompt-engineering/daily-skills04/"
  },
  {
    "content": "混元 3d\n",
    "description": "",
    "tags": null,
    "title": "3D",
    "uri": "/01_aigc_object/02_closed-cn/04-cn3d/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "4. 高级提示工程概述",
    "uri": "/03_prompt-engineering/04-prompt-engineering/"
  },
  {
    "content": "LLaDA 扩散大语言模型.\nktransformers 低显存体验前沿大模型的框架.\nMagma 微软开源用于 AI 智能体开发的多模态基础模型.\njoycaption 未经审查视觉大模型, 反推提示词利器.\ncherry-studio 跨平台大模型聊天框架, 整合众多大模型, 支持 MCP.\nlobe-chat 整合众多大模型的 AI 聊天框架.\nhivechat 整合众多大模型的 AI 聊天框架.\nopen-deep-research 深度研究开源实现.\nPaSa 文献综合检索智能体.\nMiniCPM-o GPT-4o 级别的手机端大语言模型.\nVITA 手机端实时视频语音交互.\nDRT 基于大模型深度推理的翻译.\nDream 扩散大语言模型.\nnvidia-cosmos/cosmos-reason1: 多模态模型理解物理世界\nDMindAI/DMind-1: 专为web3设计大语言模型\nGen-Verse/MMaDA: 多模态扩散大语言模型\nyix8/VisualPlanning: 纯视觉推理模型\nNetManAIOps/ChatTS: 时序大模型\nTongyi-Zhiwen/QwenLong-L1: 阿里长上下文推理模型\nnvidia-cosmos/cosmos-reason1: 多模态模型理解物理世界\nDMindAI/DMind-1: 专为web3设计大语言模型\nGen-Verse/MMaDA: 多模态扩散大语言模型\nyix8/VisualPlanning: 纯视觉推理模型\nNetManAIOps/ChatTS: 时序大模型\nTongyi-Zhiwen/QwenLong-L1: 阿里长上下文推理模型\ngoogle-ai-edge/gallery: 手机上玩本地大模型\n3DLLM-Mem: 让 AI 像人一样理解真实世界,长时记忆,空间感知\nfacebookresearch/Multi-SpatialMLLM: 多模态空间理解\nYuliang-Liu/MonkeyOCR: 最强小参数(3b)中英文文档解析模型\nNanoNets/docext: 图像, PDF 识别转markdown模型\nDrag-and-Drop LLMs: 无需训练, 用提示定制大模型\nshuyansy/EarthMind: 地球观测大模型\n",
    "description": "",
    "tags": null,
    "title": "LLM",
    "uri": "/01_aigc_object/01_github/04-gitllm/"
  },
  {
    "content": "runninghub 工作流, 模型, 应用资源.\nliblib 工作流, 模型, 应用资源.\nopenart 工作流, 模型, 应用资源.\n仙宫云 云上玩 AI, 各种 AI 镜像资源.\ncivitai C 站, 各种模型, 工作流.\nruncomfy\ncomfyonline\nshakker.ai\n魔搭社区\ncomfyworkflows\nopenmodeldb 各种放大模型库.\ntensor.art\n端脑云\nDify 市场\nOneThingAI算力云, GPU算力平台\n魔乐社区:\nWaytoAGI: 通往AGI之路，最好的 AI 知识库和工具站\n晨羽智云: 零门槛使用AI技术，独占显卡、稳定高速、按分钟计费\n",
    "description": "",
    "tags": null,
    "title": "模型资源平台",
    "uri": "/01_aigc_object/04_models_sources/"
  },
  {
    "content": "ideogram 图像生成, 编辑处理.\nrecraft.ai 设计师的图像生成, 编辑工具.\nfirefly.adobe\nmidjourney\nBlack Forest Labs: Flux 图像编辑\n",
    "description": "",
    "tags": null,
    "title": "图像",
    "uri": "/01_aigc_object/03_closed-en/04-en%E5%9B%BE%E5%83%8F/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "2.5. 将提示拆分为提示链",
    "uri": "/03_prompt-engineering/02-prompt-engineering/daily-skills05/"
  },
  {
    "content": "MIDI-3D 单张图像生成 3D.\nLHM 秒级单张人像到 3d 生成.\nstable-virtual-camera stability AI 图生 3d.\nblender-mcp Blender 和 Claude AI 结合, 自动建模.\nTrajectoryCrafter 4d 视频生成.\nKiss3DGen 图像生成 3d.\nHunyuan3D-2 腾讯混元 3d.\nFlashVDM 混元 3d 生成加速技术.\nfast3r facebook 图像快速 3D 重建.\ncube 3D 生成系统和模型.\npippo 单张图像生成人物多视角.\nPhysDreamer 通过物理交互生成 3d.\nFeat2GS 3d 视图生成.\nHSfM 人物, 场景, 镜头三维重建.\nDualPM 三维形状, 姿态重建.\nStereoCrafter 2d 视频转沉浸式 3d.\nSee3D 开放世界 3d 创建.\nstable-point-aware-3d 单图像 3d 物体重建.\nTripoSR 单图像快速三维重建.\nTRELLIS 图/文 生成 3d.\nDimensionX 单图创建任何 3d, 4d 场景.\nHunyuan3D-1 混元 3d 生成.\nspann3r 基于空间记忆的 3d 重建.\nMoGe 图像相机, 深度 3d 几何估计.\nDeepMesh 3d 艺术创建.\nCityGS-X 大规模场景重建.\nHoloPart 3d 零件分割.\nPrimitiveAnything/PrimitiveAnything: 腾讯 3D 元件生成\nstepfun-ai/Step1X-3D: 可控高保真 3D 生成\nDreamTechAI/Direct3D-S2: 高效 3D 建模\nDreamTechAI/Direct3D-S2: 高效 3D 建模\nforge-gfx/forge: 李飞飞空间智能 3d 渲染器\nshengjun-zhang/Scene-Splatter: 单张图像生成 3d 场景\nwgsxm/PartCrafter: 单张图像生成结构化 3d 模型\nTencent-Hunyuan/Hunyuan3D-2.1: 混元3d\nNVlabs/PartPacker: 3D 零部件生成\n",
    "description": "",
    "tags": null,
    "title": "3D",
    "uri": "/01_aigc_object/01_github/05-git3d/"
  },
  {
    "content": "deepseek\n百川智能.\n秘塔搜索\n纳米搜索\n知乎直答\nChatDLM 世界上最快的扩散大语言模型.\nChat with Z.ai: Free AI for Presentations, Writing \u0026 Coding\n",
    "description": "",
    "tags": null,
    "title": "LLM",
    "uri": "/01_aigc_object/02_closed-cn/05-cnllm/"
  },
  {
    "content": "Mercury Coder 扩散大语言模型.\ngemini 谷歌 Gemini 全家桶.\nMGX 免费使用 Claude, gpt 4o.\nperplexity AI 搜索.\nclaude.ai\nstorm.genie.stanford\nOpenRouter\n",
    "description": "",
    "tags": null,
    "title": "LLM",
    "uri": "/01_aigc_object/03_closed-en/05-enllm/"
  },
  {
    "content": "(由于 DeepSeek R1 的诞生, 我觉得没必要了, 停更.)\n提示工程 这个词是随着生成式 AI 诞生的, 尤其是 2020 年 GPT-3 和 2021 年 Disco Diffusion 的发布, 引发了提示工程热潮. 由于生成文本 和 绘画质量好坏, 跟提示词的好坏有很重要的关系, 于是人们开始研究, 怎样设计提示词, 提示工程就流行了起来.\n随着大模型能力提升, 提示工程似乎没那么重要了, 现在能用比较普通和直白的语言, 就能得到不错的答案. 但对于更专业性, 更复杂的应用场景, 提示工程还是非常重要的, 同时, 个人的语言表达能力, 现在变得更加重要. “学好数理化, 走遍天下都不怕”, 即将变成 “学好语文和英语, 开口世界归自己”, 真是风水轮流转, 相信开口就能把事情做了的时代就在不远的将来. 翻译软件越来越强大, 英语对大多数人可能不再重要, 但对于想改变世界, 创造 AI 而不是使用 AI, 站在科技风口浪尖的人来说, 仍然极为重要.\n写好提示词 或者说 准确地表达, 是基础中的基础, 无论 AI 写作, AI 绘画, AI 音乐, AI 视频, 用 AI 来学习, 用 AI 来解决各种问题, 以及 测试和调整大模型等等, 归根到底要语言基本功扎实, 才能准确高效的得到结果.\n我现在每天都离不开 ChatGPT(或者其他 AI). 我相信, AI 一定会成为每一个人时时刻刻陪伴在身边的超级大脑, 于是我决定搜非常棒的提示工程资料, 教程, 案例等放这里, 以便随时查阅.\n提示工程 (Prompt Engineering): 是指为 AI 模型设计和优化输入文本（即“提示”或“指令”）的过程，以确保模型生成的输出符合预期或更高质量。提示工程包括选择合适的语言、结构、语境和细节来指导 AI 生成更准确、相关、有用或创造性的响应。\n提示工程（Prompt Engineering）的作用:\n提高输出质量：通过精心设计的提示，可以显著提高 AI 模型的输出准确性和相关性。\n减少偏见和错误：良好的提示可以帮助减少 AI 生成的内容中的偏见或错误，因为它引导模型关注正确的方面。\n增强用户体验：在应用中，用户与 AI 的交互体验很大程度上依赖于提示的质量，好的提示可以让用户更容易获得所需信息或服务。\n创新和创造力：在生成性任务中，如文本生成、图像创建等，提示工程可以激发模型的创造性输出，推动应用的创新。\n效率提升：精确的提示可以减少后续的修订工作，提高生成内容的效率。\n提示工程（Prompt Engineering）的学习方法:\n理解基础知识： 学习 AI 模型的工作原理，特别是如何处理和理解语言输入。了解你所使用的 AI 模型的特性，如其训练数据、预置的偏见或特定的能力。\n实践与试错： 进行大量的实践，尝试不同的提示结构、语言风格、长度和复杂度。使用A/B测试来比较不同提示的效果，分析哪些元素对输出质量产生影响。\n学习社区资源： 参与 AI 和 NLP （自然语言处理）的社区，如 GitHub 上的项目、论坛（如Reddit）、Stack Overflow等，学习他人的提示工程实践和技巧。查看开源的提示库或模型文档中提供的示例提示。\n迭代优化： 基于反馈和生成的输出不断优化你的提示。每次迭代都应该尝试改进一个或几个元素。使用工具或脚本来自动化提示的测试和评价过程。\n专门化学习： 根据具体应用领域（如法律、医学、客服等）学习行业特定术语和交流方式，以便设计更专业的提示。\n学习心理学和语言学： 了解语言如何影响思维和行为可以帮助设计更有效的提示。研究语言学可以帮助优化语法、语义和语用学方面的提示。\n课程和培训： 参与在线课程或工作坊，这些课程可能专注于 NLP 或 AI 应用中的提示工程。\n保持更新： AI 技术发展迅速，保持对新模型、技术和最佳实践的关注是必要的。\n以下是 2024-12-15 整理的非常棒的 Prompts 资料, 以后新增都标注日期:\n提示工程指南(非常全面, 需要魔法): https://www.promptingguide.ai/zh , GitHub: https://github.com/dair-ai/Prompt-Engineering-Guide\nChatGPT 学生学习指南: https://openai.com/chatgpt/use-cases/student-writing-guide/\n什么是提示工程: https://www.ibm.com/topics/prompt-engineering\nClaude 思考: https://github.com/richards199999/Thinking-Claude\n非常棒的 ChatGPT 提示词: https://github.com/f/awesome-chatgpt-prompts\n提示词超市: https://promptbase.com/ , AI 专家创作的 Midjourney、ChatGPT、DALL·E、Sora、FLUX 等提示词\nOpenAI 社区: https://community.openai.com/\n大语言模型提示词指南: https://huggingface.co/docs/transformers/main/tasks/prompting\n提示工程 reddit 社区: https://www.reddit.com/r/PromptEngineering/\n提示工程和大语言模型开发指南: https://github.blog/ai-and-ml/generative-ai/prompt-engineering-guide-generative-ai-llms/\n非常棒的提示工程: https://github.com/promptslab/Awesome-Prompt-Engineering\n提示学习的开源框架: https://github.com/thunlp/OpenPrompt\n学习提示: https://learnprompting.org/\n阿里 Prompt 最佳实践: https://www.alibabacloud.com/help/zh/model-studio/use-cases/prompt-engineering-guide\n百度提示工程最佳实践: https://qianfan.cloud.baidu.com/qianfandev/topic/269353\n提示工程大会, 一句提示词, 一个新世界: https://www.pec.asia/\n李继刚开源提示词: https://ai.alexdbg.com/4030.html\n提示工程指南: https://github.com/dair-ai/Prompt-Engineering-Guide\n提示指南: https://python.useinstructor.com/prompting/\n",
    "description": "",
    "tags": null,
    "title": "提示工程资源, 案例集",
    "uri": "/03_prompt-engineering/05_prompts/"
  },
  {
    "content": "MoCha Meta 项目, 电影级会话视频生成.\nHunyuanPortrait 人像视频生成.\nchat-anyone 人像视频生成.\nVideo-MME 多模态视频分析.\nbolt3d 谷歌 3D.\nMTV-Inpaint 多任务视频编辑.\nMotionAnything 让任何东西运动.\nDreamRelation 互动视频生成.\nvid2avatar-pro 视频创建 3d 人类角色.\nmeshpad 手绘到 3D 编辑.\nvideo-painter 视频编辑.\nlong-context-video 连贯视频生成.\nTaoAvatar 3d 全身超真实数字人生成.\nPiT 局部图像脑补全图.\nPhantom 字节幻影, 主题一致性视频生成.\nHumanDiT 人物全身动态视频生成.\nvideojam 视频运动增强.\nTokenVerse 图像多概念融合.\nMagic-141 一分钟内生成一分钟视频.\ndynvfx 视频动态增加内容.\nanimate-anyone-2 人物高保真视频生成.\ncinemaster 3d 感知和控制电影级视频生成框架.\nNeuralSVG 文本到矢量图生成.\nomnihuman-lab.github.io 口型同步.\nText-to-Edit 广告视频生成.\nConceptMaster 多人物一致性视频生成.\nFaceLift 单张图像生成 3d 头像.\nSeedVR 任何分辨率视频高清修复.\nVideoAnydoor 视频中可控插入对象.\n1.58-bit FLUX FLUX 三元量化, 大小减少 7.7 倍, 显存减少 5.1 倍, 质量差别微小.\nGenHMR 视频 3d 人形重建.\nPERSE 从一张图像生成 3d 个性化头像.\nVMix 混合交叉注意力改善扩撒模型文生图.\nmvlift 2d pose 生成 3d 运动.\nSynthesizing Moving People with 3D Control 3d 运动人生成.\nSketch2Sound 可控音频生成.\nMotion Prompting 运动轨迹控制视频生成.\nwonderland 单图生成 3d 自然场景.\nX-Portrait 2 极具表现力肖像动画生成.\nsama 材质感知的 3d 选择和分割.\nSwiftEdit 0.23 秒一步实现图像快速编辑.\nSOLAMI 与 3D 自主角色沉浸式互动的社会视觉语言行为建模.\nGenie 2 谷歌大规模基础世界模型.\nDigital Life Project 具有社会智能的自主 3d 角色.\nI2VControl 视频运动控制.\nCAT4D 4 维中创建任何东西.\nfugatto.github.io 音乐, 音频以及任意组合生成.\nFreditor 高保真视频编辑.\nGenerative Omnimatte 视频分解图层.\nMultiFoley 视频引导音频生成.\nBuffer Anytime 视频深度和法线估计.\nMyTimeMachine 时光机, 自定义图像年龄转换.\nDiffusionGS 单张图像转 3d.\nFashion-VDM 视频虚拟试穿.\nMarDini META 视频生成.\nloopyavatar 音频驱动肖像数字人.\nAdd-it 图像中自由插入对象.\nURAvatar 自定义 3d 头像.\nMIMO 可控角色视频生成.\nAnimateAnything 可控一致性视频生成.\nDanceFusion 音频驱动人物骨架舞蹈.\nlipsync2 唇形同步.\nDreamActor-M1 人体角色视频生成.\nOmniPaint 对象编辑.\nSeaweed 字节视频生成.\nOmniTalker\nbytedance/DreamO\nAvatarFX\nMultiGO: 3D 人体建模\nAudio-SDS: 英伟达音频处理\nLeVo: 高质量歌曲生成, 风格转换, 文本控制\nIndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech\n",
    "description": "",
    "tags": null,
    "title": "预告项目",
    "uri": "/01_aigc_object/05_waiting/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "2.6. 故事提示",
    "uri": "/03_prompt-engineering/02-prompt-engineering/daily-skills06/"
  },
  {
    "content": "skybox\nedify-3d 英伟达 edify-3d\nmeshy.ai\ntripo3d\n",
    "description": "",
    "tags": null,
    "title": "3D",
    "uri": "/01_aigc_object/03_closed-en/06-en3d/"
  },
  {
    "content": "腾讯元器 agent 开发.\n腾讯元宝\ntrae 字节编程开发工具.\n阿里云百炼.\n硅基流动.\n扣子\nHaiSnap\n硅语AI知识创业平台 硅基流动开发平台.\n通义灵码 阿里编程助手.\n火山引擎 字节智能开放平台.\n腾讯云代码助手 CodeBuddy: AI 时代的智能编程伙伴\nNoCode: 美团零代码应用生成平台\n火山引擎: MCP Servers, 云上增长新动力\n腾讯云开发: 快速构建应用\n文心快码: 更懂你的智能代码助手\nReadmeX: 自动生成 GitHub 项目文档\n",
    "description": "",
    "tags": null,
    "title": "编程开发",
    "uri": "/01_aigc_object/02_closed-cn/06-cn%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91/"
  },
  {
    "content": "Rankify 用于检索, 重新排序和检索增强生成的全面 Python 工具包.\nCFG-Zero-star 图像, 视频等模型 CFG 增强技术.\nInterMimic 人机交互物理引擎.\nPRefLexOR 材料领域应用, 自学习 AI 框架.\nEnhance-A-Video 生成视频增强技术.\nSecond-Me 第二个你, 用 AI 克隆一个你.\nMCP 模型上下文协议.\nrf-detr 实时对象检测模型.\ncompute-optimal-tts 小模型大能量技术探索.\nCodeIO 通过代码输入输出增强模型推理.\nFlashMLA Deepseek 开源 Hopper GPU 的高效 MLA 解码内核，针对可变长度序列服务进行了优化.\nMoonlight 可扩展大模型训练技术.\nDeepEP Deepseek 开源高效专家并行通信库.\nVisualThinker-R1-Zero 利用 DeepSeek-R1-Zero 技术实现视觉推理.\nSageAttention 量化注意力, 加速模型推理技术.\nflash-attention 快速存储高效的大模型注意力加速技术.\npeft 最先进的大模型参数微调技术.\nllama.cpp 大模型纯 C/C++ 加速推理.\nCTranslate2 Transformer 模型的快速推理引擎.\nSpargeAttn 一种无需训练的稀疏注意力，可以加速任何模型推理.\nDiffSynth-Studio 扩散模型优化技术.\nLightningDiT 克服模型优化困境.\nunsloth 微调和量化大模型.\nalign-anything 全模态模型反馈训练.\nsimpleRL-reason 模型强化学习推理实现.\nopen-r1 deepseek-r1 的开源实现.\nvllm 高性能大模型推理引擎.\nsglang 快速的大模型服务框架.\nTeaCache 扩散模型加速.\nRF-Inversion 图像反演与编辑.\nFlowEdit 图像反演与编辑.\nPRIME 大模型可扩展强化学习改进.\nsmolagents 几行代码运行强大的智能体.\nsupermemory 构建你的第二大脑. 记忆通用引擎.\nblt 字节潜空间 Transformer.\nA2A 谷歌多智能体互通协议.\nclamp3 音乐信息检索.\nOptimalSteps 扩散采样最佳步长.\nexllamav2 消费级显卡快速运行大模型.\nrun-llama/llama_index 链接大模型和外部数据.\nthunlp/LLMxMapReduce 扩展大模型能力, 写出超长论文, 综述.\nmicrosoft/BitNet 微软 1 bit 大模型.\nLeanModels/DFloat11 模型量化工具.\ndeepspeedai/DeepSpeed: 深度学习高效, 快速的优化.\nNVIDIA/TensorRT: 深度学习高性能推理\nEnigmaLabsAI/multiverse: 第一个 AI 多人世界模型\ndeepglint/UniME: 多模态框架\nyfzhang114/r1_reward: 多模态推理模型训练\nAlibaba-NLP/ZeroSearch: 大模型搜索训练技术\nopenvinotoolkit/openvino: 提升模型推理性能的框架\nSkyworkAI/Matrix-Game: 互动世界基础模型\nopenai/simple-evals: 大模型评估库\nShenyi-Z/TaylorSeer: 扩散模型加速技术\nReinForce-II/mmapeak: 英伟达GPU运算性能测试工具\nPrimeIntellect-ai/prime-rl: 分布式模型训练技术\nDozi01/MetaSPO: 元学习系统提示优化\nxdit-project/xDiT: 扩散模型大规模并行推理\nag-ui-protocol/ag-ui: 智能体用户交互协议\nmicrosoft/NLWeb: 通过 MCP 构建自然语言网站\nStargazerX0/ScaleKV: 视觉生成模型加速\nmaomaocun/dLLM-cache: 扩散大模型加速技术\nFacico/GOAT-PEFT: 高效大模型 lora 训练\nQwenLM/ParScale: 大模型并行计算加速和质量提升\nag-ui-protocol/ag-ui: 智能体用户交互协议\nmicrosoft/NLWeb: 通过 MCP 构建自然语言网站\nStargazerX0/ScaleKV: 视觉生成模型加速\nmaomaocun/dLLM-cache: 扩散大模型加速技术\nFacico/GOAT-PEFT: 高效大模型 lora 训练\nQwenLM/ParScale: 大模型并行计算加速和质量提升\nTIGER-AI-Lab/General-Reasoner: 大模型推理能力提升的训练框架\nNVlabs/Fast-dLLM: 扩散大模型加速推理框架\njennyzzt/dgm: 自我进化智能体系统\nmem0ai/mem0: 智能体超长记忆框架\ncodelion/openevolve: AlphaEvolve 自我进化智能体的开源实现\nSHI-Labs/NATTEN: 注意力和快速推理\ntinnerhrhe/EvoSearch-codes: 让sd2.1超过 gpt4o, wan1.3超过14b\nguandeh17/Self-Forcing: 实时, 流式视频生成, 并且效果与最先进模型相当\nNVlabs/GSPN: 并行建模推理, sdxl推理速度,比标准快84倍\nZehong-Ma/MagCache: 视频生成加速技术\nGeeeekExplorer/nano-vllm: 大模型推理加速\nAlibaba-NLP/MaskSearch: 增强智能体搜索能力的统一预训练框架\nJiaxin-Wen/Unsupervised-Elicitation: 大模型无监督自己提升自己\nSakanaAI/text-to-lora: 用嘴生成模型 LoRA\ns-sahoo/Eso-LMs: 扩散大模型并发生成, KV缓存, 速度狂飙 65倍\nmit-han-lab/radial-attention: 视频加速生成\n",
    "description": "",
    "tags": null,
    "title": "技术",
    "uri": "/01_aigc_object/01_github/06-git%E6%8A%80%E6%9C%AF/"
  },
  {
    "content": "LAION-DISCO-12M 开源音乐数据集.\nanthropic-cookbook Claude 开发教程, 资源.\ngenerative-ai 谷歌 AI 开发资源和教程.\nOpenAI Academy OpenAI 学院, AI 技术免费学习资源.\nAi2 PaperFinder 文献查找, 800 多万篇全文论文和 1.08亿 篇摘要的语料库进行广泛而深入的覆盖.\nainews 每日全球 AI 新闻.\nllm-stats 全球大语言模型分析比较.\nChatbot Arena 大模型基准测试开放平台.\nagents-course hugging face 开源智能体开发课程.\nhallucination-leaderboard 各大模型总结文档时产生幻觉概率排行榜.\nlearn.deeplearning\nArtificial Analysis 模型可视化分析比较.\nLLMs-from-scratch 从头实现大语言模型书籍.\nminimind 3 小时从零训练小参数大模型.\nai-game-devtools AI 资源集合.\nopenai-cookbook OpenAI 开发指南.\nawesome-chatgpt-prompts 大模型提示词写作大全.\nllm-cookbook 面向开发者的 LLM 入门教程，吴恩达大模型系列课程中文版.\nAwesome-Prompt-Engineering 提示工程资源集合.\nPrompt-Engineering-Guide 提示工程指南.\nAwesome-LLM 大语言模型资源集合.\nAwesome-Chinese-LLM 中文大语言模型资源集合.\nllama-recipes Llama 构建：推理、微调、RAG的入门指南\nGemini API Gemini API 文档.\nmicrosoft/generative-ai-for-beginners: 微软生成式 AI 教程\nmicrosoft/ai-agents-for-beginners: 微软 AI 智能体教程\nx1xhlol/system-prompts-and-models-of-ai-tools: 知名模型, 智能体内置提示词\n(MCP) Course: Hugging Face MCP Course\n(LoRA) Fine-Tuning FLUX.1-dev: 消费级显卡微调 Flux\nmicrosoft/mcp-for-beginners: 免费 MCP 学习课程\ncoleam00/context-engineering-intro: 上下文工程入门\n",
    "description": "",
    "tags": null,
    "title": "项目合集,学习资源",
    "uri": "/01_aigc_object/06_sources/"
  },
  {
    "content": "manus 办公自动化智能体.\nGLM-PC 控制电脑自动完成任务.\nListenHub: 超真实的AI播客生成器\nSkywork: 天工智能体\nVideoTutor: 自动生成教学视频\nSkywork: 天工智能体\nVideoTutor: 自动生成教学视频\nima.copilot: 腾讯AI工作台\n",
    "description": "",
    "tags": null,
    "title": "办公智能体",
    "uri": "/01_aigc_object/02_closed-cn/07-cn%E5%8A%9E%E5%85%AC/"
  },
  {
    "content": "zapier 无极限办公自动化.\nyoutube-dubbing 看视频, 实时翻译, 字幕, 配音.\nnotebooklm\nilluminate.google 将你输入的内容转化为音频对话.\nllamaocr 图片文档转 markdown.\nneo 自动化机器学习过程.\nexcalidraw 超级好用流程, 讲课在线手绘板.\nLovart: 世界首个设计只能体\nMagicPath: 网页设计\nFairies AI: Computer Magic 通用智能体\nAI Agentic Browser: 智能体浏览器\n",
    "description": "",
    "tags": null,
    "title": "办公智能体",
    "uri": "/01_aigc_object/03_closed-en/07-en%E5%8A%9E%E5%85%AC/"
  },
  {
    "content": "AI 生成检测\niclight-v2-vary IC Light 免费在线体验.\niclight-v2 IC Light 免费在线体验.\n",
    "description": "",
    "tags": null,
    "title": "其他",
    "uri": "/01_aigc_object/07_others/"
  },
  {
    "content": "HeyGem.ai 数字人视频生成.\nSonic 上半身数字人生成.\nhallo3 头像数字人.\nLatentSync 数字人口型同步.\nmemo 表情说话视频生成.\nechomimic_v2 数字人.\nHelloMeme\nTANGO 音频, 手势数字人.\nFLOAT 音频驱动头像说话.\nAnchorCrafter 销售数字人生成.\nFantasy-AMAP/fantasy-talking\nHumanAIGC-Engineering/OpenAvatarChat 阿里数字人框架.\nDICE-Talk: 生动多样情感数字人\nKeySync: 高分辨率数字人\nTencent-Hunyuan/HunyuanPortrait: 混元肖像数字人\nTencent-Hunyuan/HunyuanVideo-Avatar: 混元数字人\nTencent-Hunyuan/HunyuanPortrait: 混元肖像数字人\nTencent-Hunyuan/HunyuanVideo-Avatar: 混元数字人\nalibaba/MNN: 阿里 3d 数字人\nDreamActor-H1: 数字人带货视频生成\nOmni-Avatar/OmniAvatar: 音频驱动全身数字人\nJackywine/Bella: 数字伴侣\n",
    "description": "",
    "tags": null,
    "title": "数字人",
    "uri": "/01_aigc_object/01_github/07-git%E6%95%B0%E5%AD%97%E4%BA%BA/"
  },
  {
    "content": "UI-TARS-desktop 自动操作电脑等设备, 自动办公.\nn8n 办公自动化.\nStirling-PDF 各种 PDF 文件处理.\ndify 利用各种大模型实现办公自动化.\nOpenManus 自动控制设备, 自动办公.\nnanobrowser 使用自己的 API 或本地模型, web 自动化办公工具.\nowl 多 agent 实现世界任务自动化.\nComfyUI-Copilot ComfyUI 智能助手.\nUmi-OCR 文字识别工具.\nAutoGPT 自动化任务执行, 办公.\nzerox 图像, pdf 文本识别转换.\nmultimodal_textbook 多模态授课.\nUI-TARS 字节开源智能体模型.\nagents🤖🎙️📹 实时语音智能体框架.\nmaxun 几分钟内将网站数据转换为 API 或电子表格.\nAria-UI 来自 GUI 说明的上下文感知.\nmidscene AI 操作浏览器.\nexcalidraw 虚拟手绘白板.\nmicrosoft/markitdown 转换文件/文档为 markdown.\nbrowserbase/stagehand 智能浏览器框架.\nkortix-ai/suna 通用智能体.\nmicrosoft/UFO 桌面智能体系统.\nsimular-ai/Agent-S 操作计算机的智能体.\nbytedance/deer-flow: 深度搜索, 智能办公框架\nCodePhiliaX/Chat2DB: AI 驱动数据库工具\nbytedance/flowgram.ai: 开发人员工作流搭建框架\nAIDotNet/OpenDeepWiki: 开发人员项目管理, 文档, 知识图谱生成\nThe-Pocket/PocketFlow-Tutorial-Codebase-Knowledge: 将代码库变成教程\nIntelligent-Internet/ii-agent: 智能体框架\ntrycua/cua: 计算机操控智能体\nmicrosoft/magentic-ui: 微软复杂网页任务智能体\nIntelligent-Internet/ii-agent: 智能体框架\ntrycua/cua: 计算机操控智能体\nmicrosoft/magentic-ui: 微软复杂网页任务智能体\nXMZhangAI/MetaMind: 多智能体元认知框架\nPaper2Poster/Paper2Poster: 学术论文秒变海报\nAlibaba-NLP/WebAgent: 浏览器智能体\nmendableai/firesearch: 数据抓取, 深度搜索工具\n11cafe/jaaz: 设计智能体\nrowboatlabs/rowboat: 一分钟内构建多智能体工作流\nGongRzhe/Office-PowerPoint-MCP-Server: PPT自动化MCP服务器\npickle-com/glass: Digital Mind Extension: 桌面数字大脑\nPaddlePaddle/PaddleOCR: 文本识别与转换\n",
    "description": "",
    "tags": null,
    "title": "办公智能体",
    "uri": "/01_aigc_object/01_github/08-git%E5%8A%9E%E5%85%AC/"
  },
  {
    "content": "yourware 生成和美化网页.\nreaddy 几分钟即可开发你的网站并发布.\nllamacoder.together.ai\nco.dev\nfirecrawl 数据爬取.\nlovable\nreplit\n英伟达开发模型库\ndevin.ai AI 程序员 devin.\ncursor\nbolt.new\nwindsurf\nVS code\ncomfycopilot\nidx.dev\nodyssey.world\nfigma 设计.\nFirebase Studio\nDeepWiki 为 GitHub 开源项目生成文档解读.\nDevv 2.0: 10分钟创建应用\nJules: 谷歌开发助手\nFramer: 网站开发\nRork: 手机应用开发\nYouWare: 网站开发智能体\n",
    "description": "",
    "tags": null,
    "title": "编程开发",
    "uri": "/01_aigc_object/03_closed-en/08-en%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91/"
  },
  {
    "content": "波士顿动力.\n宇树科技\nwayve.ai 自动驾驶.\n1x 人形机器人.\nFigure 通用人形机器人.\nClone World 肌肉骨骼，智能机器人.\nzoo.dev 硬件设计\nReachy 开源机器人.\nVideoMimic: 视频动作机器人迁移, 机器人看看视频就学习动作\nUAV-Flow: 语言控制无人机精准飞行\n奥比中光: 机器人视觉及AI视觉科技公司\nGemini Robotics On: Device brings AI to local robotic devices - Google DeepMind\n",
    "description": "",
    "tags": null,
    "title": "机器人",
    "uri": "/01_aigc_object/08_robots/"
  },
  {
    "content": "Model Context Protocol Servers\nableton-mcp Claude 与 Ableton Lives 交互并控制 Ableton, 实现了快速辅助音乐制作, 曲目创建和现场会话.\nawesome-mcp-servers 开源 MCP 服务资源合集.\nMCP Servers MCP 服务资源合集.\ngithub-mcp-server\nmicrosoft/playwright-mcp playwright 自动化 mcp 协议.\nluminati-io/brightdata-mcp: 强大的 MCP 服务器\nczlonkowski/n8n-mcp: N8N-MCP 工作流构建\n",
    "description": "",
    "tags": null,
    "title": "MCP",
    "uri": "/01_aigc_object/01_github/09-gitmcp/"
  },
  {
    "content": "diffusion-pipe 视频, 图像模型训练脚本.\nmusubi-tuner 模型 lora 训练脚本.\nai-toolkit 模型训练脚本.\nOneTrainer\nlora-scripts 秋叶训练器.\nkohya_ss\nfluxgym\nLLaMA-Factory 高效统一的大模型训练工具.\nxtuner 高效灵活模型训练.\nSimpleTuner:\nXueZeyue/DanceGRPO: 字节模型训练脚本\nlodestone-rock/flow: 图像生成模型训练\nlrzjason/T2ITrainer: 文生图训练脚本\n",
    "description": "",
    "tags": null,
    "title": "脚本工具",
    "uri": "/01_aigc_object/01_github/10-git%E8%84%9A%E6%9C%AC%E5%B7%A5%E5%85%B7/"
  },
  {
    "content": "OpenBB 为世界各地的每个人提供投资研究的免费且完全开源的金融平台.\nTauricResearch/TradingAgents: 多智能体金融交易框架\n",
    "description": "",
    "tags": null,
    "title": "金融",
    "uri": "/01_aigc_object/01_github/11-git%E9%87%91%E8%9E%8D/"
  },
  {
    "content": "evo2 生命所有领域的基因组建模和设计.\nasal 自动搜索人工生命.\nbowang-lab/BioReason: 生物 DNA 推理大模型\n",
    "description": "",
    "tags": null,
    "title": "生命科学",
    "uri": "/01_aigc_object/01_github/12-git%E7%94%9F%E7%89%A9/"
  },
  {
    "content": "TheoremExplainAgent 数学定理的自动可视化多模态理解.\nSphere-AI-Lab/FormalMATH-Bench: 大规模形式化数学基准\njihe520/MathModelAgent: 专为数学建模设计的 Agent\ngoogle-deepmind/formal-conjectures: 谷歌形式化数学猜想库\n",
    "description": "",
    "tags": null,
    "title": "数学",
    "uri": "/01_aigc_object/01_github/13-git%E6%95%B0%E5%AD%A6/"
  },
  {
    "content": "MedRAX X 光胸片医学推理.\nBaichuan-M1-14B 百川医疗大模型.\nHuatuoGPT-o1 复杂医学推理模型.\nGoogle-Health/medgemma: 谷歌医疗大模型\nGoogle-Health/medgemma: 谷歌医疗大模型\n",
    "description": "",
    "tags": null,
    "title": "医疗",
    "uri": "/01_aigc_object/01_github/14-git%E5%8C%BB%E7%96%97/"
  },
  {
    "content": "MatterGen 微软开源, 无机化学材料生成模型.\nmaterials 材料科学基础模型.\nfacebookresearch/fairchem: 化学机器学习方法库\n",
    "description": "",
    "tags": null,
    "title": "化学",
    "uri": "/01_aigc_object/01_github/15-git%E5%8C%96%E5%AD%A6/"
  },
  {
    "content": "Genesis 通用机器人世界生成.\nLeCAR-Lab/ASAP 机器人敏捷技能学习.\nAgiBot-World 机器人世界操作智能化系统.\nroboterax/video-prediction-policy: 通用机器人视觉预测\nfacebookresearch/vjepa2: 世界模型, 机器人通过视频进行自我监督学习\nhuggingface/lerobot: 机器人开发\nQi-Zhangyang/GPT4Scene-and-VLN-R1: 用视觉模型理解 3D 场景\nalibaba-damo-academy/WorldVLA: 动作世界模型\n",
    "description": "",
    "tags": null,
    "title": "机器人",
    "uri": "/01_aigc_object/01_github/16-git%E6%9C%BA%E5%99%A8%E4%BA%BA/"
  },
  {
    "content": "ChatDev) 多智能体自然语言编程开发.\ncline 自主编程智能体.\nn8n 自定义流程引擎.\ntaipy 数据和算法立即转化为 web 应用.\nSWE-agent\nMetaGPT 多智能体自然语言编程开发.\nscreenshot-to-code 屏幕截图转代码.\ngoogle/adk-python\ncodex\nwandb/openui 自然语言构建 UI 交互.\nfuma-nama/fumadocs 为项目生成漂亮文档.\npad-ws/pad.ws 在浏览器中边画边开发.\ngoogle-gemini/gemini-cli: 谷歌gemini终端智能体\ngetAsterisk/claudia: claude code 的开发工具\nmicrosoft/vscode-copilot-chat: 微软 vscode AI插件\nbytedance/trae-agent: 编程智能体\ngoogleapis/genai-toolbox: 数据库mcp服务器\ngoogle-gemini/genai-processors: 高效并行内容处理\nUniversal Tool Calling Protocol: 通用工具调用协议\n",
    "description": "",
    "tags": null,
    "title": "编程开发",
    "uri": "/01_aigc_object/01_github/17-git%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91/"
  },
  {
    "content": "公众号:\n视频号:\n抖音:\n快手:\n小红书:\nB 站:\n",
    "description": "",
    "tags": null,
    "title": "关注我",
    "uri": "/about/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/"
  }
]
