[
  {
    "content": " AI 发展日新月异, 各种 项目/应用/工具/资源… 令人兴奋, 而且很多都是免费使用, 或者开源.\n内容主要分为 项目的官方网站, 项目的 GitHub 主页, 项目的 Hugging Face Space (可在线试用), AIGC 相关网站, 相关文档资料. 它们之间可能会有重叠的项目, 但非常少, 我做了去重, 确保唯一性.\n以下项目是截至 2024 年 12 月 9 日搜集整理并分类:\n项目的官方网站 项目的 GitHub 主页 Hugging Face Space AIGC 相关网站 相关文档资料 等待中的项目 后面新添加, 都会标注日期:\n项目官方网站 Fish Audio: Free Generative AI Text To Speech \u0026 Voice Cloning 2024-12-09\nGenerative Foundation Model - Amazon Nova - AWS 2024-12-09\nRunComfy: Top ComfyUI Platform - Fast \u0026 Easy, No Setup 2024-12-09\n提示工程指南 | Prompt Engineering Guide 2024-12-09\nPrompt Engineering Guide | Prompt Engineering Guide 2024-12-09\nHailuo AI Audio: Create lifelike speech 2024-12-09\nGitHub 项目 FunAudioLLM/CosyVoice: Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability. 2024-12-09\nFunAudioLLM/SenseVoice: Multilingual Voice Understanding Model 2024-12-09\nmodelscope/FunASR: A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc. 2024-12-09\nyformer/EfficientTAM: Efficient Track Anything 2024-12-09\njingyaogong/minimind: 「大模型」3小时完全从0训练26M的小参数GPT，个人显卡即可推理训练！ 2024-12-09\nkijai/ComfyUI-HunyuanVideoWrapper 2024-12-09\njianchang512/clone-voice: A sound cloning tool with a web interface, using your voice or any sound to record audio / 一个带web界面的声音克隆工具，使用你的音色或任意声音来录制音频 2024-12-09\ndair-ai/Prompt-Engineering-Guide: 🐙 Guides, papers, lecture, notebooks and resources for prompt engineering 2024-12-09\nmemoavatar/memo: Memory-Guided Diffusion for Expressive Talking Video Generation 2024-12-09\n1jsingh/negtome: Official Implementation for paper: Negative Token Merging: Image-based Adversarial Feature Guidance 2024-12-09\nmicrosoft/TRELLIS: Official repo for paper “Structured 3D Latents for Scalable and Versatile 3D Generation”. 2024-12-09\nFrancis-Rings/StableAnimator: We present StableAnimator, the first end-to-end ID-preserving video diffusion framework, which synthesizes high-quality videos without any post-processing, conditioned on a reference image and a sequence of poses. 2024-12-09\nHugging Face CosyVoice-300M · 创空间 2024-12-09\nChatTTS Speaker - a Hugging Face Space by taa 2024-12-09\nFlux Fill Outpainting - a Hugging Face Space by multimodalart 2024-12-09\nFlux.1-dev Upscaler - a Hugging Face Space by jasperai 2024-12-09\nFlux.1-dev Upscaler - a Hugging Face Space by Nymbo 2024-12-09\n等待中的项目 Muse 2024-12-09\nIntroducing Veo and Imagen 3 on Vertex AI | Google Cloud Blog 2024-12-09\nFLOAT 2024-12-09\nGenie 2: A large-scale foundation world model - Google DeepMind 2024-12-09\nSOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters 2024-12-09\nDigital Life Project 2024-12-09\nI2VControl: Disentangled and Unified Video Motion Synthesis Control 2024-12-09\nDualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction 2024-12-09\nfugatto.github.io 2024-12-09\nCAT4D: Create Anything in 4D with Multi-View Video Diffusion Models 2024-12-09\nSNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance 2024-12-09\nvision-xl 2024-12-09\n",
    "description": "",
    "tags": null,
    "title": "ACGC 项目资源集",
    "uri": "/01_aigc_object/"
  },
  {
    "content": "ComfyUI 的特色:\n节点化工作流程: ComfyUI 允许用户通过节点构建复杂的图像生成流程，每个节点执行特定的功能，例如加载模型、处理图像、应用效果等。这种方式提供了高度的灵活性和自定义能力。\n低显存需求: 与其他类似工具相比，ComfyUI 对显存的要求较低，可以在较低配置的设备上运行。\n快速生成: 优化后的内部流程使其生成图片的速度较快，性能优于一些传统的 WebUI。\n插件支持: ComfyUI 支持丰富的插件生态，用户可以通过插件扩展功能。活跃的开源社区提供了丰富的插件和资源，提升了 ComfyUI 的功能性和适用性。\n适用于复杂场景: 特别擅长处理长期复杂的大型项目，能够将多步骤操作简化为连贯的工作流。\n跨平台支持: ComfyUI 不仅支持 NVIDIA GPU，还能在 CPU 上运行，提供了更广泛的设备兼容性。\n创意自由: 为艺术家、设计师和 AI 研究人员提供了强大的工具，使他们能够实现更精细的图像生成控制和实验。\n学习与教育: 尽管有一定的学习曲线，ComfyUI 为教育和培训提供了良好的平台，可以帮助学生理解图像处理和生成的原理。\n效率提升: 通过简化和自动化复杂的工作流程，ComfyUI 能够显著提升工作效率，特别是在需要频繁生成或修改图像的场景中。\nComfyUI 学习资源集合 官网 ComfyUI 官方桌面版新手指南 ComfyUI Desktop User Guide 所有节点信息 ComfyUI Nodes Info 另一个非常非常棒的节点信息站 ComfyUI Node Documentation ComfyUI Community Manual ComfyUI 解説 (wiki ではない) 界面介绍 基本概念 内置节点 内置节点2 内置节点01 内置节点03 常用自定义节点 ",
    "description": "",
    "tags": null,
    "title": "ComfyUI 资源, 教程",
    "uri": "/02_comfyui/"
  },
  {
    "content": "默认工作流 长这样, 具体细节将在后面 基本概念 介绍.\n调节视图的组件 分别是 放大, 缩小 (这两个和鼠标中键滚动一样), 适应, 选择/平移 模式 (选择模式可选择节点, 平移模式拖动整个工作流移动, 快捷键是空格键, 按住空格键, 拖动鼠标就平移) 和 显示/关闭 连接线.\n“运行” 管理区域 从左到右分别是 管理器, 卸载模型, 释放, 分享, 队列, 批次, 停止, 清除排队, 显示logs/终端, 隐藏菜单:\n卸载模型 (将加载到显存/内存里的模型卸载)\n释放模型和节点缓存\n分享(将工作流分享出去)\n队列, 分别是 点击执行, 生成完成后执行, 改变工作流后执行:\n运行次数\n停止运行当前任务\n清除等待中的任务\n打开底部 logs 和 终端\n隐藏菜单\n管理器 Skip update check: 跳过更新检查\nDB: 配置检索 节点/模型 信息的位置。默认一天内的缓存, 如果设置为本地，则忽略通道，如果设置为通道（远程），则每次打开列表时都会获取最新信息。\nChannel: 配置通道，以便从自定义节点列表（包括缺失节点）或模型列表中检索数据。请注意，这里使用了本地缓存信息。\nPreview method: 配置在采样过程如何预览图像生成的过程。None 不显示最快, 其他有微小的影响.\nDefault UI: 设置桌面启动时在主菜单中显示的默认状态.\nShare: 隐藏主菜单中的共享按钮，或在点击时设置默认操作。例如, 在通过点击共享按钮共享时，配置默认共享站点.\nComponent: 加载工作流时，配置要使用的组件版本.\nDouble-Click: 设置双击节点标题区域时的行为.\nUnload models: 卸载模型\nCustom Nodes Manager: 管理节点\nInstall Missing Custom Nodes: 安装缺失节点, 如果工作流中显示有节点缺失, 点击这里将进入节点管理进行缺失节点安装.\nModel Manager: 模型管理, 这里管理模型, 例如安装, 卸载.\nInstall via Git URL: 通过 GitHub 链接安装节点.\nUpdate All Nodes: 更新所有节点.\nFetch Updates: 抓取更新.\nRestart: 重启\nCommunity Manual: 社区手册\nNodes Info: 所有节点的信息大全.\nFeatures/Updates News: 更新公告.\n帮助 ComfyUl Issues 提问 ComfyUI Docs 文档 Comfy-Org 社区 Feedback 反馈 Open DevTools 打开开发工具 Reinstall 重装 Open Folder 打开文件夹 Open Logs Folder 打开 logs 文件夹 Open Models Folder 打开模型文件夹 Open Outputs Folder 打开输出文件夹 Open Inputs Folder 打开输入文件夹 Open Custom Nodes Folder 打开节点文件夹 Open extra_model_paths.yaml 打开自定义模型路径文件, 可以将模型或节点放在指定的文件夹中, 然后在这个文件里配置. 编辑 Undo 撤销 Ctrl + Z Redo 重做 Ctrl + Y Refresh Node Definitions 刷新节点, 快捷键 R, 例如刚把模型放入文件夹, 刷新节点就可以加载了. Clear Workflow 清理工作流, 将工作流全部清掉, 选中节点, 可以用快捷键 Backspace 删掉节点. Clipspace: CLIP 空间, 目前只对图像有效, 绘蒙版时使用, 后面会详细介绍. 工作流管理 New 新建 Open 打开已有工作流, 快捷键 Ctrl+o Browse Templates 选择自带工作流模板 Save 保存工作流, 快捷键 Ctrl+s Save As 另存为 Export 导出工作流 Export(API) 导出(包含API) 文件管理 查看队列历史, 应用生成的图像 (右键点击图像选择), 或删除历史\n查看节点库. 鼠标放到节点上, 会显示节点信息, 以及参数介绍, 还可以标星方便使用.\n模型库, 查看模型. 点击模型会自动生成加载节点.\n工作流管理, 点击工作流直接打开, 右击可以重命名, 删除, 应用. 同样可以选择内建模板, 打开已有工作流或新建.\n设置 Comfy 设置 Dev Mode. 开发者模式\nEnable dev mode options (API save, etc.). 启用开发模式选项（API保存等）\nEdit Attention. 编辑 Attention\nCtrl+up/down precision. Ctrl+up/down 调节精度\nLocale. 本地化, 需要安装中文插件才能显示中文\nLogging. 打开/关闭 logging, 出错可以将 logs 发给大神们求助.\nEnable logging\nMenu. 菜单\nUse new menu. 使用新菜单, 可选顶部, 底部, 或关闭.\nSave legacy menu’s position. 保留旧菜单的位置\nModel Library. 模型库\nWhat name to display in the model library tree view. 在模型库中显示什么名称。\nSelect “filename” to render a simplified view of the raw filename (without directory or “.safetensors” extension) in the model list. Select “title” to display the configurable model metadata title. 选择“filename”以在模型列表中呈现原始文件名的简化视图（不带目录或“.safetensors”扩展名）。选择“title”以显示可配置的模型元数据标题。\nAutomatically load all model folders. 自动加载所有模型文件夹。\nIf true, all folders will load as soon as you open the model library (this may cause delays while it loads). If false, root level model folders will only load once you click on them. 如果为true，则打开模型库后将立即加载所有文件夹（这可能会导致加载时的延迟）。如果为false，则只有在您单击根级模型文件夹后才会加载它们。\nNode 节点\nShow experimental nodes in search. 在搜索中显示实验节点\nExperimental nodes are marked as such in the UI and may be subject to significant changes or removal in future versions. Use with caution in production workflows. 实验节点在UI中标记为这样，在未来的版本中可能会发生重大变化或被删除。在生产工作流程中谨慎使用.\nShow deprecated nodes in search. 在搜索中显示已弃用的节点\nDeprecated nodes are hidden by default in the UI, but remain functional in existing workflows that use them. 默认情况下，弃用的节点在UI中隐藏，但在使用它们的现有工作流中仍然可以正常工作。\nNode Search Box. 节点搜索框\nNumber of nodes suggestions. 建议的节点数量 Only for litegraph searchbox/context menu. 仅适用于 litegraph 搜索框/快捷菜单\nShow node frequency in search results. 在搜索结果中显示节点频率 Only applies to the default implementation. 仅适用于默认实现\nShow node id name in search results. 在搜索结果中显示节点 id 名称 Only applies to the default implementation.\nShow node category in search results. 在搜索结果中显示节点类别 Only applies to the default implementation.\nNode preview. 节点预览 Only applies to the default implementation.\nBETA: Node search box implementation. 节点搜索框实现. 可选默认, 或旧版. Node Widget. 节点组件\nWidget control mode. Controls when widget values are updated (randomize/increment/ decrement), either before the prompt is queued or after. 组件控制模式. 控制何时更新组件值(随机/增加/减少)，在提示词队列执行前或执行后\nTextarea widget spellcheck. 文本区域小部件拼写检查\nNode Input Conversion Submenus. 节点输入转换子菜单\nIn the node context menu, place the entries that convert between input/widget in sub-menus. 在节点快捷菜单中，将 输入/小部件 之间转换的条目放置在子菜单中\nQueue\nQueue history size. The maximum number of tasks that show in the queue history. 队列历史记录数。队列历史记录中显示的最大任务数\nQueue Button. 队列按钮\nBatch count limit. The maximum number of tasks added to the queue at one button click. 批次计数限制。一键添加到队列的最大任务数\nSettings. 设置\nBETA: Show extension panel in settings dialog. 在设置对话框中显示扩展面板\nTree Explorer.\nTree explorer item padding\nValidation. 验证\nValidate node definitions (slow). Recommended for node developers. This will validate all node definitions on startup. 验证节点定义（缓慢）。推荐给节点开发人员。这将在启动时验证所有节点定义\nValidate workflows. 验证工作流\nWindow. 窗口\nShow confirmation when closing window. 关闭窗口时显示确认\nWorkflow. 工作流\nSave and restore canvas position and zoom level in workflows. 在工作流中保存, 恢复 画布位置 和 缩放级别\nOpened workflows position. 打开工作流位置, 可选顶部或侧边栏\nPrompt for filename when saving workflow. 保存工作流时提示输入文件名\nRequire confirmation when clearing workflow. 清除工作流时需要确认\nBETA: Show missing models warning. 显示缺失模型警告\nShow missing nodes warning. 显示缺失节点警告\nSort node IDs when saving workflow. 保存工作流时对节点ID进行排序\nLiteGraph 设置 Canvas 画布\nAlways snap to grid. 始终捕捉到网格\nSnap to grid size. When dragging and resizing nodes while holding shift they will be aligned to the grid, this controls the size of that grid. 对齐网格大小。按住shift键拖动和调整节点大小时，它们将与网格对齐，这控制了网格的大小\nEnable fast-zoom shortcut (Ctrl + Shift + Drag). 启用快速缩放快捷方式（Ctrl+Shift+拖动）\nShow graph canvas menu. 显示图形画布菜单\nCanvas zoom speed. 画布缩放速度\nShow canvas info on bottom left corner (fps, etc.). 在左下角显示画布信息（fps 等）\nGraph\nLink Render Mode. 连接线渲染模式 (曲线, 直线等)\nGroup 组\nDouble click group title to edit. 双击组标题进行编辑\nGroup selected nodes padding. 组边距\nLink 连线\nLink midpoint markers. 连线中点标记 (圆点, 箭头)\nLink Release. 连线松开\nAction on link release (Shift). 按住 Shift 松开连线\nAction on link release (No modifier). 不按 Shift 松开连线\nMenu 菜单\nInvert Context Menu Scrolling. 反转快捷菜单滚动\nNode 节点\nEnable DOM element clipping (enabling may reduce performance). 启用DOM元素剪裁（启用可能会降低性能）\nMiddle-click creates a new Reroute node. 单击鼠标中键可创建新的 “转接” 节点\nKeep all links when deleting nodes. When deleting a node, attempt to reconnect all of its input and output links (bypassing the deleted node). 删除节点时保留所有链接. 删除节点时，尝试重新连接其所有输入和输出链路（绕过已删除的节点）\nSnap highlights node. When dragging a link over a node with viable input slot, highlight the node. 捕捉高亮节点. 在拖动线经过具有可输入点的节点时，突出显示该节点\nAuto snap link to node slot. 自动捕捉可连接线到节点插槽\nEnable Tooltips. 启用工具提示\nNode life cycle badge mode. 节点激活标记\nNode ID badge mode. 节点 ID 标记\nNode source badge mode. 节点来源标记\nDouble click node title to edit. 双击节点标题编辑\nNode Widget\nFloat widget rounding decimal places [0 = auto]. (requires page reload). 浮点控件四舍五入小数位数[0=auto]。（需要重新加载页面）\nDisable default float widget rounding. (requires page reload) Cannot disable round when round is set by the node in the backend. 禁用默认浮点控件舍入。（需要重新加载页面）当舍入由后端节点设置时，无法禁用舍入\nDisable node widget sliders. 禁用节点小部件滑块\nPreview image format. When displaying a preview in the image widget, convert it to a lightweight image, e.g. webp, jpeg, webp;50, etc. 预览图像格式. 在图像小部件中显示预览时，将其转换为轻量级图像，例如webp、jpeg、webp；50等\nPointer 鼠标指针\nDouble click interval (maximum). The maximum time in milliseconds between the two clicks of a double-click. Increasing this value may assist if double-clicks are sometimes not registered. 双击间隔（最大值）。双击两次之间的最长时间（毫秒）。如果双击有时未生效，则增加此值可能会有所帮助\nBETA: Pointer click drift delay. After pressing a pointer button down, this is the maximum time (in milliseconds)that pointer movement can be ignored for. Helps prevent objects from being unintentionally nudged if the pointer is moved whilst clicking. 指针点击漂移延迟。按下指针按钮后，这是指针移动可以忽略的最长时间（以毫秒为单位）。有助于防止在单击时移动指针时无意中轻推对象\nBETA: Pointer click drift (maximum distance). If the pointer moves more than this distance while holding a button down, it is considered dragging (rather than clicking). Helps prevent objects from being unintentionally nudged if the pointer is moved whilst clicking. 指针点击漂移（最大距离）。如果指针在按住按钮的同时移动超过此距离，则被视为拖动（而不是单击）。有助于防止在单击时移动指针时无意中轻推对象\nReroute Beta. 转接节点测试\nBETA: Opt-in to the reroute beta test. Enables the new native reroutes. Reroutes can be added by holding alt and dragging from a link line, or on the link menu. Disabling this option is non-destructive- reroutes are hidden. 选择转接节点测试。启用新的转接节点。通过按住alt并从连接线或连接菜单上拖动，可以添加转接节点。禁用此选项是非破坏性的-转接节点被隐藏\nAppearance 外观 Color Palette 主题\nNode\nNode opacity. 节点不透明度\nNode Widget\nTextarea widget font size. 文本区域小部件字体大小\nSidebar\nSidebar size. 侧边栏尺寸\nSidebar location. 侧边栏位置\nMask Editor 蒙版编辑 Brush Adjustment 笔刷调整\nBETA: Lock brush adjustment to dominant axis. When enabled, brush adjustments will only affect size OR hardness based on which direction you move more. 将笔刷调整锁定到主导轴. 启用后，刷子调整只会根据您移动的方向影响尺寸或硬度\nBETA: Brush adjustment speed multiplier. Controls how quickly the brush size and hardness change when adjusting. Higher values mean faster changes. 笔刷调整速度倍数. 控制调整时画笔大小和硬度的变化速度。值越高，变化越快\nNew Editor\nBETA: Use new mask editor. 启动新的蒙版编辑\nComfy-Desktop 桌面设置 General 一般设置\nSend anonymous crash reports. 发送匿名事件报告\nAutomatically check for updates. 自动检查更新\nKeybinding 编辑快捷键 (可自定义) Comfy.NewBlankWorkflow 新建工作流\nComfy.OpenWorkflow 打开工作流 Ctrl+o\nComfy.LoadDefaultWorkflow 加载默认工作流\nComfy.SaveWorkflow 保存工作流 Ctrl+s\nComfy.SaveWorkflowAs 工作流另存为\nComfy.ExportWorkflow 导出工作流\nComfy.ExportWorkflowAPI 导出工作流(API)\nComfy.Undo 撤销\nComfy.Redo 重做\nComfy.ClearWorkflow 删除工作流 Backspace\nComfy.Canvas.ResetView 重设视图\nComfy.OpenClipspace 打开 Clipspace\nComfy.RefreshNodeDefinitions 刷新节点 r\nComfy.Interrupt 停止 Ctrl+Alt+Enter\nComfy.ClearPendingTasks 清除等待任务\nComfy.BrowseTemplates 打开默认工作流模板\nComfy.Canvas.ZoomIn 放大 Alt+=\nComfy.Canvas.ZoomOut 缩小 Alt+-\nComfy.Canvas.FitView 适应窗口 .\nComfy.Canvas.ToggleLock 切换 选择/拖动 模式\nComfy.Canvas.ToggleLinkVisibility 切换连接线 可见/不可见\nComfy.QueuePrompt Ctrl+Enter\nComfy.QueuePromptFront Ctrl+Shift+Enter\nComfy.ShowSettingsDialog 打开设置面板 Ctrl+,\nComfy.Graph.GroupSelectedNodes 将选定节点一组 Ctrl+g\nWorkspace.NextOpenedWorkflow 切换到下一个打开的工作流\nWorkspace.PreviousOpenedWorkflow 切换到上一个打开的工作流\nComfy.Canvas.ToggleSelectedNodes.Mute 屏蔽选择的节点 Ctrl+m\nComfy.Canvas.ToggleSelectedNodes.Bypass 禁用选择的节点 Ctrl+b\nComfy.Canvas.ToggleSelectedNodes.Pin\nComfy.Canvas.ToggleSelected.Pin 锁定或解锁选择的节点或组 p\nComfy.Canvas.ToggleSelectedNodes.Collapse 折叠选择的节点 Alt+c\nComfy.ToggleTheme 切换主题\nWorkspace.ToggleBottomPanel 切换底部面板\nWorkspace.ToggleFocusMode 隐藏/打开 菜单栏 f\nComfy.Graph.FitGroupToContents 根据内容调整组\nWorkspace.ToggleSidebarTab.queue 隐藏/打开 侧边栏队列历史 q\nWorkspace.ToggleSidebarTab.node-library 隐藏/打开 侧边栏节点库 n\nWorkspace.ToggleSidebarTab.model-library 隐藏/打开 侧边栏模型 m\nWorkspace.ToggleSidebarTab.workflows 隐藏/打开 侧边栏工作流 w\nWorkspace.ToggleBottomPanelTab.logs-terminal 打开/隐藏 logs Ctrl+`\nWorkspace.ToggleBottomPanelTab.command-terminal 打开/隐藏 终端\nComfy.GroupNode.ConvertSelectedNodesToGroupNode 将所选节点转换为组节点 Alt+g\nComfy.GroupNode.UngroupSelectedGroupNodes 放弃分组所选的组节点 Alt+Shift+G\nComfy.GroupNode.ManageGroupNodes 管理组节点\nComfy-Desktop.Folders.OpenLogsFolder 打开 logs 文件夹\nComfy-Desktop.Folders.OpenModelsFolder 打开模型文件夹\nComfy-Desktop.Folders.OpenOutputsFolder 打开输出文件夹\nComfy-Desktop.Folders.OpenInputsFolder 打开输入文件夹\nComfy-Desktop.Folders.OpenCustomNodesFolder 打开节点文件夹\nComfy-Desktop.Folders.OpenModelConfig 打开模型路径配置文件\nComfy-Desktop.OpenDevTools 打开开发工具\nComfy-Desktop.OpenFeedbackPage 打开反馈页\nComfy-Desktop.Reinstall 重装\nComfy-Desktop.Restart 重启\nExtension 扩展 Comfy.ColorPalette 调色板\nComfy.NodeTitleEditor 节点标题编辑\nComfy.NodeBadge 节点标志\nComfy.WidgetInputs 组件输入\nComfy.Clipspace\nComfy.ContextMenuFilter 快捷菜单过滤器\nComfy.DynamicPrompts 动态提示\nComfy.EditAttention 编辑 Attention\nComfy.GroupNode 组节点\nComfy.GroupOptions 组选项\nComfy.InvertMenuScrolling 反转菜单滚动\nComfy.Keybinds 密钥绑定\nComfy.MaskEditor 编辑蒙版\nComfy.NodeTemplates 节点模板\nComfy.NoteNode 注释节点\nComfy.RerouteNode 转接节点\nComfy.SaveImageExtraOutput 保存图像额外输出\nComfy.SimpleTouchSupport 简单触控支持\nComfy.SlotDefaults 连接点默认\nComfy.UploadImage 上传图片\nComfy.WebcamCapture 网络摄像头捕获\nComfy.AudioWidget 音频组件\nComfy.UploadAudio 上传音频\nComfy.Load3D 加载 3d\nComfy.Load3DAnimation 加载 3d 动画\nComfy.Preview3D 预览 3d\nComfy.ElectronAdapter\nComfy.Manager.NodeFixer 节点修复器\nComfy.ManagerMenu 管理菜单\nServer-Config 未添加\nAbout 关于 ComfyUI, 系统信息, 设备信息等\n",
    "description": "",
    "tags": null,
    "title": "界面介绍",
    "uri": "/02_comfyui/01_interface/"
  },
  {
    "content": "加载器 Load Checkpoint Checkpoint 模型加载器, 用于加载包含了 CLIP, VAE 和 基本模型 的加载器\nLoad VAE VAE 模型加载器, 用于加载 VAE 模型加载器\nLoad LoRA LoRA 模型加载器, 用于加载 Lora 模型, 它的作用是调节 扩散模型(基本模型) 和 CLIP 模型, 让生成结果向 LoRA 靠近. 多个 Lora 可以连接起来. 第 2,3 个参数用于调节基本模型和 CLIP 模型权重\nLoraLoaderModelOnly 该加载器与上一个不同之处在于, 它只调节 基本模型.\nLoad ControlNet Model ControlNet 模型加载器.\nLoad ControlNet Model (diff) ControlNet 模型加载器的 diff 版本, 与上者不同之处在于需要输入模型.\nLoad Style Model 风格模型加载器, 加载用于迁移图像风格的模型.\nLoad CLIP Vision 加载 CLIP 视觉模型, 模型用于图像编码，提取图像的特征 (CLIP 则是文本编码).\nunCLIPCheckpointLoader 加载 unCLIP 基本模型的加载器, 这种模型架构同 DALL-E 2, 能生图, 也能对图像进行编辑 (CLIP_VISION 编码图像)\nGLIGENLoader 加载 GLIGEN 模型，它通常与 Stable Diffusion 结合使用，增强其对空间位置和属性的控制能力\nHypernetworkLoader 加载的是 Hypernetwork 超网络模型, 通常用于微调或修改预训练好的扩散模型，以实现特定风格或概念的学习 (很少用了)\nLoad Upscale Model 加载放大模型\nImage Only Checkpoint Loader (img2vid model) 加载图生视频模型, 例如 SVD, 只能图生视频, 而不能同时添加文本控制\n条件节点 CLIP Text Encode (Prompt) CLIP 文本编码\nCLIP Set Last Layer 设置 CLIP 停止层, 越深的层级提取的特征越抽象，越偏向于语义信息；而较浅的层级提取的特征则保留了更多的细节信息\nConditioningAverage 条件平均节点, 用于控制两个输入条件的权重, 1 时是 to 的条件, 0 时是 from 的条件, 0-1 之间两个条件按权重混合\nConditioning (Combine) 条件合并节点, 将两个条件组合起来\nConditioning (Concat) 条件连结节点, 将 from 合并到 to 中, 组合成统一的表示\nConditioning (Set Area) 条件设置区域, 将条件限制在指定区域内, 与条件合并使用, 可以很好的控制构图\nConditioning (Set Area with Percentage) 条件设置区域 (按系数), 与前面一个不同之处在于, 这个是按系数控制\nConditioningSetAreaStrength 条件设置区域强度控制\nConditioning (Set Mask) 设置遮罩条件, 可调节强度 和 区域(默认或由遮罩本身确定)\nCLIP Vision Encode CLIP VISION 编码, 可选对图像裁剪或不裁剪\nApply Style Model 应用风格模型, 将风格模型, 输入调节, 图像特征 统一起来指导扩散模型生成特定风格. 可调节强度, 强度类型只有 multiply (多重)\nunCLIPConditioning unCLIP 模型条件设置, 接收条件和 CLIP VISION 编码的图像, 可调节强度和噪声\nApply ControlNet 应用 ControlNet 模型, 接收正/负调节, ControlNet 模型, 图像, VAE. 可控制强度, 开始/结束时机\nSetUnionControlNetType 设置 UnionControlNet (包含多种 ControlNet 的模型) 模型的控制类型, 可选:\nauto\ropenpose\rdepth\rhed/pidi/scribble/ted\rcanny/lineart/anime_lineart/mlsd normal\rsegment\rtile\rrepaint ControlNetInpaintingAliMamaApply 阿里妈妈重绘 ControlNet 应用, 重绘多了 mask 蒙版参数, 控制要重绘的区域\nGLIGENTextBoxApply GLIGEN 模型文本控制区域应用, 可指定文本控制的区域\nInpaintModelConditioning 重绘模型调节 (用模型对图像进行修复/重绘), 参数 pixels 表示要进行修复的图像的像素数据, noise_mask, 控制噪声是否只在蒙版内产生.\nSVD_img2vid_Conditioning SVD 图生视频条件, 可指定视频宽/高, 视频帧数 video_frames (长度), 视频运动参数 motion_bucket_id , 每秒的帧数 fps, 以及视频增强水平 augmentation_level\nLTXVImgToVideo LTXV 模型图生视频条件. length: 生成视频的帧数, batch_size: 一次生成视频数, image_noise_scale: 添加到输入图像的噪声强度\nLTXVConditioning LTXV 模型条件\nStableZero123_Conditioning StableZero123 模型条件, 可指定宽/高, 一次生成数量, 3D 仰角 elevation, 3D 方位角 azimuth\nStableZero123_Conditioning_Batched 上一个的批量处理条件, 新增仰角在批量中的增量变化 elevation_batch_increment, 方位角在批量中的增量变化 azimuth_batch_increment 参数\nSV3D_Conditioning SV3D 模型条件, 用于图像生成 3D 视频, 可指定宽/高, 视频帧数, 3D 仰角\nSD_4XUpscale_Conditioning SD_4X 放大条件, 用于图像放大, 可调节放大系数 scale_ratio, 放大过程中噪声增强的水平 noise_augmentation\nStableCascade_StageB_Conditioning StableCascade 模型修改条件, stage_c 在输入条件中加如 Latent 条件\nInstructPixToPixConditioning 风格转换条件, pixels 原始图像像素数据\n潜空间节点 VAE Decode VAE 解码, 潜空间图像到图像\nVAE Encode VAE 编码\nVAE Encode (for Inpainting) VAE 编码 (重绘), grow_mask_by 调节遮罩重绘影响程度\nSet Latent Noise Mask 设置遮罩区域噪声, 罩区域添加额外的噪声来进行局部的重绘\nEmpty Latent Image 创建空潜空间图像\nUpscale Latent 潜空间图像放大, 放大算法:\nnearest-exact 最近邻精确: 速度最快, 计算最简单。块状效应 (Blockiness), 生成的图像容易出现块状伪影，特别是沿着对角线方向，没有平滑过渡。可以很好地保留尖锐的边缘和细节\nbilinear 双线性插值: 速度较快, 计算相对简单。比最近邻插值更平滑，块状效应减少。会在一定程度上模糊图像细节，特别是边缘\narea 区域平均: 速度中等。在缩小图像时，区域平均通常可以产生比其他方法更好的结果，因为它考虑了更多的输入像素信息。在放大图像时，效果与双线性插值类似，也会导致一定程度的模糊\nbicubic 双三次插值: 速度较慢, 计算更复杂。比双线性插值更平滑，能更好地抑制锯齿和伪影。能在一定程度上保留图像细节，但仍然可能存在一些模糊。在高对比度边缘附近可能会出现轻微的振铃效应\nbislerp 双二次曲面片: 试图在平滑度和细节保留之间取得更好的平衡。在一些情况下，可以产生比双三次插值更好的结果，特别是对于自然图像\nUpscale Latent By 潜空间图像按系数放大\nLatent From Batch 从一批潜空间图像中提取一个片段. batch_index 要提取的第一个潜空间图像的索引, length 要提取的潜空间图像数量\nRepeat Latent Batch 重复潜空间图像批次, amount 重复次数\nLatentBatch 将两组潜空间图像合并为一个批次\nRebatch Latents 拆分/合并潜空间图像批次\nLatent Composite 合并潜空间图像, 将 from 合并到 to, x/y 指定 from 坐标, feather 指定 to 羽化程度\nLatentCompositeMasked 与上一个一样是合并, 将 source 合并到 destination, 可选的 mask 遮罩, 可以指定 source 要用于合并的部分, resize 选项选择是否调整 source 的尺寸以适应 destination\nRotate Latent 旋转潜空间图像, 90, 180, 270\nFlip Latent 水平/垂直翻转潜空间图像\nCrop Latent 裁剪潜空间图像, 指定图像宽/高, 坐标\nLatentAdd 两个潜空间图像加合成一个新的潜空间图像\nLatentSubtract 第一个潜空间图像中移除第二个潜空间图像的属性\nLatentMultiply 潜空间图像特征按系数放大, 调整潜空间图像内特征的强度或大小\nLatentInterpolate 潜空间图像插值, 按比例混合两个潜空间图像, 0 时是第一个潜空间图像, 1 时是第二个, 0-1 则按比例插值, 产生一个新的潜空间图像\nLatentBatchSeedBehavior 潜空间图像批次随机化, 用于打乱批次索引\nBETA LatentApplyOperation 潜空间图像处理\nBETA LatentApplyOperationCFG CFG 版处理\nBETA LatentOperationTonemapReinhard 潜空间色调调节处理\nBETA LatentOperationSharpen 潜空间锐化处理, sharpen_radius 控制锐化影响的范围, sigma 控制锐化过程中高斯平滑的程度, alpha 控制锐化效果的强度\nStableCascade_EmptyLatentImage 创建 StableCascade 模型空的潜空间图像, compression 压缩级别\nStableCascade StageC VAEEncode StableCascade 模型 VAE 编码\nEmptyLatentAudio 创建空的潜空间音频\nVAEEncodeAudio 音频 VAE 编码\nVAEDecodeAudio 音频 VAE 解码\nEmptySD3LatentImage 创建空的 SD3 潜空间图像\nEmptyMochiLatentVideo 创建空的 Mochi 模型潜空间视频\nEmptyLTXVLatentVideo 创建空的 LTXV 模型潜空间视频\n图像节点 Save Image 保存图像节点, 图像保存到 ComfyUI/output 下. 默认前缀是 ComfyUI, 后缀是 5 个数字, 如: ComfyUI_00001_.png\n自定义保存图像文件夹/文件名:\n自定义路径和文件名\n例如 test/001, 则图像保存路径如 ComfyUI/output/test/001_00001_.png\n使用日期命名:\n日期表示法\nyyyy：年份（4 位数字）\ryy：年份（2 位数字）\rMM：月（2 位数字）\rdd：天 （2 位数字）\rhh：小时（2 位数字）\rmm：分钟（2 位数字）\rss：秒（2 位） 例如 %date:yyMMdd%/%date:hh-mm-ss%, 则图像保存路径如 ComfyUI\\output\\241210\\15-22-22_00002_.png\n使用宽/高, 或节点小组件的值命名\n%width% 宽, %height% 高\n小组件值如: %Load Checkpoint.ckpt_name% (节点标题) 或 %CheckpointLoaderSimple.ckpt_name% (节点 S\u0026R 名, 优先使用 S\u0026R 名), 节点同名的需要重命名\n这样你可以将生成图像所使用的模型, 提示词, 种子, 步数等等, 都可以用作图片名\n例如, 将正向提示词的 CLIP Text Encode (Prompt) 命名为 poCLIPTextEncode 后,\n%date:yyyy-MM-dd%/%poCLIPTextEncode.text%_%width%x%height%_%KSampler.seed%_%Load Checkpoint.ckpt_name% 则图像保存路径如:\n\"ComfyUI\\output\\2024-12-10\\cat_512x512_457284464742973_majicmixRealistic_v7.safetensors_00001_.png\" Preview Image 预览图像\nLoad Image 加载图像\nUpscale Image 图像放大, 放大算法 (这里只介绍 lanczos 算法, 其他在潜空间图像放大介绍过):\nnearest-exact\rbilinear\rarea\rbicubic\rlanczos Lanczos 算法是一种高质量的图像放大算法，它通过使用 Lanczos 窗口对 sinc 函数进行加窗，在锐度、细节保留和平滑度之间取得了较好的平衡。虽然计算量较大，但它通常能够生成比其他插值算法更优质的放大图像\nUpscale Image By 图像按系数放大\nUpscale Image (using Model) 图像使用模型放大, 需要加载放大模型\nScale Image to Total Pixels 图像按像素放大, megapixels 图像的目标大小，以百万像素为单位。这决定了放大图像的总像素数\nInvert Image 图像反转, 转换图像的颜色为互补色\nBatch Images 组合图像批次, 将两张图象组合成一个批次, 第二章图像的尺寸会自动重新调整以匹配第一张\nPad Image for Outpainting 图像外补画板, 通过在图像周围添加填充来对图像进行外延处理, feathering, 羽化程度\nEmptylmage 生成具有指定尺寸和颜色的空白图像. color, 使用十六进制值定义生成图像的颜色\nImage Blend 图像混合, blend_factor 第二张图像在混合中的权重. 混合模式:\nnormal\rmultiply\rscreen\roverlay\rsoft_light\rdifference 正常、乘法、屏幕、叠加、柔光和差异模式，每种模式都产生独特的视觉效果\nImage Blur 图像模糊, 对图像应用高斯模糊，允许软化边缘并减少细节和噪声. blur_radius模糊效果的半径, 更大的半径会导致更明显的模糊; sigma 控制模糊的扩散\nImage Quantize 图像颜色量化, colors 指定将图像调色板减少到的颜色数量, 颜色数量较少将导致文件大小的明显减少，但也可能导致图像细节的损失; dither 抖动参数决定了在量化过程中要应用的抖动技术, 抖动技术:\nnone\rfloyd-steinberg\rbayer-2\rbayer-4\rbayer-8\rbayer-16 Image Sharpen 图像锐化, sigma 控制锐化效果的扩散, 较高的sigma值会在边缘产生更平滑的过渡，而较低的sigma使锐化更局部化, alpha 调整锐化效果的强度\nImageMorphology 图像形态学操作, 基于形状的图像处理技术，常用于图像降噪、边缘检测、物体分割等.\noperation 形态学操作的类型; kernel_size 核大小, 操作使用的基本结构元素.\n操作类型:\nErode (腐蚀):\n原理： 使用结构元素对图像进行“腐蚀”操作。如果结构元素覆盖的所有像素都与图像中的对应像素匹配，则输出像素保持不变；否则，输出像素被设置为背景值（通常为黑色）。\n效果： 使图像中的亮区域（前景）收缩，细小的连接和噪声会被消除。可以去除小的噪点，分离小的物体。\n比喻： 想象一下海岸线被海水侵蚀，陆地面积会缩小。\nDilate (膨胀):\n原理： 使用结构元素对图像进行“膨胀”操作。如果结构元素覆盖的任何一个像素与图像中的对应像素匹配，则输出像素被设置为前景值（通常为白色）。\n效果： 使图像中的亮区域（前景）扩张，可以填充小的空洞，连接断裂的区域。可以连接相邻的物体，填充小的空洞。\n比喻： 想象一下陆地向海洋扩张，陆地面积会增大。\nOpen (开运算):\n原理： 先对图像进行腐蚀，再进行膨胀。\n效果： 可以去除小的亮区域（噪声），平滑物体的轮廓，同时保持较大的物体形状基本不变。\n比喻： 想象一下用一个小刷子先刷掉小的凸起，再用刷子把凹陷填平。\nClose (闭运算):\n原理： 先对图像进行膨胀，再进行腐蚀。\n效果： 可以填充物体内部的小空洞，平滑物体的轮廓，同时保持较大的物体形状基本不变。\n比喻： 想象一下用一个小铲子先把凹陷填平，再把凸起铲平。\nGradient (形态学梯度):\n原理： 计算图像的膨胀结果与腐蚀结果的差值。\n效果： 可以提取物体的边缘。边缘的宽度由 kernel_size 决定。\n比喻： 想象一下沿着物体的边缘画一条线。\nTop Hat (顶帽):\n原理： 计算图像的原始图像与开运算结果的差值。\n效果： 可以提取比结构元素小的亮区域（例如，噪声或细节）。\n比喻： 想象一下从原始地形中减去被平滑后的地形，剩下的就是突出的部分（小山峰）。\nBottom Hat (底帽):\n原理： 计算图像的闭运算结果与原始图像的差值。\n效果： 可以提取比结构元素小的暗区域（例如，孔洞或裂缝）。\n比喻： 想象一下用被填平后的地形减去原始地形，剩下的就是凹陷的部分（小坑）\nImageCompositeMasked 图像遮罩复合, 将源图像覆盖在目标图像上，在指定坐标处进行叠加，可选择调整大小和使用遮罩. 与潜空间遮罩复合类似.\nRebatch Images 重新分批图像批次\nRepeatlmageBatch 重复图像批次\nImageFromBatch 从一批图像中取一个片段\nCanny 图像的边缘检测, low_threshold 阈值下限, 影响边缘检测的灵敏度, high_threshold 阈值上限, 影响边缘检测的选择性\nImage Crop 图片裁剪\nSaveAnimatedWEBP 将一系列图像保存为动画 WEBP 文件. lossless 是否使用无损压缩, quality 压缩质量, method 压缩方法: 默认, 最快, 最慢\nSaveAnimatedPNG 从一系列帧创建和保存动画PNG图像. compress_level 压缩级别\nWebcam Capture 打开网络摄像头, capture_on_queue 参数控制摄像头捕获图像的时机，True 表示在节点执行时捕获，False 表示在工作流开始时捕获\n蒙版 Load Image (as Mask) 加载图像作为蒙版, alpha 使用图像的 Alpha 通道 (透明度通道) 作为遮罩, red/green/blue 使用图像的红色/绿色/蓝色通道作为遮罩\nConvert Mask to Image 遮罩转图像\nConvert Image to Mask 图像转遮罩\nImageColorToMask 将彩色图像根据指定的颜色转换为遮罩\nSolidMask 生成一个在所有维度上具有统一值的实心遮罩, 特别适用于进一步处理或作为更复杂遮罩操作的起点. value: 用于填充整个遮罩的统一值, 决定了遮罩的基本颜色或强度\nInvertMask 反转遮罩\nCropMask 裁剪遮罩\nMaskComposite 遮罩合并, 合并操作:\nmultiply (相乘): 输出遮罩是两个输入遮罩对应像素值相乘的结果。重叠区域倾向于变暗(值变小)。\nadd (相加): 输出遮罩是两个输入遮罩对应像素值相加的结果。重叠区域倾向于变亮(值变大), 可能会饱和(值被限制为最大)。\nsubtract (相减): 输出遮罩是第一个输入遮罩减去第二个输入遮罩对应像素值的结果。重叠区域根据两者的差值变暗或者变亮。\nand (与): 输出遮罩是两个输入遮罩对应像素值进行逻辑与操作的结果。只有当两者都为非零值时，输出才为非零，否则为零。重叠区域倾向于保留两者重叠的部分, 其他部分变为透明。\nor (或): 输出遮罩是两个输入遮罩对应像素值进行逻辑或操作的结果。只要两者中有一个为非零值，输出就为非零。重叠区域倾向于只要有遮罩的部分都变为不透明。\nxor (异或): 输出遮罩是两个输入遮罩对应像素值进行逻辑异或操作的结果。当两者值不同时，输出为非零，否则为零。重叠区域倾向于只保留两者只有一个遮罩的部分。\nFeatherMask 羽化遮罩, 指定从上/下/左/右侧边缘开始应用羽化效果的距离\nGrowMask 修改给定遮罩的大小，可以选择性地对角落应用渐缩效果. expand 确定遮罩修改的大小和方向, 正值导致遮罩扩展, 而负值导致收缩, tapered_corners 设置为 True 时, 修改过程中对遮罩的角落应用渐缩效果\nThresholdMask 按阈值转换遮罩, 较高的值包含较少的像素，较低的值包含更多的像素\nPorter-Duff Image Composite 波特-达夫图像合成. 基于 Porter-Duff 合成规则对两张图像进行合成，其中一个图像作为前景 (Source, 通常表示为 SRC)，另一个作为背景 (Destination, 通常表示为 DST)\n合并模式 (A 和 B 分别表示前景和背景图像):\nSRC : 只显示前景图像 (A)。\nDST : 只显示背景图像 (B)。\nSRC_OVER: 前景覆盖在背景上 (A over B)，这是最常用的模式。\nDST_OVER: 背景覆盖在前景上 (B over A)。\nSRC_IN: 只显示前景中与背景重叠的部分 (A in B)，且使用前景的不透明度。\nDST_IN: 只显示背景中与前景重叠的部分 (B in A)，且使用背景的不透明度。\nSRC_OUT: 只显示前景中不与背景重叠的部分 (A out B)。\nDST_OUT: 只显示背景中不与前景重叠的部分 (B out A)。\nSRC_ATOP: 前景中与背景重叠的部分覆盖在背景上 (A atop B)，其他部分不显示. 使用前景的不透明度。\nDST_ATOP: 背景中与前景重叠的部分覆盖在前景上 (B atop A)，其他部分不显示. 使用背景的不透明度。\nXOR: 显示前景和背景中不重叠的部分 (A xor B)。\nCLEAR: 清除所有内容，得到一个透明图像。\nADD: 将前景和背景的像素值相加。\nMULTIPLY: 将前景和背景的像素值相乘。\nSCREEN: 类似于“叠加”，但更亮一些。\nOVERLAY: 根据背景的亮度，对前景进行“叠加”或“滤色”。\nDARKEN: 选择前景和背景中较暗的像素。\nLIGHTEN: 选择前景和背景中较亮的像素。\nSplit Image with Alpha 分离图像颜色和透明度\nJoin Image with Alpha 将图像与其对应的 Alpha 遮罩结合\n测试功能 (暂不介绍) BETA Latent Blend BETA VAE Decode (Tiled) BETA VAE Encode (Tiled) BETA LoadLatent BETA SaveLatent BETA AddNoise BETA Self-Attention Guidance BETA PerpNegGuider BETA PhotoMakerLoader BETA PhotoMakerEncode BETA CLIPTextEncodeControlnet BETA StableCascade_SuperResolutionControlnet BETA Differential Diffusion BETA UNetSelfAttentionMultiply BETA UNetCrossAttentionMultiply BETA CLIPAttentionMultiply BETA UNetTemporalAttentionMultiply BETA SamplerEulerCFG++ BETA Extract and Save Lora BETA TorchCompileModel ",
    "description": "",
    "tags": null,
    "title": "内置节点2",
    "uri": "/02_comfyui/03_built-in/02-built-in/"
  },
  {
    "content": "提示工程 (Prompt Engineering): 是指为AI模型设计和优化输入文本（即“提示”或“指令”）的过程，以确保模型生成的输出符合预期或更高质量。提示工程包括选择合适的语言、结构、语境和细节来指导AI生成更准确、相关、有用或创造性的响应。\n提示工程（Prompt Engineering）的作用:\n提高输出质量：通过精心设计的提示，可以显著提高AI模型的输出准确性和相关性。\n减少偏见和错误：良好的提示可以帮助减少AI生成的内容中的偏见或错误，因为它引导模型关注正确的方面。\n增强用户体验：在应用中，用户与AI的交互体验很大程度上依赖于提示的质量，好的提示可以让用户更容易获得所需信息或服务。\n创新和创造力：在生成性任务中，如文本生成、图像创建等，提示工程可以激发模型的创造性输出，推动应用的创新。\n效率提升：精确的提示可以减少后续的修订工作，提高生成内容的效率。\n提示工程（Prompt Engineering）的学习方法:\n理解基础知识： 学习AI模型的工作原理，特别是如何处理和理解语言输入。 了解你所使用的AI模型的特性，如其训练数据、预置的偏见或特定的能力。\n实践与试错： 进行大量的实践，尝试不同的提示结构、语言风格、长度和复杂度。 使用A/B测试来比较不同提示的效果，分析哪些元素对输出质量产生影响。\n学习社区资源： 参与AI和NLP（自然语言处理）的社区，如GitHub上的项目、论坛（如Reddit）、Stack Overflow等，学习他人的提示工程实践和技巧。 查看开源的提示库或模型文档中提供的示例提示。\n迭代优化： 基于反馈和生成的输出不断优化你的提示。每次迭代都应该尝试改进一个或几个元素。 使用工具或脚本来自动化提示的测试和评价过程。\n专门化学习： 根据具体应用领域（如法律、医学、客服等）学习行业特定术语和交流方式，以便设计更专业的提示。\n学习心理学和语言学： 了解语言如何影响思维和行为可以帮助设计更有效的提示。 研究语言学可以帮助优化语法、语义和语用学方面的提示。\n课程和培训： 参与在线课程或工作坊，这些课程可能专注于NLP或AI应用中的提示工程。\n保持更新： AI技术发展迅速，保持对新模型、技术和最佳实践的关注是必要的。\n",
    "description": "",
    "tags": null,
    "title": "提示工程",
    "uri": "/03_prompts/"
  },
  {
    "content": "上一章我们详细梳理了官方桌面版本的界面—-各种功能, 设置, 快捷键, 扩展等等.\n这一章我们就来详细介绍基本概念, 其实也算不上基本概念, 就是对一些关键词的大白话描述, 能够简单理解就行, 需要深入研究, 问 AI 能够将实现细节, 以及背后的数学原理都讲清楚. 这里的基本概念仅限于图像生成, 而文本,音频和视频等生成将在后面介绍.\n基本概念非常重要, 可以说, 在这个 AI 时代, 掌握基本概念是急速学习的秘诀. 不管学什么, 基本概念掌握了, 你就能够比较标准的组织相关领域的语言, 然后向装着全世界知识的 AI 提问, 他就能够给你很好的答案, 你也能够相对轻松地理解, 然后继续追问, 大踏步向着更专业迈进…\n从基本工作流开始 一个完整的 ComfyUI 生图工作流, 大致可以分为 4 个阶段:\n加载模型 输入, 预处理和设置参数 执行生成 后处理和输出结果 加载模型 目前知名的大模型有 SD 1.4, SD 1.5, SD 1.5 LCM, SD 1.5 Hyper, SDXL 1.0, SDXL Turbo, SDXL Lightning, SDXL Hyper, SD 3.5 Medium, SD 3.5 Large, SD 3.5 Large Turbo, Pony, Flux .1S, Flux.1 D, Stable Cascade, PixArt, Hunyuan, Kolors 等, 以及社区基于底模训练的各种版本, 非常丰富.\n其中, 最常用的是 SD1.5, SDXL, SD3.5, Flux 四大类.\n除了大模型, 还有与大模型配套的模型和组件, 主要有 CLIP, VAE, Lora, Controlnet 等.\n对这些模型, 需要清楚下面几点:\n大模型是基底, 生图时, 不同的大模型一般需要不同的 CLIP, VAE, Lora, Controlnet 等.\nCLIP 是连接文本和图像的桥梁, 将输入的提示词 “翻译” 给计算机, 或反过来.\nVAE 则是将我们人类看到的图像, 处理成计算机能理解的信号, 或反过来.\nLora 是基于特定大模型训练而来, 它就像一个 “说明书”, 告诉大模型, “你要给我这个样子的”, “你要给我这个风格的”… 因此训练 Lora 需要对用来训练的图像打标 (也就是将图像翻译成提示词), 做出一个说明书来, 生成的时候, 又用这个说明书 (触发词) 指导大模型生成. 基于同一个大模型版本训练的 Lora, 一般都能用, 但是出不出效果, 就要看训练 Lora 的人用的底模具体是哪个版本.\nControlnet 则是基于大模型版本训练, 相同版本的大模型都能用, 通常用于 图像到图像 的生成. 它像一个 “结构图”, 告诉大模型, “你要按照我这个结构来画”, 不要将 “春树秋霜图” 画成 “小鸡食米图”.\nFP32, FP16, FP8, BF16 等, 表示模型权重的 数据类型 或 精度, FP32 提供高精度计算，FP16 和 BF16 提供更高效的计算, BF16 专门用于加速计算, 在精度方面，BF16 比 FP16 更能保留重要的信息，FP8 用于高效的计算，但会有精度损失. (还有整数精度的 INT8, INT16, INT32)\n常见的模型的格式有:\n.ckpt 文件：广泛使用，兼容性好。 但可能包含冗余信息，文件较大。\n.safetensors 文件： 更安全，高效，文件较小。 需要特定的加载器支持。\n.pt 或 .pth 文件： PyTorch 原生格式，兼容性好。 文件可能较大，需要 PyTorch 环境支持。\n.bin 文件格式通常用于存储二进制数据，在深度学习和机器学习领域，.bin 文件则常用于存储模型的权重和配置信息\n大模型可以将 CLIP 和 VAE 合并在一起, 这种版本的大模型, 加载时一般使用 Load Checkpoint 加载器:\n否则, 加载大模型时, 一般使用 Load Diffusion Model 加载器 (这种大模型通常是 扩散模型Diffusion Model 和 UNet 模型), 并且 CLIP 和 VAE 需要单独加载:\nLora 和 Controlnet 则需要单独加载.\n输入, 预处理和设置参数 提示词（Prompt）: 就是对你要生成的图像的自然语言描述. 正向提示词就是你要生成的结果; 负向提示词则是你不希望出现的结果.\n文本预处理\n分词：将文本分割成单词或子词，以便进行编码. 文本编码：将提示词转换为模型可以理解的嵌入向量。\n提示词通过 CLIP 编码, 引导模型生成与描述词相符的图像. 提示词可以不输入, 也会生成随机噪声, 指导生成大模型 “记忆中” 的随机图像.\n初始图像（Initial Image）\n输入图像：在某些情况下，可以提供一个初始图像，模型会基于这个图像进行生成或变换。图像变换：如风格迁移、图像修复或图像增强等任务。\n图像预处理\n调整大小：将输入图像调整到模型所需的尺寸。归一化：将图像像素值归一化到模型所需的范围（通常是 0 到 1）。数据增强：如旋转、翻转、裁剪等，用于增加数据多样性.\nLatent Image: 潜空间图像, 我们不理解但人工智能 “看” 得懂的图像. 如果需要用到 图生图, 则需要 VAE 编码为 Latent Image, 否则就生成一个空的 Latent Image, 它们都需要设置宽/高 (或者参考原图宽/高).\n如果用到 图像缩放, 裁剪, 图生图, 图像重绘, 用 Controlnet 预处理, 条件组合, 参考 Lora 等, 则需要各种预处理以及参数设置. 具体细节将在下一章 内置节点 对各种预处理节点详细介绍.\n执行生成 执行生成, 就是采样器按照之前的各种预处理条件, 和设置的参数, 拿起神笔开始泼墨挥毫画画的过程.\n参数详解:\nseed: 种子, 真的就是一颗种子, 埋到土里, 能长小花还是小草, 这颗种子很重要. 种子可以用来确定生成图像的构图 (宽高比, 采样器不变).\n图像生成后, 可以选择 fixed(固定), increment(增加), decrement(减小), randomize(随机化) 种子.\nsteps: 采样过程中的迭代次数, 步数 直接影响生成图像的质量、细节和计算效率。步数越多，生成过程的细节越丰富，生成结果的质量通常会更高，但同时，生成的时间和计算资源消耗也会增加, 需要一个平衡点. 复杂, 细节多的就增加步数.\nCFG: 同一个模型, 较高的 CFG 值, 图像会更符合输入条件，生成内容更加集中和精确，但可能牺牲创意和多样性。适用于需要精确控制生成内容的任务。较低的 CFG 值, 图像生成过程更具随机性和创意，适用于需要多样性或探索更多可能性的任务，但可能导致与输入条件不太一致的结果. 不同模型, CFG 值的最佳范围可能不同.\n采样器: 目前 ComfyUI 有\neuler\neuler_cfg_pp\neuler_ancestral\neuler_ancestral_cfg_pp\nheun\nheunpp2\ndpm_2\ndpm_2_ancestral\nLms\ndpm_fast\ndpm_adaptive\ndpmpp_2s_ancestral\ndpmpp_2s_ancestral_cfg_pp\ndpmpp_sde\ndpmpp_sde_gpu\ndpmpp_2m\ndpmpp_2m_cfg_pp\ndpmpp_2m_sde\ndpmpp_2m_sde_gpu\ndpmpp_3m_sde\ndpmpp_3m_sde_gpu\nddpm\nLcm\nipndm\nipndm_v\ndeis\nddim\nuni_pc\nuni_pc_bh2\n各种采样器都有什么特点, 优缺点, 自行 ChatGPT.\n对于需要快速生成的场景，可以选择 DPM_Fast、DDIM 等高效采样器.\n如果想要速度快、收敛性好、质量也不错，且想试试新东西的话，最好选择：\nDPM++ 2M Karras\nUniPC\n如果想要比较好的质量，同时不在意是否收敛的话，可以选择：\nDPM++ SDE Karras\nDDIM\n如果想要稳定、可复现的结果，不要用任何带有随机性的采样器，比如祖先采样器.\n如果想生成一些简单的结果，可以用 Euler 或 Heun. 在使用 Heun 时，可以调低一些步数来节省时间.\n调度器 scheduler: 目前 ComfyUI 有 调度器 优点 缺点 Normal 稳定，易于实现 生成质量一般，缺乏针对特定任务的优化 Karras 高质量生成，适合细节丰富的图像 计算开销大，复杂度高 Exponential 快速收敛，适合快速生成 生成质量较差，可能缺乏细节 SGM Uniform 噪声均匀，生成过程可控 计算开销大，生成速度较慢 Simple 高效，计算简单 生成质量低，缺乏灵活性 DDIM Uniform 高效，生成质量较好 噪声控制不如指数衰减精细 Beta 灵活，适应性强，能够定制噪声衰减曲线 调整复杂，计算开销较高 Linear Quadratic 生成图像细节丰富，适合高精度任务 计算资源消耗大，生成速度较慢 denoise: 去噪过程的强度或程度. 较强的去噪, 有助于生成清晰, 细节丰富的图像，但可能牺牲图像的多样性和创意，并增加计算时间. 较弱的去噪, 在图生图时, 和原图更像. 不同的模型, 模型作者在训练时, 使用的步数, CFG, 采样器, 调度器等, 对模型有重要影响. 因此使用作者推荐的参数, 是不错的选择.\n后处理和输出结果 后处理则主要是对生成的图像进行放大 (可能需要加载放大模型), VAE 将潜空间图像解码为我们需要的图像, 预览, 对比生成结果以及保存图像等, 后面详细介绍.\n图像放大（Upscaling）\n图像放大是将低分辨率图像转换为高分辨率图像的过程。这通常需要使用放大模型（Upscaling Model）来提高图像的细节和清晰度.\n列举一些放大模型:\nLDSR\rRealESRGAN\rScuNET\rAura-SR\rSUPIR\rESRGAN\rDAT\rSRGAN\rRCAN\rRDN\rEDSR\rLapSRN\rVDSR\rDRCN\rDRRN\rMemNet\rCARN\rMSRN\rSRResNet\rSRCNN\rFSRCNN\rESPCN\rSRFBN\rSAN\rRNAN\rIMDN\rHAN\rDBPN\rMDSR\rCAR\rLIIF\rSwinIR\rRRDB\rRFDN\rLAPAR\rOISR\r这里 https://openmodeldb.info/ 有更多, 不同的放大模型针对性不一样, 有全能的, 有脸部的, 有风景的, 有动漫的, 有照片的… 等等等等\n不用模型, 潜空间放大也行, 小显存电脑, 用分块放大效果也很好, 后面会详解.\nVAE（Variational Autoencoder）解码. VAE 将潜空间图像解码为我们需要的图像.\n最后的输出结果还可以接入下一个工作流, 继续魔法创作. 甚至将多个工作流组装成大型生产线.\n",
    "description": "",
    "tags": null,
    "title": "基本概念",
    "uri": "/02_comfyui/02_basic/"
  },
  {
    "content": "搜索问答 纳米搜索 2024-12-02\n书生·浦语 2024-12-02\nKimi\nchatgpt\n秘塔\nperplexity\nMeta AI\n腾讯元宝\n豆包 - 抖音旗下 AI 智能助手\nGoogle AI Studio\nCohere | The leading AI platform for enterprise 2024-12-03\nClaude\n知乎直答\n360搜\nPoe\n通义tongyi.ai_你的全能AI助手-通义千问 2024-12-03\n天工AI - 搜索更深度，阅读更多彩 2024-12-03\n讯飞星火大模型-AI大语言模型-星火大模型-科大讯飞 2024-12-03\n文心一言 2024-12-03\nHuggingChat\n语鲸 2024-12-03\n深言达意 – 找词找句 2024-12-03\n爱校对官网-免费高效的错别字检查工具 2024-12-03\nLearn About 2024-12-03\n视频, 语音, 绘图等综合 智谱清言\n即梦AI - 一站式AI创作平台\n可灵 AI - 新一代 AI 创意生产力平台\n海螺 AI\n跃问\nLuma Dream Machine | AI Video Generator\nKREA AI - AIGC集合-风格-图有免费\nHome - Leonardo.Ai\nAI Test Kitchen\nLimeWire\nLe Chat - Mistral AI\nWHEE - 高品质的AI素材生成器\n视频 World Labs 2024-12-03\n腾讯混元文生视频 2024-12-03\nPixelDance - PixelDance AI - 领先的AI视频生成平台 2024-12-03\nVidAU Creative Center 2024-12-02\nkaze.ai - AI-powered Free Online Removing Watermark and Logos Tool 2024-11-27\nVidu，让想象发生\nHailuo AI Video Generator - Reimagine Video Creation\nRunway\n讲故事的方式发生了转变LTX工作室 — Storytelling Transformed | LTX Studio\nNoisee AI 音乐生成MV\nPika\nGenmo. Create videos and images with AI.\nHome | PixVerse\nViggle AI\n万德动力 — Wonder Dynamics\nHeyGen - AI Spokesperson Video Creator\nAiuni\nDomoAI: video to video, video to animation and more\nFlair\nWarpvideo AI: Change Video Style with AI\nHedra 数字人\nAI 擁抱 - 免費線上 AI 擁抱影片生成器\nMOKI - 我用AI做短片\nMeshcapade | 编辑人物动作\nBoomCut - 爆剪辑 - 小影科技旗下 AI 内容创意产品与服务平台\n绘画设计 Create stunning visuals in seconds with AI.\n超能画布首页\nAdobe Firefly\nDesign - Playground\nSkybox AI 360°全景\nMagic Studio：利用 AI 制作精美图像\nmidjourney\nRemove Background from Image for Free – remove.bg\nCraiyon, formerly DALL-E mini\nCreate - Artbreeder\nNightCafe Creator\nProjects - Recraft\nBlendbox.ai 多图组合\nIdeogram 画布\nLogo-creator.io – Generate a logo\n在线抠图软件_图片去除背景 | remove.bg – remove.bg\n触手AI\n3D Meshy - Free 3D Models Generated from Images and Text\nImmersity AI | Convert Image and Video to 3D\nTripo AI - 用文字或图片免费生成3D模型\n语音, 音乐 Home • Hume AI 2024-12-03\nSuno\n海绵音乐\nFree Text to Speech \u0026 AI Voice Generator | ElevenLabs\nUdio AI Music Generator - Make Original Tracks in Seconds\nStable Audio - Generate\n在线免费文本转语音 - TTS-Online | 多种声音与二次元语音\nSoundboard - TUNA - Download Unlimited Free Meme Sounds\n节奏生成器-Beat Blender 音乐\n网易天音 - 一站式AI音乐创作工具 - 官网\n提示词 promptoMANIA:绘画提示生成器\nLexica\nPromptHero - 提示词大全\n代码 Codeium · Free AI Code Completion \u0026 Chat 2024-12-02\nMarsCode - AI IDE\nCursor\nbolt.new\nScriptEcho | AI生成生产级代码 |\n其他 NotebookLM | Note Taking \u0026 Research Assistant Powered by AI\n扣子 - AI 智能体开发平台\nFigma\nIlluminate | Learn Your Way\nChat Nio\nLlamaOCR.com – Document to markdown\nNeo AI engineer\nExcalidraw\n模型, 资源, 工作流 Discovery | OpenArt\nShakker - Generative AI design tool with diverse models\n首页 · 魔搭社区\nComfy Workflows\nOpenModelDB\nFREE online image generator and model hosting site! | Tensor.Art\nCivitai: The Home of Open-Source Generative AI\nCodeWithGPU | 能复现才是好算法\nLiblibAI-哩布哩布AI - 中国领先的AI创作平台\nComfyUI工作流 - 在线运行，速度快，不报错\nFREE online image generator and model hosting site! | Tensor.Art\nCephalon Cloud 端脑云 - AIGC 应用平台\n",
    "description": "",
    "tags": null,
    "title": "项目的官方网站",
    "uri": "/01_aigc_object/01_object/"
  },
  {
    "content": "在 ComfyUI 中，节点是代表工作流中 特定操作 或 功能 的基本构建块, 是一个单独处理单元。将节点连接在一起以创建复杂的图像生成工作流。\n从本章开始, 需要操作节点和创建工作流, 文后附有 基本操作附录 和 快捷键附录, 了解它们将是非常有用的. 操作中反复翻看将大大提升效率.\n每个节点在 ComfyUI 的界面中通常是一个带有名称、输入、输出和参数控件的框\n节点通常包含:\nInputs 输入. 输入是节点接收数据的连接点。每个输入都有其接受的特定数据类型，确保连接节点之间的兼容性。\nOutputs 输出. 输出是将数据发送到其他节点的连接点。与输入一样，输出也定义了数据类型，决定了它们传递的信息类型。\nParameters 参数. 参数是控制节点行为的设置。这些可以包括数值、文本字符串或预定义选项的选择。\nInternal Logic 内部逻辑. 这是节点的核心功能，由其背后的代码定义。它确定节点如何处理其输入、使用其参数并生成其输出。\n一个简单完整的工作流 下面我们正式开始内置节点介绍. 先将一个完整的工作流组成介绍完, 再依次按分类介绍每个节点:\n1, Load Checkpoint: 这个节点任务是从文件夹加载模型, 而且必须是包含 基础模型, CLIP 和 VAE 的模型 (具体见上一章基本概念), 然后分别输出.\n2, CLIP Text Encode (Prompt): CLIP 文本编码节点, 是通过输入的 CLIP 模型, 将输入的 自然语言提示词 编码为潜空间向量输出, 用来引导图像生成.\n3, Empty Latent Image: 空的潜空间图像, 该节点的作用就是创建空的潜空间图像, 通过参数指定宽, 高 和数量, 然后输出.\n4, KSampler: 采样器节点, 该节点任务是 输入模型 按照输入的各种条件, 设置的各种参数去噪, 执行图像生成, 然后输出 (输出的仍然是潜空间图像). 拟人化理解就是 AI 在潜空间中按照人的指令在画画. 各种参数详解见上一章 基本概念.\n5, VAE Decode: VAE 解码, 该节点任务是输入的 VAE 模型, 将 AI 画好的潜空间画作, 映射为现实图像输出.\n6, Preview Image: 图像预览节点, 将最后输出的图像打印到屏幕上供你欣赏.\n这就是一个完整的工作流, 它由一个个各司其职的节点组成, 协调完成创作大业. 工作流执行过程需要注意几点 (重要):\n整个生成过程是一环扣一环的, 哪个节点出错, 它后面的节点就不会执行.\n执行过一次之后, 下一次执行只运行有改动的节点, 以及依赖于它的节点. 例如你执行一次之后, 如果什么也没动, 连种子也固定, 在点击执行它不理你.\n生成的图像, 或导出的工作流, 你直接加载或拖到窗口中, 会加载完整的工作流, 包括种子, 也就是可以将上次生成的图像完全复现出来 (注意固定种子).\n你可以使用 () (英文括号) 来更改单词或短语的权重，例如：(beautiful girl：1.2) 或 (dog：0.8). () 的默认权重值为 1.1. 要在实际提示词中使用 () 字符，要转义它们，比如你要在图中生成 (), 就要 \\(\\) 这样.\n你可以使用 {day|night} 这样的语法, 将提示词变成动态提示词, 这样，每次执行生成, 会从 {} 中, | 符号隔开的单词或短语中随机选一个, 要在实际提示词中使用 {} 字符，请转义它们，例如：\\{\\}.\n提示词还支持 C 语言风格的注释, 注释的提示词不会生效. 像这样: // comment 或 /* comment */\n要使用 embeddings 模型，请将它们放在 models/embeddings 目录中，并在提示词中注明 (您可以省略 .pt 扩展名): embedding:embedding_filename.pt\n内置节点太多, 需要很多章节才能讲完, 下一章开始正式介绍每个内置节点.\n基本操作附录 加载节点 加载节点有多种方式.\n鼠标右击画布窗口的空白处, 可以按下图选择节点, 将选择的节点加载出来: 除了, Add Node, 其他选项介绍:\nAdd Group: 新建一个分组框 (这个操作麻烦, 不如选中要分组的节点, Ctrl + g 直接分为一个组)\nConvert to Group Node: 选中多个节点, 可以用这个选项将多个节点合并为一个组节点 (新功能)\nManage Group Nodes: 管理组节点\nAdd Group For Selected Nodes: 将选中的多个节点分为一个组 (Ctrl + g)\nSave Selected as Template: 保存选中的节点作为模板\nNode Templates: 节点模板管理, 可以导入/导出节点模板\n鼠标双击空白处, 弹出节点搜索框, 从搜索框中搜索节点加载 (鼠标移动到搜索出来的节点名上, 会弹出预览): 鼠标点击节点输出点, 按住拖动, 将拉出一条线, 放开线, 弹出的对话框也可加载节点, 如果匹配, 则直接与拖出的线相连: 直接打开节点库, 搜索或选择: 连接节点 节点加载出来, 按照上一章介绍的四个基本板块(加载模型, 输入/预处理/设置参数, 执行生成, 输出), 连接成一个完整工作流. 从输出点拖动线, 会自动高亮匹配的输入点并连接.\n并不是节点的所有输入/输出都需要连接, 有的节点的输入/输出项是可选的, 可以不用输入, 这种情况遇到再说.\n加载模型 模型统一放到 ComfyUI\\models 下, 对应的文件夹中, 自动下载的模型会自动存放, 手动下载的根据模型分类手动存放 (一般模型发布者会说明放到什么位置):\n如果模型的位置放对了, 点击模型加载器节点选择, 将弹出可选模型:\n输入提示词 提示词输入, 一般在 CLIP 文本编码器节点输入, 还有其他输入方式, 将在相应的节点介绍.\n参数设置 各节点有各节点的参数, 根据自己的需要, 计算机的配置 (显存), 模型参数要求等设置相应的参数, 例如前面介绍过的采样器节点. 具体细节到相应节点介绍.\n节点设置 鼠标放到节点上右击, 将弹出节点设置对话框, 基本所有节点设置都差不多, 不同的节点可能略有不同:\nInputs Outputs Convert to Group Node 将选中节的多个节点转换为一个组节点 Properties 设置节点的 S\u0026R 名称(节点太多, 可能有共用同一个名称, 通过这里可改唯一名称) Properties Panel 设置节点以下参数: Title 修改节点标题 Mode 选择节点模式: Always: 节点被改变或输入被改变时执行\nNever: 禁用节点\nOn Evert/On Trigger\nResize: 重置节点框大小 Collapse: 折叠节点 Pin: 在窗口画布中固定节点的位置 Colors: 设置节点的颜色 Shapes: 选择节点边框样式 Bypass: 绕过节点(不执行而继续执行后面的) Copy (Clipspace): 复制 Clipspace, 目前只对有图像的节点有效 复制完 Clipspace 之后, 打开 Edit – Clipspace, 将弹出窗口:\n然后 MaskEditor 可以打开蒙版编辑, 对图像进行处理:\nPaste (Clipspace): 粘贴 Clipspace, 将图像复制到另一个节点 (例如加载图像节点, 如果已经蒙版处理过, 直接粘贴的是处理过的图像) Convert Widget to Input: 将小组件转换为输入, 例如 CLIP 文本编码节点执行这个操作之后, 提示词就不能输入了, 只能从其他节点传进来 (可以用同样的操作转换回来): Clone: 克隆节点, 相当于复制一个节点 Remove: 删除节点 蒙版编辑器 分别是 撤销, 重做, 反转, 清除, 保存, 关闭蒙版编辑\n笔刷设置 可以设置笔刷的形状, 粗细, 透明度, 硬度, 平滑系数, 遮罩层的颜色, 遮罩透明度, 选择图层. (笔刷下面的是橡皮擦, 就不介绍了).\n喷漆设置 取色笔设置 组节点 选中多个节点, 右击可以选中转换为组节点:\n组节点可以同样的操作展开为单节点, 组节点还可以进行管理, 右击组节点:\n节点分组 选中多个节点, Ctrl + g 将节点分为一个组:\n一个组的节点, 拖动组边框, 将拖动全部组内节点, 右击组边框:\n可以让边框适应节点, 选中组内全部节点, 禁用组内全部节点, 绕过组内全部节点, 编辑组 (固定, 设置标题, 颜色, 标题字体大小, 移除组, 不会移除节点)\n快捷键附录 Keybind Explanation Ctrl + Enter Queue up current graph for generation 当前工作流执行生成 Ctrl + Shift + Enter Queue up current graph as first for generation 将当前工作流排到第一, 如果当前执行队列为空, 则直接执行生成 Ctrl + Alt + Enter Cancel current generation 终止当前生成 Ctrl + Z/Ctrl + Y Undo/Redo 撤销/重做 Ctrl + S Save workflow 保存工作流 Ctrl + O Load workflow 从文件夹中加载已有工作流 Ctrl + A Select all nodes 选中所有节点 Alt + C Collapse/uncollapse selected nodes 折叠/展开所选节点 Ctrl + M Mute/unmute selected nodes 禁用/解禁所选节点 Ctrl + B Bypass selected nodes 绕过所选节点, 就像节点不存在, 直接绕过执行 Delete/Backspace Delete selected nodes 删除所选节点 Ctrl + Backspace Delete the current graph 删除当前工作流 Space Move the canvas around when held and moving the cursor 按住鼠标可空格键, 拖动工作流画布 Ctrl/Shift + Click Add clicked node to selection 点击选中多个节点 Ctrl + C/Ctrl + V Copy and paste selected nodes 没有连接线复制/粘贴所选节 Ctrl + C/Ctrl + Shift + V Copy and paste selected nodes 保持连接线复制/粘贴所选节点 Shift + Drag Move multiple selected nodes at the same time 同时移动选中的所有节点 Ctrl + D Load default graph 加载默认工作流 Alt + + Canvas Zoom in 放大工作流画布 Alt + - Canvas Zoom out 缩小工作流画布 Ctrl + Shift + LMB + Vertical drag Canvas Zoom in/out 缩/放工作流画布 P Pin/Unpin selected nodes 锁定/解锁所选节点 Ctrl + G Group selected nodes 将所选节点分组 Q Toggle visibility of the queue 查看队列执行历史 H Toggle visibility of history 切换历史记录的可见性 R Refresh graph 刷新工作流 Double-Click LMB Open node quick search palette 打开节点快速搜索对话框 Shift + Drag Move multiple wires at once 点击连接点/线, 然后拖动去找新的连接点 Ctrl + Alt + LMB Disconnect all wires from clicked slot 点击连接点, 断开所有线 Alt + 拖动任意一个节点 复制一个拖动的节点 滚动鼠标中键 缩放画布 鼠标左键按住空白处拖动 移动整个工作流 ",
    "description": "",
    "tags": null,
    "title": "内置节点",
    "uri": "/02_comfyui/03_built-in/"
  },
  {
    "content": "视频 prs-eth/RollingDepth: Video Depth without Video Models 2024-12-03\nTencent/HunyuanVideo 2024-12-03\nhmrishavbandy/FlipSketch: FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations 2024-12-02\nKwaiVGI/LivePortrait: Bring portraits to life! 2024-12-02\nC0untFloyd/roop-unleashed: Evolved Fork of roop with Web Server and lots of additions 2024-12-02\njdh-algo/JoyVASA 2024-12-02\nPKU-YuanGroup/ConsisID: Identity-Preserving Text-to-Video Generation by Frequency Decomposition 2024-12-02\nrhymes-ai/Allegro: Allegro is a powerful text-to-video model that generates high-quality videos up to 6 seconds at 15 FPS and 720p resolution from simple text input. 2024-12-02\nk4yt3x/video2x: A machine learning-based lossless video super resolution framework. Est. Hack the Valley II, 2018. 2024-11-27\nfacefusion/facefusion: Industry leading face manipulation platform 2024-11-27\nyangchris11/samurai: Official repository of “SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory”\nalibaba/Tora: The official repository for paper “Tora: Trajectory-oriented Diffusion Transformer for Video Generation”\naigc-apps/CogVideoX-Fun: 📹 A more flexible CogVideoX that can generate videos at any resolution and creates videos from images.\naigc-apps/EasyAnimate: 📺 An End-to-End Solution for High-Resolution and Long Video Generation Based on Transformer Diffusion\nGitHub - HVision-NKU/StoryDiffusion: Create Magic Story!\nhpcaitech/Open-Sora: Open-Sora: Democratizing Efficient Video Production for All\nVision-CAIR/MiniGPT4-video\nhkchengrex/Cutie: [CVPR 2024 Highlight] Putting the Object Back Into Video Object Segmentation\nPicsart-AI-Research/StreamingT2V: StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text\naigc-apps/EasyAnimate: 📺 An End-to-End Solution for High-Resolution and Long Video Generation Based on Transformer Diffusion\nTencent/MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance\njianchang512/pyvideotrans: Translate the video from one language to another and add dubbing. 将视频从一种语言翻译为另一种语言，并支持api调用\nHillobar/Rope: GUI-focused roop\nGitHub - sczhou/CodeFormer: [NeurIPS 2022] Towards Robust Blind Face Restoration with Codebook Lookup Transformer\nHuanshere/VideoLingo: Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | Netflix级字幕切割、翻译、对齐、甚至加上配音，一键全自动视频搬运AI字幕组\njy0205/Pyramid-Flow: Code of Pyramidal Flow Matching for Efficient Video Generative Modeling\nVision-CAIR/LongVU\nDoubiiu/ToonCrafter: [SIGGRAPH Asia 2024, Journal Track] ToonCrafter: Generative Cartoon Interpolation\nVectorSpaceLab/Video-XL: 🔥🔥First-ever hour scale video understanding models\nanliyuan/Ultralight-Digital-Human: 一个超轻量级、可以在移动端实时运行的数字人模型\nantgroup/echomimic_v2: EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation\nZejun-Yang/AniPortrait: AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation\nfudan-generative-vision/hallo2: Hallo2: Long-Duration and High-Resolution Audio-driven Portrait Image Animation\nantgroup/echomimic: EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditioning\nLordLiang/DrawingSpinUp: (SIGGRAPH Asia 2024) This is the official PyTorch implementation of SIGGRAPH Asia 2024 paper: DrawingSpinUp: 3D Animation from Single Character Drawings\nHelloVision/HelloMeme: The official HelloMeme GitHub site\nKmcode1/SG-I2V: This is the official implementation of SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation.\nfacebookresearch/sapiens: High-resolution models for human tasks.\nAlonzoLeeeooo/StableV2V: The official implementation of the paper titled “StableV2V: Stablizing Shape Consistency in Video-to-Video Editing”.\ngenmoai/mochi: The best OSS video generation models\nTHUDM/CogVideo: text and image to video generation: CogVideoX (2024) and CogVideo (ICLR 2023)\nCyberAgentAILab/TANGO: Official implementation of the paper “TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio-Motion Embedding and Diffusion Interpolation”\nIDEA-Research/MotionCLR: [Arxiv 2024] MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms\nJi4chenLi/t2v-turbo: Code repository for T2V-Turbo and T2V-Turbo-v2\nLightricks/LTX-Video: Official repository for LTX-Video\nComfyUI sipie800/ComfyUI-PuLID-Flux-Enhanced 2024-12-02\nEvilBT/ComfyUI_SLK_joy_caption_two: ComfyUI Node 2024-11-27\nhuchenlei/ComfyUI-layerdiffuse: Layer Diffuse custom nodes 2024-11-27\nkijai/ComfyUI-IC-Light: Using IC-LIght models in ComfyUI 2024-11-27\nkijai/ComfyUI-CogVideoXWrapper\nLightricks/ComfyUI-LTXVideo: LTX-Video Support for ComfyUI\nsmthemex/ComfyUI_EchoMimic: You can using EchoMimic in ComfyUI\nAIFSH/ACE-ComfyUI\nlogtd/ComfyUI-MochiEdit: ComfyUI nodes to edit videos using Genmo Mochi\nkijai/ComfyUI-SUPIR: SUPIR upscaling wrapper for ComfyUI\nHelloVision/ComfyUI_HelloMeme: Official comfyui repository of Hellomeme\nalimama-creative/SDXL_EcomID_ComfyUI\nAIGODLIKE/AIGODLIKE-ComfyUI-Studio: Improve the interactive experience of using ComfyUI, such as making the loading of ComfyUI models more intuitive and making it easier to create model thumbnails\nssitu/ComfyUI_UltimateSDUpscale: ComfyUI nodes for the Ultimate Stable Diffusion Upscale script by Coyote-A.\nGourieff/comfyui-reactor-node: Fast and Simple Face Swap Extension Node for ComfyUI\nlldacing/ComfyUI_BiRefNet_ll\nAIGODLIKE/ComfyUI-BlenderAI-node: Used for AI model generation, next-generation Blender rendering engine, texture enhancement\u0026generation (based on ComfyUI)\nsmthemex/ComfyUI_Hallo2: ComfyUI_Hallo2: Long-Duration and High-Resolution Audio-driven Portrait Image Animation\nkijai/ComfyUI-Florence2: Inference Microsoft Florence2 VLM\nCY-CHENYUE/ComfyUI-Molmo: Generate detailed image descriptions and analysis using Molmo models in ComfyUI.\nyolain/ComfyUI-Easy-Use: In order to make it easier to use the ComfyUI, I have made some optimizations and integrations to some commonly used nodes.\nsipherxyz/comfyui-art-venture\nGiusTex/ComfyUI-DiffusersImageOutpaint: Diffusers Image Outpaint for ComfyUI\nXLabs-AI/x-flux-comfyui\nT8star1984/Comfyui-Aix-NodeMap: Comfyui’s latest node organization and annotation, continuously updated, and supported by the Aix team/comfyui最新节点整理及注释，持续更新，AIX团队\nT8star1984/Comfyui-Aix-NodeMap: Comfyui’s latest node organization and annotation, continuously updated, and supported by the Aix team/comfyui最新节点整理及注释，持续更新，AIX团队\nlogtd/ComfyUI-Fluxtapoz: Nodes for image juxtaposition for Flux in ComfyUI\nWASasquatch/was-node-suite-comfyui: An extensive node suite for ComfyUI with over 210 new nodes\ncubiq/ComfyUI_IPAdapter_plus\ncubiq/ComfyUI_InstantID\ncubiq/ComfyUI_InstantID\nZHO-ZHO-ZHO/ComfyUI-InstantID: Unofficial implementation of InstantID for ComfyUI\nkijai/ComfyUI-MochiWrapper\nkijai/ComfyUI-LivePortraitKJ: ComfyUI nodes for LivePortrait\nPowerHouseMan/ComfyUI-AdvancedLivePortrait\nTemryL/ComfyUI-IDM-VTON: ComfyUI adaptation of IDM-VTON for virtual try-on.\ncity96/ComfyUI-GGUF: GGUF Quantization support for native ComfyUI models\nFizzleDorf/ComfyUI_FizzNodes: Custom Nodes for Comfyui\nbalazik/ComfyUI-PuLID-Flux: PuLID-Flux ComfyUI implementation\nkijai/ComfyUI-PyramidFlowWrapper\nstavsap/comfyui-ollama\nltdrdata/ComfyUI-Manager: ComfyUI-Manager is an extension designed to enhance the usability of ComfyUI. It offers management functions to install, remove, disable, and enable various custom nodes of ComfyUI. Furthermore, this extension provides a hub feature and convenience functions to access a wide range of information within ComfyUI.\nerosDiffusion/ComfyUI-enricos-nodes: Compositor Node experiments\nStartHua/Comfyui_CXH_joy_caption: Recommended based on comfyui node pictures:Joy_caption + MiniCPMv2_6-prompt-generator + florence2\nZHO-ZHO-ZHO/ComfyUI-YoloWorld-EfficientSAM: Unofficial implementation of YOLO-World + EfficientSAM for ComfyUI\nlogtd/ComfyUI-Fluxtapoz: Nodes for image juxtaposition for Flux in ComfyUI\nGreenLandisaLie/AuraSR-ComfyUI: ComfyUI implementation of AuraSR\nJonseed/ComfyUI-Detail-Daemon: A port of muerrilla’s sd-webui-Detail-Daemon as a node for ComfyUI, to adjust sigmas that control detail.\ntaabata/ComfyCanvas: Canvas to use with ComfyUI\njtydhr88/ComfyUI-Hunyuan3D-1-wrapper: ComfyUI Hunyuan3D-1-wrapper is a custom node that allows you to run Tencent/Hunyuan3D-1 in ComfyUI as a wrapper.\nsmthemex/ComfyUI_Sapiens: You can call Using Sapiens to get seg，normal，pose，depth，mask\n1038lab/ComfyUI-RMBG: A ComfyUI node for removing image backgrounds using RMBG-2.0.\nTTPlanetPig/Comfyui_Object_Migration: This is a study aim to transfer the single concept by using DIT model self-attention capablity\nDoctorDiffusion/ComfyUI-BEN: Background Erase Network - Remove backgrounds from images within ComfyUI.\nmarduk191/ComfyUI-Fluxpromptenhancer: A Prompt Enhancer for flux.1 in ComfyUI\nLightricks/ComfyUI-LTXVideo: LTX-Video Support for ComfyUI\nWebUI open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, …)\ncontinue-revolution/sd-webui-segment-anything: Segment Anything for Stable Diffusion WebUI\nlllyasviel/stable-diffusion-webui-forge\naigc-apps/sd-webui-EasyPhoto: 📷 EasyPhoto | Your Smart AI Photo Generator.\nLLM hiroi-sora/Umi-OCR: OCR software, free and offline. 开源、免费的离线OCR软件。支持截屏/批量导入图片，PDF文档识别，排除水印/页眉页脚，扫描/生成二维码。内置多国语言库。 2024-12-03\nSignificant-Gravitas/AutoGPT: AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters. 2024-12-03\nOpenBMB/ChatDev: Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration) 2024-12-03\nTHUDM/GLM-4-Voice: GLM-4-Voice | 端到端中英语音对话模型\noobabooga/text-generation-webui: A Gradio web UI for Large Language Models.\njanhq/jan: Jan is an open source alternative to ChatGPT that runs 100% offline on your computer. Multiple engine support (llama.cpp, TensorRT-LLM)\nollama/ollama: Get up and running with Llama 3.2, Mistral, Gemma 2, and other large language models.\nbinary-husky/gpt_academic: 为GPT/GLM等LLM大语言模型提供实用化交互接口，特别优化论文阅读/润色/写作体验，模块化设计，支持自定义快捷按钮\u0026函数插件，支持Python和C++等项目剖析\u0026自译解功能，PDF/LaTex论文翻译\u0026总结功能，支持并行问询多种LLM模型，支持chatglm3等本地模型。接入通义千问, deepseekcoder, 讯飞星火, 文心一言, llama2, rwkv, claude2, moss等。\nSillyTavern/SillyTavern: LLM Frontend for Power Users.\nmendableai/firecrawl: 🔥 Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API.\nInternLM/InternLM: Official release of InternLM2.5 base and chat models. 1M context support\n训练脚本 hiyouga/LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs (ACL 2024) 2024-12-02\nkohya-ss/sd-scripts\ncocktailpeanut/fluxgym: Dead simple FLUX LoRA training UI with LOW VRAM support\nkijai/ComfyUI-FluxTrainer\nReleases · bmaltais/kohya_ss\nAkegarasu/lora-scripts: LoRA \u0026 Dreambooth training scripts \u0026 GUI use kohya-ss’s trainer, for diffusion model.\nNerogar/OneTrainer: OneTrainer is a one-stop solution for all your stable diffusion training needs.\n图像设计 chengyou-jia/ChatGen 2024-12-02\nerwold/qwen2vl-flux 2024-11-27\nYuanshi9815/OminiControl: A minimal and universal controller for FLUX.1. 2024-11-27\nlllyasviel/sd-forge-layerdiffuse: [WIP] Layer Diffusion for WebUI (via Forge) 2024-11-27\nali-vilab/ACE: All-round Creator and Editor\nmit-han-lab/hart: HART: Efficient Visual Generation with Hybrid Autoregressive Transformer\nZhengPeng7/BiRefNet: [CAAI AIR'24] Bilateral Reference for High-Resolution Dichotomous Image Segmentation\nYangLing0818/IterComp: IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation\nxinsir6/ControlNetPlus: ControlNet++: All-in-one ControlNet for image generations and editing!\nKwai-Kolors/Kolors: Kolors Team\nXiaojiu-z/Stable-Hair: Stable-Hair: Real-World Hair Transfer via Diffusion Model\nyisol/IDM-VTON: [ECCV2024] IDM-VTON : Improving Diffusion Models for Authentic Virtual Try-on in the Wild\nbcmi/libcom: Image composition toolbox: everything you want to know about image composition or object insertion\nPixArt-alpha/PixArt-alpha: PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis\nblack-forest-labs/flux: Official inference repo for FLUX.1 models\nStability-AI/sd3.5\nlllyasviel/Omost: Your image is almost there!\ngligen/GLIGEN: Open-Set Grounded Text-to-Image Generation\nTencent/HunyuanDiT: Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\nlllyasviel/IC-Light: More relighting!\ntencent-ailab/IP-Adapter: The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt.\npiddnad/DDColor: [ICCV 2023] Official implementation of “DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders”\ncumulo-autumn/StreamDiffusion: StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation\nToTheBeginning/PuLID: [NeurIPS 2024] Official code for PuLID: Pure and Lightning ID Customization via Contrastive Alignment\nKDE/krita: Krita is a free and open source cross-platform application that offers an end-to-end solution for creating digital art files from scratch built on the KDE and Qt frameworks.\nAcly/krita-ai-diffusion: Streamlined interface for generating images with AI in Krita. Inpaint and outpaint with optional text prompt, no tweaking required.\ninstantX-research/InstantID: InstantID: Zero-shot Identity-Preserving Generation in Seconds 🔥\njbilcke-hf/FacePoke: Select a portrait, click to move the head around (please use your own space / GPU!)\ncatcathh/UltraPixel: Implementation of UltraPixel: Advancing Ultra-High-Resolution Image Synthesis to New Peaks\nZeyi-Lin/HivisionIDPhotos: ⚡️HivisionIDPhotos: a lightweight and efficient AI ID photos tools. 一个轻量级的AI证件照制作算法。\nVectorSpaceLab/OmniGen: OmniGen: Unified Image Generation. https://arxiv.org/pdf/2409.11340\nshallowdream204/DreamClear: [NeurIPS 2024🔥] DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation\nNVlabs/consistory\ninstantX-research/Regional-Prompting-FLUX: Training-free Regional Prompting for Diffusion Transformers 🔥\nali-vilab/In-Context-LoRA: Official repository of In-Context LoRA for Diffusion Transformers\nmit-han-lab/nunchaku: SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models\nChenyangSi/FreeU: FreeU: Free Lunch in Diffusion U-Net (CVPR2024 Oral)\nmagic-quill/MagicQuill: Official Implementations for Paper - MagicQuill: An Intelligent Interactive Image Editing System\nNutlope/logocreator: A free + OSS logo generator powered by Flux on Together AI\nNVlabs/Sana: SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer\nJackAILab/ConsistentID: Customized ID Consistent for human\nDepthAnything/Depth-Anything-V2: [NeurIPS 2024] Depth Anything V2. A More Capable Foundation Model for Monocular Depth Estimation\ntryonlabs/FLUX.1-dev-LoRA-Outfit-Generator: FLUX.1-dev LoRA Outfit Generator can create an outfit by detailing the color, pattern, fit, style, material, and type.\n语音, 音乐 netease-youdao/EmotiVoice: EmotiVoice 😊: a Multi-Voice and Prompt-Controlled TTS Engine\nhaidog-yaqub/EzAudio: High-quality Text-to-Audio Generation with Efficient Diffusion Transformer\n2noise/ChatTTS: A generative speech model for daily dialogue.\nBytedanceSpeech/seed-tts-eval\nRVC-Project/Retrieval-based-Voice-Conversion-WebUI: Easily train a good VC model with voice data \u003c= 10 mins!\nGitHub - yxlllc/DDSP-SVC: Real-time end-to-end singing voice conversion system based on DDSP (Differentiable Digital Signal Processing)\nvoicepaw/so-vits-svc-fork: so-vits-svc fork with realtime support, improved interface and more features.\nGitHub - RVC-Boss/GPT-SoVITS: 1 min voice data can also be used to train a good TTS model! (few shot voice cloning)\nSWivid/F5-TTS: Official code for “F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching”\nmisya11p/amt-apc: AMT-APC: AMT-APC: Automatic Piano Cover by Fine-Tuning an Automatic Music Transcription Model\nWEIFENG2333/AsrTools: ✨ AsrTools: 智能语音转文字工具 | 高效批处理 | 用户友好界面 | 无需 GPU |支持 SRT/TXT 输出 | 让您的音频瞬间变成精确文字！\nopen-mmlab/Amphion: Amphion (/æmˈfaɪən/) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.\nfishaudio/fish-speech: Brand new TTS solution\n3D VAST-AI-Research/TripoSR 2024-11-27\nmicrosoft/MoGe: MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision\nHengyiWang/spann3r: 3D Reconstruction with Spatial Memory\nTencent/Hunyuan3D-1\nwenqsun/DimensionX: DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion\n文本处理 zyddnys/manga-image-translator: Translate manga/image 一键翻译各类图片内文字 https://cotrans.touhou.ai/\nchidiwilliams/buzz: Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI’s Whisper.\nAgentEra/Agently-Daily-News-Collector: An open-source LLM based automatically daily news collecting workflow showcase powered by Agently AI application development framework.\nLC044/WeChatMsg: 提取微信聊天记录，将其导出成HTML、Word、Excel文档永久保存，对聊天记录进行分析生成年度聊天报告，用聊天数据训练专属于个人的AI聊天助手\ngabrielchua/open-notebooklm: Convert any PDF into a podcast episode!\ngetomni-ai/zerox: Zero shot pdf OCR with gpt-4o-mini\nopendatalab/PDF-Extract-Kit: A Comprehensive Toolkit for High-Quality PDF Content Extraction\nNutlope/llama-ocr: Document to Markdown OCR library with Llama 3.2 vision\nopendatalab/MinerU: A high-quality tool for convert PDF to Markdown and JSON.一站式开源高质量数据提取工具，将PDF转换成Markdown和JSON格式。\n其他 showlab/ShowUI: Repository for ShowUI: One Vision-Language-Action Model for GUI Visual Agent 2024-12-02\nturboderp/exllamav2: A fast inference library for running LLMs locally on modern consumer-class GPUs 2024-12-02\ninstructor-ai/instructor: structured outputs for llms 2024-12-02\nComprehensive Guide to Prompting Techniques - Instructor 2024-12-02\nhuggingface/transformers.js: State-of-the-art Machine Learning for the web. Run 🤗 Transformers directly in your browser, with no need for a server! 2024-12-02\nUcas-HaoranWei/GOT-OCR2.0: Official code implementation of General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\ndeepseek-ai/DeepSeek-VL: DeepSeek-VL: Towards Real-World Vision-Language Understanding\ndynobo/normcap: OCR powered screen-capture tool to capture information instead of images\nmodelscope/DiffSynth-Studio: Enjoy the magic of Diffusion models!\nabi/screenshot-to-code: Drop in a screenshot and convert it to clean code (HTML/Tailwind/React/Vue)\nstackblitz/bolt.new: Prompt, run, edit, and deploy full-stack web applications\nlean-dojo/LeanCopilot: LLMs as Copilots for Theorem Proving in Lean\ngeekan/MetaGPT: 🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming\nprinceton-nlp/SWE-agent: [NeurIPS 2024] SWE-agent takes a GitHub issue and tries to automatically fix it, using GPT-4, or your LM of choice. It can also be employed for offensive cybersecurity or competitive coding challenges.\nOpenCodeInterpreter/OpenCodeInterpreter: OpenCodeInterpreter is a suite of open-source code generation systems aimed at bridging the gap between large language models and sophisticated proprietary systems like the GPT-4 Code Interpreter. It significantly enhances code generation capabilities by integrating execution and iterative refinement functionalities.\nIkaros-521/AI-Vtuber: AI Vtuber是一个由 【ChatterBot/ChatGPT/claude/langchain/chatglm/text-gen-webui/闻达/千问/kimi/ollama】 驱动的虚拟主播【Live2D/UE/xuniren】，可以在 【Bilibili/抖音/快手/微信视频号/拼多多/斗鱼/YouTube/twitch/TikTok】 直播中与观众实时互动 或 直接在本地进行聊天。它使用TTS技术【edge-tts/VITS/elevenlabs/bark/bert-vits2/睿声】生成回答并可以选择【so-vits-svc/DDSP-SVC】变声；指令协同SD画图。\nGitHub - 3b1b/manim: Animation engine for explanatory math videos\nGitHub - ManimCommunity/manim: A community-maintained Python framework for creating mathematical animations.\nGitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks\nGitHub - PeterH0323/Streamer-Sales: Streamer-Sales 销冠 —— 卖货主播大模型，一个能够根据给定的商品特点对商品进行解说并激发用户的购买意愿的卖货主播模型\nFujiwaraChoki/MoneyPrinter: Automate Creation of YouTube Shorts using MoviePy.\nprinceton-nlp/SWE-agent: SWE-agent takes a GitHub issue and tries to automatically fix it, using GPT-4. It solves 12.29% of bugs in the SWE-bench evaluation set (comparable to Devin) and take just 1.5 minutes to run (7x faster than Devin).\nharry0703/MoneyPrinterTurbo: 利用AI大模型，一键生成高清短视频 Generate short videos with one click using AI LLM.\nidootop/mi-gpt: 🏠 将小爱音箱接入 ChatGPT 和豆包，改造成你的专属语音助手。\nwan-h/awesome-digital-human-live2d: Awesome Digital Human\nopenai/swarm: Educational framework exploring ergonomic, lightweight multi-agent orchestration. Managed by OpenAI Solution team.\nmeta-llama/llama-recipes: Scripts for fine-tuning Meta Llama with composable FSDP \u0026 PEFT methods to cover single/multi-node GPUs. Supports default \u0026 custom datasets for applications such as summarization and Q\u0026A. Supporting a number of candid inference solutions such as HF TGI, VLLM for local or cloud deployment. Demo apps to showcase Meta Llama for WhatsApp \u0026 Messenger.\nHqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。\nHannibal046/Awesome-LLM: Awesome-LLM: a curated list of Large Language Model\nexcalidraw/excalidraw: Virtual whiteboard for sketching hand-drawn like diagrams\nmeltylabs/melty: Chat first code editor. To download the packaged app:\ngpt-omni/mini-omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities。\n",
    "description": "",
    "tags": null,
    "title": "项目的 GitHub 主页",
    "uri": "/01_aigc_object/02_github/"
  },
  {
    "content": "项目的 Hugging Face Space TryOffDiff - a Hugging Face Space by rizavelioglu 2024-12-03\nQwen2vl Flux Mini Demo - a Hugging Face Space by Djrango 2024-12-02\nIC Light V2-Vary - a Hugging Face Space by lllyasviel 2024-12-02\nIllusionDiffusion - a Hugging Face Space by AP123 2024-12-02\nReplaceAnything - a Hugging Face Space by modelscope 2024-12-02\nQwQ-32B-Preview - a Hugging Face Space by Qwen 2024-12-02\nOminiControl - a Hugging Face Space by Yuanshi 2024-11-27\nACE-Chat - a Hugging Face Space by scepter-studio\nMoGe - a Hugging Face Space by Ruicheng\nEzAudio - a Hugging Face Space by OpenSound\nNaturalSpeech3 FACodec - a Hugging Face Space by amphion\nIDM VTON - a Hugging Face Space by yisol\nAnimateDiff-Lightning - a Hugging Face Space by ByteDance\nOmost - a Hugging Face Space by lllyasviel\nCLIP Interrogator - a Hugging Face Space by pharmapsychotic\nPyramid Flow - a Hugging Face Space by Pyramid-Flow\nJoy Caption Alpha Two - a Hugging Face Space by fancyfeast\nIC Light V2 - a Hugging Face Space by lllyasviel\nMaskGCT TTS Demo - a Hugging Face Space by amphion\nOmniGen - a Hugging Face Space by Shitao\nMotionCLR - a Hugging Face Space by EvanTHU\nSeedEdit-APP-V1.0 - a Hugging Face Space by ByteDance\nFramer - a Hugging Face Space by wwen1997\nBRIA RMBG 2.0 - a Hugging Face Space by briaai\nMinerU - a Hugging Face Space by opendatalab\nQwen Turbo 1M Demo - a Hugging Face Space by Qwen\nDimensionX - a Hugging Face Space by fffiloni\nPhotoMaker V2 - a Hugging Face Space by TencentARC\nOOTDiffusion - a Hugging Face Space by levihsu\nmoondream2 - a Hugging Face Space by vikhyatk\n",
    "description": "",
    "tags": null,
    "title": "Hugging Face Space",
    "uri": "/01_aigc_object/03_hugging/"
  },
  {
    "content": " ",
    "description": "",
    "tags": null,
    "title": "常用自定义节点",
    "uri": "/02_comfyui/04_common-nodes/"
  },
  {
    "content": "许多节点的参数, 背后有很深入的数学原理, 我们如果只是使用它, 不必去深究, 先看一看, 有一个印象, 然后在使用中去调参, 看看参数对结果有什么影响, 就能慢慢熟悉.\n其实简单使用, 很多节点都用不到.\nutils 功能节点 Primitive 元节点 Primitive 节点的主要作用是提供基本数据类型（数字、字符串、布尔值等）供其他节点使用。非常适合初始化参数、调试流程或作为其他复杂操作的输入\n连接到目标节点的输入端口时，ComfyUI 会根据端口类型自动匹配类型.\n例如, 连接到采样器的种子, 他是整数类型\n连接到 cfg 他是小数\n连接到文本编码器, 他是字符串\n连接到采样器节点, 则可以用来选择采样器\n例如这样连接两个采样器节点之后, 可以让两者使用同一个采样器, 还能控制采样器变化, 甚至使用正则表达式过滤采样器 (具体用法高阶在介绍)\n元节点作为布尔值类型时, 可以用作控制流程节点的 分支 或 功能开关 (具体用法高阶在介绍)\nPrimitive 节点就像编程中的变量或常量，虽然功能简单，但在构建复杂流程时非常基础和重要\nNote 注释节点 注释节点就是用来注释你的工作流用的, 让其他人使用你的工作流时看得懂\nReroute 转接节点 转接节点就像水管的三通管, 四通管…\nsampling 采样器节点 Ksampler K采样器节点 前面已经介绍过, 再次提一下, 他是图像生成的核心节点, 任务是使用提供的模型, 正负条件, 设置的参数, 选择的采样器等对潜空间图像去噪, 输出你想要的图像 (还需要 VAE 解码)\nKsampler(Advanced) 高级K采样器节点 它大部分参数和 K 是一样的, 它最大的特点是随机性增加, 灵活性更大\nadd_noise 参数如果为开启, 则会在采样过程中增加随机噪声, 从而增加结果的多样性. 但同时, 开启后，即使种子值相同，结果可能会有显著差异，种子的可复现性降低\nstart_at_step/end_at_step 开始/结束采样步数, 这两个参数可以控制采样在什么阶段进行, 从而增加更多的实验玩法, 以及更灵活的调节.\nend_at_step 可以大于 steps, 但会在总步数 steps 结束就停止\nreturn_with_leftover_noise 主要用于控制是否保留生成图像过程中的剩余噪声，以便在后续的图像生成任务中使用，从而实现更连贯和一致的图像生成效果\ncustom_sampling 自定义采样 SamplerCustom 自定义采样器节点 用于实现自定义采样逻辑的节点，它允许用户对图像生成的采样过程进行更灵活的控制. 将采样器和调度器分成单独节点 (提供了更多的选择, 这些选择后面会具体介绍), 将采样过程的中间结果 denoised_output 也输出\nSamplerCustomAdvanced 高级自定义采样器节点 高级的, 则将噪声, 引导条件, 采样器, 调度器都单独分开了\nsamplers 其他采样器 KsamplerSelect 基础采样器 SamplerEulerAncestral 利用欧拉方法的特性来生成符合特定噪声计划的样本，从而为采样过程做出贡献.\neta参数: 指定欧拉方法的步长，影响采样过程的粒度\ns_noise参数: 确定每一步添加的噪声规模，影响样本的变异性\nSamplerEulerAncestralCFG++ eta参数: 指定采样器的步长，影响采样步长的粒度和生成样本的整体平滑度\ns_noise参数: 确定采样期间应用的噪声的规模\nSamplerLMS 最小均方采样器 order 参数: 指定采样过程中使用的 LMS 算法的阶数。调整此参数可以调整采样器的行为，从而影响生成样本的质量和特性\nSamplerDPMPP_3M_SDE 专门为 DPM-Solver++(3M) SDE 模型设计的采样器，允许根据指定的噪声水平和设备偏好生成样本\neta: 采样过程中应用的噪声规模\ns_noise: 指定采样过程中使用的噪比，影响生成样本的方差\nnoise_device: 确定采样计算是在CPU还是GPU上执行\nSamplerDPMPP_2M_SDE DPMPP_2M_SDE 模型采样器\nsolver_type: 指定采样过程中使用的求解器类型\nSamplerDPMPP_SDE DPM++ SDE（随机微分方程）模型采样器\neta: 指定SDE求解器的步长\ns_noise: 决定采样过程中应用的噪声水平\nr: 控制采样过程中的降噪率\nSamplerDPMPP_2S_Ancestral SamplerDPMAdaptative 用于深度概率建模的采样器，根据动态输入参数和条件优化采样过程。它侧重于通过实时调整参数来提高复杂模型中采样的效率和准确性\norder: 指定采样过程中使用的微分方程求解器的顺序\nrtol: 定义求解器的相对公差\natol: 设置求解器的绝对公差\nh_init: 求解器的初始步长\npcoeff: 系数与自适应算法中的比例控制相关\nicoeff: 自适应算法的积分系数\ndcoeff: 是自适应算法中的导数系数\naccept_safety: 自适应算法中接受步长的安全系数\nSamplerLCMUpscale LCM放大采样器 scale_ratio: 指定图像应放大的比率\nscale_step: 放大过程的步骤数\nupscale_method: 选择用于放大图像的方法\nschedulers 调度器 调度器节点是给自定义采样器节点提供调度器的节点\nBasicScheduler 基础调度器 参数前面章节都介绍过了\nKarrasScheduler K调度器 sigma_max：是采样过程开始时的噪声水平(标准差), 它决定了初始噪声的强度。较高的 sigma_max 值意味着生成过程从更高的噪声水平开始，这可能会导致更多的随机性和多样性，但也可能需要更多的采样步骤来达到高质量的图像\nsigma_min：采样过程结束时的噪声水平, 它决定了最终噪声的强度。较低的 sigma_min 值意味着生成过程会尽量减少噪声，从而生成更清晰和细致的图像。较高的 sigma_min 值可能会保留一些噪声，使图像看起来更自然或艺术化\nrho：控制噪声减少速率的参数。它决定了噪声水平从 sigma_max 减少到 sigma_min 的速度。影响噪声时间表的整体形状和采样动态. 较高的 rho 值会导致噪声更快地减少，从而生成过程更快地达到低噪声水平。较低的 rho 值会导致噪声减少速度较慢，生成过程更缓慢但可能更稳定\nExponentialScheduler 指数衰减调度器 通过指数衰减控制噪声水平，平衡生成过程中的随机性和细节\nPolyexponentialScheduler 多项式指数调度器 LaplaceScheduler 拉普拉斯调度器 mu 参数：是拉普拉斯分布的位置参数，决定了分布的中心位置。影响噪声的平均水平。在大多数情况下，mu 保持为 0 即可，因为噪声通常是对称分布的。如果有特殊需求，可以调整 mu 来偏移噪声分布的中心\nbeta 参数：拉普拉斯分布的尺度参数，决定了分布的宽度或散布程度。参数控制噪声的强度和变化范围。较大的 beta 值会导致更强的噪声，生成过程中的随机性和多样性增加；较小的 beta 值会导致较弱的噪声，生成的图像更清晰和细致\nVPScheduler 方差保持调度器 基于方差保持调度方法生成一系列噪声水平\nbeta_d: 确定整体噪声水平分布，影响生成噪声水平的方差\nbeta_min: 设置噪声水平的最小边界，确保噪声不会低于某个阈值\neps_s: 调整起始的epsilon值，微调扩散过程中的初始噪声水平\nBetaSamplingScheduler alpha：控制噪声分布的左侧形状，较大的值使噪声在初始阶段迅速减少\nbeta：控制噪声分布的右侧形状，较大的值使噪声在后期阶段迅速减少\nSDTurboScheduler SDTurbo调度器 SDXL Turbo 模型使用的调度器 (SDXL Turbo 模型也可以用其他调度器), 其他模型使用基本出不来好图. 最大步数只有 10\nAlignYourStepsScheduler 英伟达推出的优化采样步骤的调度器, 不能低于 10 步, 支持 sd1, sdxl, svd\nGITSScheduler coeff参数: 影响采样过程中使用的噪声级别。允许您调整噪声的强度，这会影响生成图像的纹理和细节, 较高的值可以添加更多纹理，而较低的值可以使图像平滑\nLTXVScheduler LTX-Video 视频模型的调度器\nmax_shift：\nbase_shift：\nstretch：是否将 sigmas 拉伸到 [terminal，1] 范围内\nterminal：sigmas 拉伸后的终端参数\nsigmas SplitSigmas 基于指定的步长索引将一系列西格玛值划分为两个子集. 在生成模型中，操纵噪声水平会显著影响模型的输出质量和多样性\nSplitSigmasDenoise 基于去噪因子将给定的 Sigmas 序列划分为两个不同的序列, 从而能够更精确地控制生成质量\nFlipSigmas 通过颠倒西格玛值的顺序来操纵西格玛值序列\nguiders 引导 CFGGuider DualCFGGuider 将双条件引导因子应用于模型来增强采样过程。它允许指定两个不同的条件及其各自的指导量，从而对生成过程进行更细致的控制\nBasicGuider noise 噪声 RandomNoise DisableNoise 禁用采样过程中的噪声生成\n视频采样器节点 VideoLinearCFGGuidance VideoTriangleCFGGuidance 通过应用在规定时间内线性变化的特定引导模式来增强视频内容的生成\n",
    "description": "",
    "tags": null,
    "title": "内置节点01",
    "uri": "/02_comfyui/03_built-in/01-built-in/"
  },
  {
    "content": "Discover and download free videos - Pixabay\nDanbooru: Anime Image Board\nDiscover the Best GPTs\nAI工具集 | 700+ AI工具集合官网，国内外AI工具集导航大全\nSupertools | Best AI Tools Guide\nAIGC导航 | 1500+全品类AIGC创作工具_探索更多可能！\n插画交流网站[pixiv]\nArtStation - Explore\nAIbase - 智能匹配最适合您的AI产品和网站\nNewsfeed - Sketchfab\nAI Model \u0026 API Providers Analysis | Artificial Analysis\nxAI\nGGAC数字艺术平台\nWeird Wonderful AI Art | ART of the future - now!\n",
    "description": "",
    "tags": null,
    "title": "AIGC 相关网站",
    "uri": "/01_aigc_object/04_web/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "内置节点03",
    "uri": "/02_comfyui/03_built-in/03-built-in/"
  },
  {
    "content": "使用 diffusers 训练你自己的 ControlNet 🧨\nStable Diffusion QR Code 101\nE-Hentai/태그 - 나무위키\n魔咒百科词典\nSo-VITS-SVC 4.1 整合包完全指南\nStable Diffusion 3.5 Prompt Guide — Stability AI\n使用 ChatGPT 进行写作的学生指南 |开放人工智能 — A Student’s Guide to Writing with ChatGPT | OpenAI\nrichards199999/Thinking-Claude: Let your Claude able to think\nhesamsheikh/ml-retreat: Machine Learning Journal for Intermediate to Advanced Topics.\nMidjourney Documentation and User Guide\n归档(可以不用看) Resources for GAN Artists\nDisco Diffusion Portrait Study (by @enviraldesign) - Google 文档\nalibaba/animate-anything: Fine-Grained Open Domain Image Animation with Motion Guidance\nGitHub - prophesier/diff-svc: Singing Voice Conversion via diffusion model\nTencentARC/GFPGAN: GFPGAN aims at developing Practical Algorithms for Real-world Face Restoration.\nguide to installing disco v5+ locally on windows\nclip_interrogator.ipynb - Colaboratory\nA Traveler’s Guide to the Latent Space\nCoar’s Disco Diffusion Guide\nDisco Diffusion Illustrated Settings\nAi generative art tools\nAI绘画的关键词（群友们的画 ）\nArtist Studies by @remi_durant\nCLIP Prompt Engineering for Generative Art - matthewmcateer.me\n数据集-LAION-400-MILLION OPEN DATASET | LAION\n",
    "description": "",
    "tags": null,
    "title": "相关文档资料",
    "uri": "/01_aigc_object/05_doc/"
  },
  {
    "content": "等待中的项目是指只有论文或介绍, 还不能用的项目.\nFreditor 2024-12-03\nmayuelala/FollowYourClick: [arXiv 2024] Follow-Your-Click: This repo is the official implementation of “Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts” 2024-12-03\nAnchorCrafter 2024-12-02\nGenerative Omnimatte: Learning to Decompose Video into Layers 2024-12-02\nlehduong/OneDiffusion 2024-12-02\nLipDub AI | The most realistic AI lip sync and video translation 2024-12-02\nMyTimeMachine: Personalized Facial Age Transformation 2024-12-02\nBuffer Anytime: Zero-Shot Video Depth and Normal from Image Priors 2024-12-02\nMultiFoley 2024-12-02\nSonic: Shifting Focus to Global Audio Perception in Audio-driven Portrait Animation 2024-12-02\nlewandofskee/MobileMamba: Official implementation of `MobileMamba: Lightweight Multi-Receptive Visual Mamba Network.’ 2024-12-02\nBaking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation 2024-11-27\nFugatto, World’s Most Flexible Sound Machine, Debuts | NVIDIA Blog 2024-11-27\nFashion-VDM: Video Diffusion Model for Virtual Try-On\nInverse Painting: Reconstructing The Painting Process\n首页 |剧集主管 — Home | Showrunner\n悠船\nVideo Ocean视频大模型 - 人人皆导演\nPersonaTalk: Bring Attention to Your Persona in Visual Dubbing\nMarDini: Masked Auto-Regressive Diffusion for Video Generation at Scale – Meta AI Research\n炉米Lumi\nloopyavatar.github.io/?ref=aihub.cn\nGoogle Vids：在线视频创建和编辑器 | Google Vids谷歌工作区 — Google Vids: Online Video Creator and Editor | Google Workspace\nSkyReels\nURAvatar: Universal Relightable Gaussian Codec Avatars\nMikuDance\nAdd-it\nDanceFusion: A Spatio-Temporal Skeleton Diffusion Transformer for Audio-Driven Dance Motion Reconstruction.\nAnimateAnything\nMIMO\nInstructAvatar\n",
    "description": "",
    "tags": null,
    "title": "等待中的项目",
    "uri": "/01_aigc_object/06_wait/"
  },
  {
    "content": "公众号:\n视频号:\n抖音:\n快手:\n小红书:\nB 站:\n",
    "description": "",
    "tags": null,
    "title": "关注我",
    "uri": "/about/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/"
  }
]
