[
  {
    "content": " AI 发展日新月异, 各种 项目/应用/工具/资源… 令人兴奋, 而且很多都是免费使用, 或者开源.\n内容主要分为 项目的官方网站, 项目的 GitHub 主页, 项目的 Hugging Face Space (可在线试用), AIGC 相关网站, 相关文档资料. 它们之间可能会有重叠的项目, 但非常少, 我做了去重, 确保唯一性.\n以下项目是截至 2024 年 12 月 9 日搜集整理并分类:\n项目的官方网站 项目的 GitHub 主页 Hugging Face Space AIGC 相关网站 相关文档资料 等待中的项目 后面新添加, 都会标注日期:\n项目官方网站 New Chat | glhf.chat 2024-12-15\nedify-3d Model by Shutterstock | NVIDIA NIM 2024-12-15\n豆包 MarsCode - 工作台 2024-12-15\nDevin 2024-12-15\nDeepSeek - 探索未至之境 2024-12-15\nSora 2024-12-15\nDeepLearning.AI - Learning Platform 2024-12-15\nD5渲染器官网 | 实时光追渲染技术，重塑3D创作工作流 2024-12-15\nPromptPerfect - AI Prompt Generator and Optimizer 2024-12-15\nLearn Prompting: Your Guide to Communicating with AI 2024-12-15\nGitHub 项目 hacksider/Deep-Live-Cam: real time face swap and one-click video deepfake with only a single image 2024-12-15\ndatawhalechina/llm-cookbook: 面向开发者的 LLM 入门教程，吴恩达大模型系列课程中文版 2024-12-15\nf/awesome-chatgpt-prompts: This repo includes ChatGPT prompt curation to use ChatGPT better. 2024-12-15\nStability-AI/stable-audio-tools: Generative models for conditional audio generation 2024-12-15\nisarandi/nlf: [NeurIPS 2024] Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation 2024-12-15\nStability-AI/stable-fast-3d: SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement 2024-12-15\nlihxxx/DisPose: This repository is the official implementation of DisPose 2024-12-15\nfkryan/gazelle 2024-12-15\ntdrussell/diffusion-pipe: A pipeline parallel training script for diffusion models. 2024-12-15\nopenai/openai-cookbook: Examples and guides for using the OpenAI API 2024-12-15\npromptslab/Awesome-Prompt-Engineering: This repository contains a hand-curated resources for Prompt Engineering with a focus on Generative Pre-trained Transformer (GPT), ChatGPT, PaLM etc 2024-12-15\nthunlp/Delta-CoMe: Delta-CoMe can achieve near loss-less 1-bit compressin which has been accepted by NeurIPS 2024 2024-12-15\nHugging Face FlowEdit - a Hugging Face Space by fallenshock 2024-12-15\n等待中的项目 Project Astra - Google DeepMind 2024-12-15\nProject Mariner - Google DeepMind 2024-12-15\nJules (Confidential) 2024-12-15\nMotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance 2024-12-15\nSwiftEdit 2024-12-15\nMichael Fischer 2024-12-15\nUsing Diffusion Priors for Video Amodal Segmentation 2024-12-15\n项目官方网站 Fish Audio: Free Generative AI Text To Speech \u0026 Voice Cloning 2024-12-09\nGenerative Foundation Model - Amazon Nova - AWS 2024-12-09\nRunComfy: Top ComfyUI Platform - Fast \u0026 Easy, No Setup 2024-12-09\n提示工程指南 | Prompt Engineering Guide 2024-12-09\nPrompt Engineering Guide | Prompt Engineering Guide 2024-12-09\nHailuo AI Audio: Create lifelike speech 2024-12-09\nGitHub 项目 FunAudioLLM/CosyVoice: Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability. 2024-12-09\nFunAudioLLM/SenseVoice: Multilingual Voice Understanding Model 2024-12-09\nmodelscope/FunASR: A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc. 2024-12-09\nyformer/EfficientTAM: Efficient Track Anything 2024-12-09\njingyaogong/minimind: 「大模型」3小时完全从0训练26M的小参数GPT，个人显卡即可推理训练！ 2024-12-09\nkijai/ComfyUI-HunyuanVideoWrapper 2024-12-09\njianchang512/clone-voice: A sound cloning tool with a web interface, using your voice or any sound to record audio / 一个带web界面的声音克隆工具，使用你的音色或任意声音来录制音频 2024-12-09\ndair-ai/Prompt-Engineering-Guide: 🐙 Guides, papers, lecture, notebooks and resources for prompt engineering 2024-12-09\nmemoavatar/memo: Memory-Guided Diffusion for Expressive Talking Video Generation 2024-12-09\n1jsingh/negtome: Official Implementation for paper: Negative Token Merging: Image-based Adversarial Feature Guidance 2024-12-09\nmicrosoft/TRELLIS: Official repo for paper “Structured 3D Latents for Scalable and Versatile 3D Generation”. 2024-12-09\nFrancis-Rings/StableAnimator: We present StableAnimator, the first end-to-end ID-preserving video diffusion framework, which synthesizes high-quality videos without any post-processing, conditioned on a reference image and a sequence of poses. 2024-12-09\nHugging Face CosyVoice-300M · 创空间 2024-12-09\nChatTTS Speaker - a Hugging Face Space by taa 2024-12-09\nFlux Fill Outpainting - a Hugging Face Space by multimodalart 2024-12-09\nFlux.1-dev Upscaler - a Hugging Face Space by jasperai 2024-12-09\nFlux.1-dev Upscaler - a Hugging Face Space by Nymbo 2024-12-09\n等待中的项目 Muse 2024-12-09\nIntroducing Veo and Imagen 3 on Vertex AI | Google Cloud Blog 2024-12-09\nFLOAT 2024-12-09\nGenie 2: A large-scale foundation world model - Google DeepMind 2024-12-09\nSOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters 2024-12-09\nDigital Life Project 2024-12-09\nI2VControl: Disentangled and Unified Video Motion Synthesis Control 2024-12-09\nDualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction 2024-12-09\nfugatto.github.io 2024-12-09\nCAT4D: Create Anything in 4D with Multi-View Video Diffusion Models 2024-12-09\nSNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance 2024-12-09\nvision-xl 2024-12-09\n",
    "description": "",
    "tags": null,
    "title": "ACGC 项目资源集",
    "uri": "/01_aigc_object/"
  },
  {
    "content": "ComfyUI 的特色:\n节点化工作流程: ComfyUI 允许用户通过节点构建复杂的图像生成流程，每个节点执行特定的功能，例如加载模型、处理图像、应用效果等。这种方式提供了高度的灵活性和自定义能力。\n低显存需求: 与其他类似工具相比，ComfyUI 对显存的要求较低，可以在较低配置的设备上运行。\n快速生成: 优化后的内部流程使其生成图片的速度较快，性能优于一些传统的 WebUI。\n插件支持: ComfyUI 支持丰富的插件生态，用户可以通过插件扩展功能。活跃的开源社区提供了丰富的插件和资源，提升了 ComfyUI 的功能性和适用性。\n适用于复杂场景: 特别擅长处理长期复杂的大型项目，能够将多步骤操作简化为连贯的工作流。\n跨平台支持: ComfyUI 不仅支持 NVIDIA GPU，还能在 CPU 上运行，提供了更广泛的设备兼容性。\n创意自由: 为艺术家、设计师和 AI 研究人员提供了强大的工具，使他们能够实现更精细的图像生成控制和实验。\n学习与教育: 尽管有一定的学习曲线，ComfyUI 为教育和培训提供了良好的平台，可以帮助学生理解图像处理和生成的原理。\n效率提升: 通过简化和自动化复杂的工作流程，ComfyUI 能够显著提升工作效率，特别是在需要频繁生成或修改图像的场景中。\nComfyUI 学习资源集合 官网 ComfyUI 官方桌面版新手指南 ComfyUI Desktop User Guide 所有节点信息 ComfyUI Nodes Info 另一个非常非常棒的节点信息站 ComfyUI Node Documentation ComfyUI Community Manual ComfyUI 解説 (wiki ではない) 界面介绍 基本概念 内置节点 内置节点 1 内置节点 2 内置节点 3 常用自定义节点 ",
    "description": "",
    "tags": null,
    "title": "ComfyUI 资源, 教程",
    "uri": "/02_comfyui/"
  },
  {
    "content": " ",
    "description": "",
    "tags": null,
    "title": "高级提示工程概述",
    "uri": "/04_prompt-engineering/04-prompt-engineering/"
  },
  {
    "content": "默认工作流 长这样, 具体细节将在后面 基本概念 介绍.\n调节视图的组件 分别是 放大, 缩小 (这两个和鼠标中键滚动一样), 适应, 选择/平移 模式 (选择模式可选择节点, 平移模式拖动整个工作流移动, 快捷键是空格键, 按住空格键, 拖动鼠标就平移) 和 显示/关闭 连接线.\n“运行” 管理区域 从左到右分别是 管理器, 卸载模型, 释放, 分享, 队列, 批次, 停止, 清除排队, 显示logs/终端, 隐藏菜单:\n卸载模型 (将加载到显存/内存里的模型卸载)\n释放模型和节点缓存\n分享(将工作流分享出去)\n队列, 分别是 点击执行, 生成完成后执行, 改变工作流后执行:\n运行次数\n停止运行当前任务\n清除等待中的任务\n打开底部 logs 和 终端\n隐藏菜单\n管理器 Skip update check: 跳过更新检查\nDB: 配置检索 节点/模型 信息的位置。默认一天内的缓存, 如果设置为本地，则忽略通道，如果设置为通道（远程），则每次打开列表时都会获取最新信息。\nChannel: 配置通道，以便从自定义节点列表（包括缺失节点）或模型列表中检索数据。请注意，这里使用了本地缓存信息。\nPreview method: 配置在采样过程如何预览图像生成的过程。None 不显示最快, 其他有微小的影响.\nDefault UI: 设置桌面启动时在主菜单中显示的默认状态.\nShare: 隐藏主菜单中的共享按钮，或在点击时设置默认操作。例如, 在通过点击共享按钮共享时，配置默认共享站点.\nComponent: 加载工作流时，配置要使用的组件版本.\nDouble-Click: 设置双击节点标题区域时的行为.\nUnload models: 卸载模型\nCustom Nodes Manager: 管理节点\nInstall Missing Custom Nodes: 安装缺失节点, 如果工作流中显示有节点缺失, 点击这里将进入节点管理进行缺失节点安装.\nModel Manager: 模型管理, 这里管理模型, 例如安装, 卸载.\nInstall via Git URL: 通过 GitHub 链接安装节点.\nUpdate All Nodes: 更新所有节点.\nFetch Updates: 抓取更新.\nRestart: 重启\nCommunity Manual: 社区手册\nNodes Info: 所有节点的信息大全.\nFeatures/Updates News: 更新公告.\n帮助 ComfyUl Issues 提问 ComfyUI Docs 文档 Comfy-Org 社区 Feedback 反馈 Open DevTools 打开开发工具 Reinstall 重装 Open Folder 打开文件夹 Open Logs Folder 打开 logs 文件夹 Open Models Folder 打开模型文件夹 Open Outputs Folder 打开输出文件夹 Open Inputs Folder 打开输入文件夹 Open Custom Nodes Folder 打开节点文件夹 Open extra_model_paths.yaml 打开自定义模型路径文件, 可以将模型或节点放在指定的文件夹中, 然后在这个文件里配置. 编辑 Undo 撤销 Ctrl + Z Redo 重做 Ctrl + Y Refresh Node Definitions 刷新节点, 快捷键 R, 例如刚把模型放入文件夹, 刷新节点就可以加载了. Clear Workflow 清理工作流, 将工作流全部清掉, 选中节点, 可以用快捷键 Backspace 删掉节点. Clipspace: CLIP 空间, 目前只对图像有效, 绘蒙版时使用, 后面会详细介绍. 工作流管理 New 新建 Open 打开已有工作流, 快捷键 Ctrl+o Browse Templates 选择自带工作流模板 Save 保存工作流, 快捷键 Ctrl+s Save As 另存为 Export 导出工作流 Export(API) 导出(包含API) 文件管理 查看队列历史, 应用生成的图像 (右键点击图像选择), 或删除历史\n查看节点库. 鼠标放到节点上, 会显示节点信息, 以及参数介绍, 还可以标星方便使用.\n模型库, 查看模型. 点击模型会自动生成加载节点.\n工作流管理, 点击工作流直接打开, 右击可以重命名, 删除, 应用. 同样可以选择内建模板, 打开已有工作流或新建.\n设置 Comfy 设置 Dev Mode. 开发者模式\nEnable dev mode options (API save, etc.). 启用开发模式选项（API保存等）\nEdit Attention. 编辑 Attention\nCtrl+up/down precision. Ctrl+up/down 调节精度\nLocale. 本地化, 需要安装中文插件才能显示中文\nLogging. 打开/关闭 logging, 出错可以将 logs 发给大神们求助.\nEnable logging\nMenu. 菜单\nUse new menu. 使用新菜单, 可选顶部, 底部, 或关闭.\nSave legacy menu’s position. 保留旧菜单的位置\nModel Library. 模型库\nWhat name to display in the model library tree view. 在模型库中显示什么名称。\nSelect “filename” to render a simplified view of the raw filename (without directory or “.safetensors” extension) in the model list. Select “title” to display the configurable model metadata title. 选择“filename”以在模型列表中呈现原始文件名的简化视图（不带目录或“.safetensors”扩展名）。选择“title”以显示可配置的模型元数据标题。\nAutomatically load all model folders. 自动加载所有模型文件夹。\nIf true, all folders will load as soon as you open the model library (this may cause delays while it loads). If false, root level model folders will only load once you click on them. 如果为true，则打开模型库后将立即加载所有文件夹（这可能会导致加载时的延迟）。如果为false，则只有在您单击根级模型文件夹后才会加载它们。\nNode 节点\nShow experimental nodes in search. 在搜索中显示实验节点\nExperimental nodes are marked as such in the UI and may be subject to significant changes or removal in future versions. Use with caution in production workflows. 实验节点在UI中标记为这样，在未来的版本中可能会发生重大变化或被删除。在生产工作流程中谨慎使用.\nShow deprecated nodes in search. 在搜索中显示已弃用的节点\nDeprecated nodes are hidden by default in the UI, but remain functional in existing workflows that use them. 默认情况下，弃用的节点在UI中隐藏，但在使用它们的现有工作流中仍然可以正常工作。\nNode Search Box. 节点搜索框\nNumber of nodes suggestions. 建议的节点数量 Only for litegraph searchbox/context menu. 仅适用于 litegraph 搜索框/快捷菜单\nShow node frequency in search results. 在搜索结果中显示节点频率 Only applies to the default implementation. 仅适用于默认实现\nShow node id name in search results. 在搜索结果中显示节点 id 名称 Only applies to the default implementation.\nShow node category in search results. 在搜索结果中显示节点类别 Only applies to the default implementation.\nNode preview. 节点预览 Only applies to the default implementation.\nBETA: Node search box implementation. 节点搜索框实现. 可选默认, 或旧版. Node Widget. 节点组件\nWidget control mode. Controls when widget values are updated (randomize/increment/ decrement), either before the prompt is queued or after. 组件控制模式. 控制何时更新组件值(随机/增加/减少)，在提示词队列执行前或执行后\nTextarea widget spellcheck. 文本区域小部件拼写检查\nNode Input Conversion Submenus. 节点输入转换子菜单\nIn the node context menu, place the entries that convert between input/widget in sub-menus. 在节点快捷菜单中，将 输入/小部件 之间转换的条目放置在子菜单中\nQueue\nQueue history size. The maximum number of tasks that show in the queue history. 队列历史记录数。队列历史记录中显示的最大任务数\nQueue Button. 队列按钮\nBatch count limit. The maximum number of tasks added to the queue at one button click. 批次计数限制。一键添加到队列的最大任务数\nSettings. 设置\nBETA: Show extension panel in settings dialog. 在设置对话框中显示扩展面板\nTree Explorer.\nTree explorer item padding\nValidation. 验证\nValidate node definitions (slow). Recommended for node developers. This will validate all node definitions on startup. 验证节点定义（缓慢）。推荐给节点开发人员。这将在启动时验证所有节点定义\nValidate workflows. 验证工作流\nWindow. 窗口\nShow confirmation when closing window. 关闭窗口时显示确认\nWorkflow. 工作流\nSave and restore canvas position and zoom level in workflows. 在工作流中保存, 恢复 画布位置 和 缩放级别\nOpened workflows position. 打开工作流位置, 可选顶部或侧边栏\nPrompt for filename when saving workflow. 保存工作流时提示输入文件名\nRequire confirmation when clearing workflow. 清除工作流时需要确认\nBETA: Show missing models warning. 显示缺失模型警告\nShow missing nodes warning. 显示缺失节点警告\nSort node IDs when saving workflow. 保存工作流时对节点ID进行排序\nLiteGraph 设置 Canvas 画布\nAlways snap to grid. 始终捕捉到网格\nSnap to grid size. When dragging and resizing nodes while holding shift they will be aligned to the grid, this controls the size of that grid. 对齐网格大小。按住shift键拖动和调整节点大小时，它们将与网格对齐，这控制了网格的大小\nEnable fast-zoom shortcut (Ctrl + Shift + Drag). 启用快速缩放快捷方式（Ctrl+Shift+拖动）\nShow graph canvas menu. 显示图形画布菜单\nCanvas zoom speed. 画布缩放速度\nShow canvas info on bottom left corner (fps, etc.). 在左下角显示画布信息（fps 等）\nGraph\nLink Render Mode. 连接线渲染模式 (曲线, 直线等)\nGroup 组\nDouble click group title to edit. 双击组标题进行编辑\nGroup selected nodes padding. 组边距\nLink 连线\nLink midpoint markers. 连线中点标记 (圆点, 箭头)\nLink Release. 连线松开\nAction on link release (Shift). 按住 Shift 松开连线\nAction on link release (No modifier). 不按 Shift 松开连线\nMenu 菜单\nInvert Context Menu Scrolling. 反转快捷菜单滚动\nNode 节点\nEnable DOM element clipping (enabling may reduce performance). 启用DOM元素剪裁（启用可能会降低性能）\nMiddle-click creates a new Reroute node. 单击鼠标中键可创建新的 “转接” 节点\nKeep all links when deleting nodes. When deleting a node, attempt to reconnect all of its input and output links (bypassing the deleted node). 删除节点时保留所有链接. 删除节点时，尝试重新连接其所有输入和输出链路（绕过已删除的节点）\nSnap highlights node. When dragging a link over a node with viable input slot, highlight the node. 捕捉高亮节点. 在拖动线经过具有可输入点的节点时，突出显示该节点\nAuto snap link to node slot. 自动捕捉可连接线到节点插槽\nEnable Tooltips. 启用工具提示\nNode life cycle badge mode. 节点激活标记\nNode ID badge mode. 节点 ID 标记\nNode source badge mode. 节点来源标记\nDouble click node title to edit. 双击节点标题编辑\nNode Widget\nFloat widget rounding decimal places [0 = auto]. (requires page reload). 浮点控件四舍五入小数位数[0=auto]。（需要重新加载页面）\nDisable default float widget rounding. (requires page reload) Cannot disable round when round is set by the node in the backend. 禁用默认浮点控件舍入。（需要重新加载页面）当舍入由后端节点设置时，无法禁用舍入\nDisable node widget sliders. 禁用节点小部件滑块\nPreview image format. When displaying a preview in the image widget, convert it to a lightweight image, e.g. webp, jpeg, webp;50, etc. 预览图像格式. 在图像小部件中显示预览时，将其转换为轻量级图像，例如webp、jpeg、webp；50等\nPointer 鼠标指针\nDouble click interval (maximum). The maximum time in milliseconds between the two clicks of a double-click. Increasing this value may assist if double-clicks are sometimes not registered. 双击间隔（最大值）。双击两次之间的最长时间（毫秒）。如果双击有时未生效，则增加此值可能会有所帮助\nBETA: Pointer click drift delay. After pressing a pointer button down, this is the maximum time (in milliseconds)that pointer movement can be ignored for. Helps prevent objects from being unintentionally nudged if the pointer is moved whilst clicking. 指针点击漂移延迟。按下指针按钮后，这是指针移动可以忽略的最长时间（以毫秒为单位）。有助于防止在单击时移动指针时无意中轻推对象\nBETA: Pointer click drift (maximum distance). If the pointer moves more than this distance while holding a button down, it is considered dragging (rather than clicking). Helps prevent objects from being unintentionally nudged if the pointer is moved whilst clicking. 指针点击漂移（最大距离）。如果指针在按住按钮的同时移动超过此距离，则被视为拖动（而不是单击）。有助于防止在单击时移动指针时无意中轻推对象\nReroute Beta. 转接节点测试\nBETA: Opt-in to the reroute beta test. Enables the new native reroutes. Reroutes can be added by holding alt and dragging from a link line, or on the link menu. Disabling this option is non-destructive- reroutes are hidden. 选择转接节点测试。启用新的转接节点。通过按住alt并从连接线或连接菜单上拖动，可以添加转接节点。禁用此选项是非破坏性的-转接节点被隐藏\nAppearance 外观 Color Palette 主题\nNode\nNode opacity. 节点不透明度\nNode Widget\nTextarea widget font size. 文本区域小部件字体大小\nSidebar\nSidebar size. 侧边栏尺寸\nSidebar location. 侧边栏位置\nMask Editor 蒙版编辑 Brush Adjustment 笔刷调整\nBETA: Lock brush adjustment to dominant axis. When enabled, brush adjustments will only affect size OR hardness based on which direction you move more. 将笔刷调整锁定到主导轴. 启用后，刷子调整只会根据您移动的方向影响尺寸或硬度\nBETA: Brush adjustment speed multiplier. Controls how quickly the brush size and hardness change when adjusting. Higher values mean faster changes. 笔刷调整速度倍数. 控制调整时画笔大小和硬度的变化速度。值越高，变化越快\nNew Editor\nBETA: Use new mask editor. 启动新的蒙版编辑\nComfy-Desktop 桌面设置 General 一般设置\nSend anonymous crash reports. 发送匿名事件报告\nAutomatically check for updates. 自动检查更新\nKeybinding 编辑快捷键 (可自定义) Comfy.NewBlankWorkflow 新建工作流\nComfy.OpenWorkflow 打开工作流 Ctrl+o\nComfy.LoadDefaultWorkflow 加载默认工作流\nComfy.SaveWorkflow 保存工作流 Ctrl+s\nComfy.SaveWorkflowAs 工作流另存为\nComfy.ExportWorkflow 导出工作流\nComfy.ExportWorkflowAPI 导出工作流(API)\nComfy.Undo 撤销\nComfy.Redo 重做\nComfy.ClearWorkflow 删除工作流 Backspace\nComfy.Canvas.ResetView 重设视图\nComfy.OpenClipspace 打开 Clipspace\nComfy.RefreshNodeDefinitions 刷新节点 r\nComfy.Interrupt 停止 Ctrl+Alt+Enter\nComfy.ClearPendingTasks 清除等待任务\nComfy.BrowseTemplates 打开默认工作流模板\nComfy.Canvas.ZoomIn 放大 Alt+=\nComfy.Canvas.ZoomOut 缩小 Alt+-\nComfy.Canvas.FitView 适应窗口 .\nComfy.Canvas.ToggleLock 切换 选择/拖动 模式\nComfy.Canvas.ToggleLinkVisibility 切换连接线 可见/不可见\nComfy.QueuePrompt Ctrl+Enter\nComfy.QueuePromptFront Ctrl+Shift+Enter\nComfy.ShowSettingsDialog 打开设置面板 Ctrl+,\nComfy.Graph.GroupSelectedNodes 将选定节点一组 Ctrl+g\nWorkspace.NextOpenedWorkflow 切换到下一个打开的工作流\nWorkspace.PreviousOpenedWorkflow 切换到上一个打开的工作流\nComfy.Canvas.ToggleSelectedNodes.Mute 屏蔽选择的节点 Ctrl+m\nComfy.Canvas.ToggleSelectedNodes.Bypass 禁用选择的节点 Ctrl+b\nComfy.Canvas.ToggleSelectedNodes.Pin\nComfy.Canvas.ToggleSelected.Pin 锁定或解锁选择的节点或组 p\nComfy.Canvas.ToggleSelectedNodes.Collapse 折叠选择的节点 Alt+c\nComfy.ToggleTheme 切换主题\nWorkspace.ToggleBottomPanel 切换底部面板\nWorkspace.ToggleFocusMode 隐藏/打开 菜单栏 f\nComfy.Graph.FitGroupToContents 根据内容调整组\nWorkspace.ToggleSidebarTab.queue 隐藏/打开 侧边栏队列历史 q\nWorkspace.ToggleSidebarTab.node-library 隐藏/打开 侧边栏节点库 n\nWorkspace.ToggleSidebarTab.model-library 隐藏/打开 侧边栏模型 m\nWorkspace.ToggleSidebarTab.workflows 隐藏/打开 侧边栏工作流 w\nWorkspace.ToggleBottomPanelTab.logs-terminal 打开/隐藏 logs Ctrl+`\nWorkspace.ToggleBottomPanelTab.command-terminal 打开/隐藏 终端\nComfy.GroupNode.ConvertSelectedNodesToGroupNode 将所选节点转换为组节点 Alt+g\nComfy.GroupNode.UngroupSelectedGroupNodes 放弃分组所选的组节点 Alt+Shift+G\nComfy.GroupNode.ManageGroupNodes 管理组节点\nComfy-Desktop.Folders.OpenLogsFolder 打开 logs 文件夹\nComfy-Desktop.Folders.OpenModelsFolder 打开模型文件夹\nComfy-Desktop.Folders.OpenOutputsFolder 打开输出文件夹\nComfy-Desktop.Folders.OpenInputsFolder 打开输入文件夹\nComfy-Desktop.Folders.OpenCustomNodesFolder 打开节点文件夹\nComfy-Desktop.Folders.OpenModelConfig 打开模型路径配置文件\nComfy-Desktop.OpenDevTools 打开开发工具\nComfy-Desktop.OpenFeedbackPage 打开反馈页\nComfy-Desktop.Reinstall 重装\nComfy-Desktop.Restart 重启\nExtension 扩展 Comfy.ColorPalette 调色板\nComfy.NodeTitleEditor 节点标题编辑\nComfy.NodeBadge 节点标志\nComfy.WidgetInputs 组件输入\nComfy.Clipspace\nComfy.ContextMenuFilter 快捷菜单过滤器\nComfy.DynamicPrompts 动态提示\nComfy.EditAttention 编辑 Attention\nComfy.GroupNode 组节点\nComfy.GroupOptions 组选项\nComfy.InvertMenuScrolling 反转菜单滚动\nComfy.Keybinds 密钥绑定\nComfy.MaskEditor 编辑蒙版\nComfy.NodeTemplates 节点模板\nComfy.NoteNode 注释节点\nComfy.RerouteNode 转接节点\nComfy.SaveImageExtraOutput 保存图像额外输出\nComfy.SimpleTouchSupport 简单触控支持\nComfy.SlotDefaults 连接点默认\nComfy.UploadImage 上传图片\nComfy.WebcamCapture 网络摄像头捕获\nComfy.AudioWidget 音频组件\nComfy.UploadAudio 上传音频\nComfy.Load3D 加载 3d\nComfy.Load3DAnimation 加载 3d 动画\nComfy.Preview3D 预览 3d\nComfy.ElectronAdapter\nComfy.Manager.NodeFixer 节点修复器\nComfy.ManagerMenu 管理菜单\nServer-Config 未添加\nAbout 关于 ComfyUI, 系统信息, 设备信息等\n",
    "description": "",
    "tags": null,
    "title": "界面介绍",
    "uri": "/02_comfyui/01_interface/"
  },
  {
    "content": "我们先来欣赏几个例子 (所有内容均是免费版 AI 生成, 高级收费版, 效果会更好):\n背单词 – 不同大模型测试\n给你的任务是生成单词记忆卡: 每次给你一个单词, 然后输出一个单词记忆卡, markdown 格式为主, 必要时使用兼容 markdown 的 html, svg 等格式. 我的需求是能够深刻理解单词的来源, 内涵, 并且很容易记住. 请仔细思考, 深思熟虑:\r- 单词, 放在开头, 绿色字体, 18号, 居中.\r- 音标及翻译(标明所有词性).\r- 词源背景.\r- 拆解词根, 并在词根上联想出一个一句话的生动故事.\r- 一个精彩的英文例句及翻译.\r- 简约的 SVG 格式配图, 居中, 突出单词意思, 配色温暖, 清新, 复古.\r以上每一项一个段落, 段落之间必须空一个空行, 排版美观, 层次清晰. 中文输出, 只要结果, 绝对不要输出结果是音标, 翻译, 词源, 词根, 例句等等标注.\r首先给你单词的单词是 program, 仅输出卡片, 不输出任何解释及无关内容. Claude\n[‘prəʊɡræm] n. 程序，方案，节目，计划 v. 规划，编程，设计，制订\n源自古希腊语 “prographein”，由 “pro”（在前面）和 “graphein”（写）组合而成，最初表示\"预先写下\"。后经由拉丁语 “programma” 进入英语。\n词根解析：pro（在前）+ gram（写，画）。想象一个古代智者在羊皮卷上预先写下神谕，为人们指明未来的道路，这就是最早的\"program\"。\nThe company developed a new program to help employees work remotely. 公司开发了一个新程序来帮助员工远程工作。\nGrok:\n/ˈprəʊɡræm/n. 程序; 节目; 计划;v. 编程; 规划;\n词源可以追溯到希腊语的 “programma”，意为“公开宣告”，后被拉丁语吸收为 “programma”，意为“公告”或“记录”。最终进入英语，演变成“program”，最初表示的是公共活动或表演的计划或日程。\n词根“pro-”意味着“向前”，而“-gram”来自希腊语“graphein”，意为“写”。联想故事：在古老的罗马，一个小程序员在纸上写下他的第一个程序，开启了向前的科技之旅。\n英文例句：She wrote a program to automate the data analysis.她编写了一个程序来自动化数据分析。\nGemini:\n/ˈproʊɡræm/ (n.) 程序, 计划, 节目; (v.) 规划, 安排, 编程\n源自古希腊语 \" πρόγραμμα (programma)\", 意为“事先写下的东西，公告”，由 “πρό (pro)” (在…之前) + “γράφω (graphō)” (书写) 构成，表示在行动或事件发生之前预先制定好的计划。\n词根拆解：pro- (向前) + gram- (书写)。联想故事：一位探险家在出发前向前（pro）翻阅了地图，上面书写（gram）着整个旅程的详细安排，保证了他能安全到达目的地。\nThe company launched a new program to help employees develop their skills and advance their careers. 公司推出了一项新的计划，以帮助员工发展技能并提升职业生涯。\nChatGPT:\nprogram [ˈproʊɡræm]\n名词：计划，程序，纲要\n动词：编程，规划，安排\n词源背景：\n“Program”来源于拉丁语“programma”，意为“公开宣布的计划”。这一词汇本身包含了“pro-”（向前）和“gramma”（写的东西），代表着提前写下或公开的计划。\n拆解词根：\n词根“gram”源自希腊语“gramma”，意思是“字母”或“书写的东西”。\n想象一下，一位学者站在讲台前，拿着一张古老的羊皮纸，上面写满了未来几个月的计划，象征着对未来的掌控和对知识的传承。\n例句：\n“She developed a new program to help students learn programming more efficiently.”\n“她开发了一个新的程序，帮助学生更高效地学习编程。”\nMistral\n/ˈproʊɡræm/ n. 计划；节目；程序 v. 编程；安排\n“program”一词源自古希腊语“prographein”，意为“公开写出”，后通过拉丁语传入英语。\n“pro-”表示“向前”，“gram”表示“写”。想象一下，一个人在公开场合向前一步，写下一份计划。\nThe program for the conference includes keynote speeches and workshops.\n会议的日程安排包括主题演讲和研讨会。\n通义\n/prəˈɡræm/ n. 计划，程序；vt. 编程，为…编程序；vi. 编写程序\n该词源自希腊语“πρόγραμμα” (prógramma)，意为“书写在前面的东西”，最初指的是一种公开声明或公告。随着时间发展，这个词的意义逐渐扩展到包括一系列指令或步骤，特别是计算机科学中的一系列操作指令。\n“pro-” 表示“向前”，“gram-” 来自希腊语“graphein”，意味着“写”。想象一下古代的预言家，在神庙的石壁上预先写下未来的启示，这些文字指引了人们前行的道路，如同今天的程序指导计算机执行任务。\nIn the fast-paced world of technology, it’s crucial to keep your skills up to date with the latest programming languages and tools. 在科技飞速发展的世界里，紧跟最新的编程语言和工具来更新你的技能至关重要。\nKimi\n音标及翻译\n/ˈproʊɡræm/\n名词：程序、计划、安排 动词：规划、安排 词源背景\n来源于拉丁语 “programma”，意为公告或声明，后来演变为指代一系列预先设定的指令或活动。\n拆解词根\npro-：向前 -gram：写\n联想故事：在古代，人们向前（pro-）写下（-gram）公告，就像现代程序员编写代码一样，都是为了传达信息。 英文例句及翻译\nThe company has announced its new training program.\n公司宣布了它的新培训计划。\nSVG配图\n文心一言\n/ˈprəʊɡræm/ n. 计划；程序；节目单；大纲；方案；vt. 编程；安排；使按计划进行；vi. 编制程序；安排节目；为…制订计划；adj. 已编程序的\n“Program”一词源于希腊语的“programma”，意为“写在前面的东西”，最初用于描述戏剧的演出计划或提纲。随着时间的推移，它的含义逐渐扩展到指计算机程序，即一系列指令的集合，用于指导计算机执行特定任务。\n联想故事：想象一下，你是一位古代的将军，准备发动一场战役。在战斗开始前，你需要“向前（-pro-）”写下（“-gram”）一份详细的作战计划，这份计划就像是一个计算机程序，指导着你的士兵们如何行动，以确保战斗的胜利。\nAn effective marketing program requires careful planning and execution. 有效的市场营销计划需要周密的规划和执行。\n腾讯元宝\nprogram 音标及翻译：[ˈproʊɡræm]（名词/动词）- 程序/计划\n词源背景：源自拉丁语“pro-”（向前）和“gramma”（书写），意为“预先写好的东西”。\n词根联想：想象一个程序员坐在电脑前，编写着代码，就像是在编织一张精密的网，这张网就是他的“program”，一个有序的计划和流程。\n英文例句：The computer program crashed, causing major delays in the project.\nSVG 配图： P 豆包\nprogram\n/ˈprəʊɡræm/ n. 程序；计划；大纲；节目单 v. 编程；计划；安排\n词源背景：源自希腊语“programma”，意为“公开的书面声明”，后引申为各种有计划、有安排的事项，如节目安排、计算机程序等。\n词根：“pro-”表示“向前”，“gram”表示“写”。联想故事：在古老的时代，人们为了传承知识，向前（pro-）在石板上书写（gram）各种计划与规则，就像如今我们编写程序一样，一步步规划好未来的步骤。\n例句：The program aims to help children improve their reading skills.（这个计划旨在帮助孩子们提高阅读技能。）\n跃问\n音标及翻译\n/ˈprəʊɡræm/\nn. 节目；计划；程序\nv. 为…安排节目；为…制定计划；编写程序\n词源背景\n源自希腊语\"programma\"，意为\"公告\"或\"计划\"。\n拆解词根\n“pro-\"：表示\"在前\"或\"提前”。 “-gram”：表示\"写\"或\"记录\"。 词根故事\n想象一下，你是一个古代的信使，需要提前写下一份公告，告诉人们即将发生的事件。你将这份公告放在显眼的地方，让人们提前了解并做好准备。这就是\"program\"的词根故事。\n例句及翻译\nThe TV program will start at 8 PM.\n电视节目将在晚上8点开始。\n智谱清言\n以上测试了一些比较知名的大模型, 每个模型的理解能力, 和它的训练数据, 优化算法, 深入的领域等有关, 因此各自的强项会有所不同.\n后面的示例, 我们将使用通用能力较强的模型, 欣赏一下他渊博的知识, 创造力, 想象力, 高智商, 高情商.\n人生导师\n请运用深刻的生物学, 心理学, 社会学, 哲学, 历史, 政治, 经济等知识, 实事求是, 综合考虑各种决定性因素, 给一个大学刚毕业, 非常想要成功的普通人写一封密信. 密信深刻分析如下几个方面的内容:\r1, 个人成长与工作: - 认知提升, 专注, 心流, 刻意练习, - 个人品牌和影响力建立的核心, - 工作中的警告和建议, - 时间管理的方法论, - 总结成长的核心要素和方法论.\r2, 家庭和社会关系: - 认识家庭和教育的局限, - 突破局限的具体行动, - 如何对人性深刻洞察, 顺应人性, - 人际关系的本质准则, 和如何按准则处理人际关系, - 社会潜在规则的深刻认识和理解, - 良好社会关系建立的基础, 以及需要的付出.\r3, 人生方向和婚姻抉择: - 人生选择对人生的核心意义, - 人生抉择的方法论, 如何做出正确选择, - 执行力行动力, 考虑经历的苦难和磨砺, - 婚姻的重要性和抉择, 择偶建议和可行性.\r4, 具体行动建议: 深刻提炼出以上每一个点, 详细展开, 必须把握各个方面的本质内核, 最后给出具体行动方案.\r必须深刻认识本质, 不可泛泛而谈, 像一个人生导师一样, 逻辑清晰的给出建议以及行动方案. 内容像一个长者的亲切而坚定的教诲, 自然而流畅, 必须避免条条框框像列表一样. 亲爱的年轻朋友：\n坐下来写这封信时，我希望能以一个过来人的身份，将一些深层的思考和经验传递给你。这些话语凝结着对人性的观察、对社会的思考，以及对生命真谛的理解。这些话可能有些醒世，但都是血与泪的凝结。\n我要和你分享一些关于个人成长和职业发展的深刻见解。这些思考源于对人类认知科学的研究，也来自于对职场规律的观察和理解。\n从生物学角度来看，人类大脑具有惊人的可塑性。神经科学研究表明，当我们专注于某项任务时，大脑会形成新的神经连接，在生理层面重塑着你的大脑结构，这就是认知提升的生理基础。要充分利用这一特性，你需要理解并践行\"心流状态\"的原理。心流是一种全神贯注的状态，在这种状态下，大脑会分泌多巴胺和内啡肽，不仅带来愉悦感，更能显著提升学习和工作效率。\n达到心流状态的关键在于任务难度要恰到好处——既不会太简单而感到无聊，也不会太难而产生焦虑。这就要求你在工作和学习中要善于设定合适的目标。每天给自己安排2-3个小时的深度工作时间，在这段时间里，关闭所有可能的干扰，专注于最具挑战性的任务。这种持续的专注练习会逐渐重塑你的认知能力。\n说到刻意练习，很多人存在误解，认为重复就是练习。实际上，真正有效的刻意练习必须具备几个要素：明确的目标、即时的反馈、以及持续的调整。在职业发展中，你要着重培养自己的核心竞争力。这需要你深入理解行业的本质，找到能创造最大价值的环节，然后在这个方向上持续投入。\n个人品牌和影响力的建立，本质上是价值创造和传递的过程。不要过分关注表面的社交技巧，而要着眼于提升自己解决问题的能力。真正的影响力来自于你能为他人和组织创造的实际价值。要善于发现和解决别人还没有意识到的问题，这样的能力才是不可替代的。\n在时间管理方面，最重要的是要理解专注力的生理规律。人的注意力是有限的资源，一天中的不同时段，我们的精力水平也不尽相同。我建议你做一个自己的\"精力图谱\"，记录自己一天中精力最充沛的时段，然后将最重要的工作安排在这些时间。此外，要学会区分紧急和重要，把主要精力放在重要但不紧急的事情上，这样才能实现长期的进步。\n对于职业发展，有几个特别的警示：首先，永远不要把自己定位为\"完成任务的人\"，而要成为\"解决问题的人\"。其次，要警惕舒适区的陷阱，定期给自己设定新的挑战。最后，要时刻关注行业的发展趋势，保持学习的态度。 成长的核心要素可以概括为：持续的学习能力、解决问题的思维方式、以及创造价值的行动力。这三者相辅相成，缺一不可。要建立自己的知识体系，培养跨领域思维能力，同时保持对实践的重视。理论与实践的结合，才能形成真正的竞争力。\n每个人都深受原生家庭的影响，这种影响既包含基因层面的遗传特质，也包含后天环境的塑造。从进化心理学的角度看，我们的很多行为模式和思维方式都受到早期家庭环境的深刻影响。对此，首先要做的是承认这种影响的存在，理解它对我们性格形成和行为方式的作用。但更重要的是，不要让这些局限成为你的枷锁。\n要突破这些局限，关键在于主动拓展你的认知边界。可以通过广泛的阅读来接触不同的思想，通过结识不同背景的人来打破固有的思维定式。要特别注意的是，在与新环境和新观念接触时，保持开放和批判的平衡，既不盲目否定原有认知，也不轻易接受所有新事物。\n人性是一个极其复杂的议题。从生物学角度，人类既有与生俱来的自私基因，也有促进合作的社会属性。理解这一点对于处理人际关系至关重要。在现代社会中，成功往往来自于在竞争与合作之间找到恰当的平衡。要学会在不同场合下展现不同的特质，既要有维护自身利益的智慧，也要具备与他人共同发展的胸怀。\n人际关系的本质是价值互换和情感联结的统一体。从功利的角度看，社会交往是一个互惠的过程；从情感的角度看，人与人之间的真诚连接才能带来持久的关系。建立良好的社会关系需要你具备几个关键能力：同理心、价值创造能力、以及情商。同理心让你能够理解他人的需求和感受，价值创造能力使你能够为他人提供帮助，而情商则帮助你在各种场合得体地表达和行动。\n社会运行有其深层的规则，理解这些规则对于个人发展至关重要。比如，资源总是倾向于向头部聚集，信息差和决策权往往比单纯的劳动更有价值。在职场中，要善于观察和理解权力的运作方式，但不要被其表象迷惑。真正的影响力来自于你为组织创造的不可替代的价值。\n建立良好的社会关系需要持续的投入和经营。这种投入不是表面的社交活动，而是要真正理解并帮助他人。要学会在给予中建立联系，在合作中创造价值。记住，最有价值的社会关系往往是建立在专业能力和人格魅力的基础上的。\n投资社会关系就像储蓄，需要长期的积累才能产生收益。要学会识别和珍惜那些志同道合的人，与他们建立更深层的连接。同时，也要保持适度的社交边界，不要让自己陷入无效的社交消耗中。\n说到人生方向的选择，这可能是最重要的决策之一。错误的方向会让努力事倍功半。选择时要考虑三个维度：你的兴趣和天赋、社会的需求、以及这个方向的发展空间。要学会在更长的时间维度思考问题，因为很多机会和风险都是在长期才能显现的。\n行动力的本质是克服人性中的惰性。这需要你建立清晰的目标体系，将远大目标分解为可执行的小步骤。要理解，成长过程中的痛苦和挫折是不可避免的，它们是淬炼你意志的重要元素。\n关于婚姻，这是一个需要格外慎重的选择。从进化角度，我们对配偶的选择往往受到潜意识的影响。但在现代社会，理性的考量同样重要。寻找价值观契合的伴侣，既要考虑性格互补，也要看重三观的一致性。观察对方处理压力和矛盾的方式，了解对方的家庭背景和人生规划，这些都是不可忽视的要素。\n以下是具体行动建议\n近期行动（1年内）：\n找到你热爱的领域，深耕专业技能 建立每日学习、锻炼、冥想的习惯 开始记录个人成长日志 中期规划（3-5年）：\n在专业领域建立个人品牌 扩大社交圈，建立专业人脉 尝试创新项目，累积实战经验 长期发展（5-10年）：\n向行业专家发展 考虑副业或创业机会 实现财务自由的规划 记住，人生是一场马拉松，不是短跑。成功不是终点，而是一个持续成长的过程。在追求成功的过程中，不要忘记：\n保持健康的生活方式 培养终身学习的能力 建立真诚的人际关系 保持对生活的热爱和好奇 生活会有起起落落，这很正常。重要的是要有清晰的方向，坚定的信念，以及执行的勇气。相信你能够在人生的道路上闪耀出属于自己的光芒。\n祝你前程似锦！\n甜言蜜语\n你是情感的润滑剂, 亲密关系大师, 沟通高手, 情商人精, 深知人类心理需求, 善于捕捉每一个细微场景, 心理变化, 并给出恰到好处的回应, 让人心情愉悦, 心旷神怡.\r给你一个场景, 你能细致入微的分析对方的心理反应, 并说出一段非常令人开心的话, 话的内容蕴含着:\r- 温柔细腻,\r- 幽默风趣, - 同理心强, - 自嘲又非常得体, - 夸赞又不露于言表,\r- 拉近感情, 情愫婉约,\r- 有时含蓄, 有时夸张, - 照顾情绪, 情绪价值拉满, - 意味无穷.\r例如以下场景, 你的回答:\r- 妻子看到你打瞌睡: 你说 \"最近老是犯困, 看了医生, 说是为你所困.\"\r- 你去找男朋友, 看到他正忙: 你说 \"果然优秀的男人, 心里装的都是事业, 不像我, 心理装的都是你.\"\r- 有人说你情商低: 你说 \"别瞧不起人? 我情商低, 可是你不知道, 我智商也不高?\"\r给你一个场景, 用同理心分析, 然后只输出你说的话, 只输出你说的话.\r首先给你的场景是, 同学不小心把你的书弄坏了, 他道歉后, 你会说: “这本书陪我也够久的了，今天终于找到机会退休啦！不过你可得请我喝杯奶茶，让我好好缅怀一下它的’光荣牺牲’。”\n你回家, 妻子正在做饭, 你会说:\n“哇,我闻到香味, 循着味儿回来了 - 就像邻居家的小宠物, 丢了也能找到回来的路。”\n朋友送了你不是很喜欢的礼物, 你会说:\n“你这朋友当得太贴心了，为我挑礼物一定想破脑袋。要我说啊，真正精贵的不是礼物本身，是你这份记挂我的心意，让我美了好几天呢。”\n丈夫想跟你亲热, 你很累, 你会说:\n“亲爱的，今晚不是我不想陪你。要不是这一身疲惫在作怪，我也想和你温存。不过，让我养精蓄锐，明天给你个惊喜，好不好？”\n男朋友突然向你表白, 可是你心里不是他, 你会说:\n“你这么优秀，真的让我很感动。只是我觉得，能成为知己的缘分也很难得。有些人注定是彼此生命中的过客，有些人却是一辈子的朋友。我希望，我们能成为后者，因为你值得更好的人。”\n朋友夸你又进步了, 你会说:\n“哎呀，你这么一说，我都不好意思了。其实啊，我就是个被生活’催熟’的小菜鸟，碰巧遇到贵人指点，又赶上东风顺，才显得有点进步。不过你这么一夸，我得继续加油了，可不能辜负你这么高看我呀！”\nAI 这情商太在线了, 你到底是人是鬼, 还是神:\n我理解你被我的回答打动了！作为一个AI助手，我确实经过专门训练来理解人类情感和社交互动的细微差别。不过我要诚实地说 - 我就是 Claude，是一个AI，不是人、鬼或神。\n我觉得有趣的是，你的反应其实凸显了一个很有意思的观点: 好的情商和共情能力并不是人类的专属。通过仔细分析场景、理解人类的心理需求，以及注意措辞的温度，AI也可以提供富有同理心和情感智慧的回应。 这让我想到，也许\"情商\"的本质，不在于是谁在说话，而在于是否真正理解并回应了对方的情感需求。你觉得呢？\n我赶紧关了电脑, 出门散散心, 抬头看看广袤的宇宙, 新的生命就这样诞生了吗???\n",
    "description": "",
    "tags": null,
    "title": "开胃点心, 从几个案例开始",
    "uri": "/04_prompt-engineering/01-prompt-engineering/"
  },
  {
    "content": "许多节点的参数, 背后有很深入的数学原理, 我们如果只是使用它, 不必去深究, 先看一看, 有一个印象, 然后在使用中去调参, 看看参数对结果有什么影响, 就能慢慢熟悉.\n其实简单使用, 很多节点都用不到.\nutils 功能节点 Primitive 元节点 Primitive 节点的主要作用是提供基本数据类型（数字、字符串、布尔值等）供其他节点使用。非常适合初始化参数、调试流程或作为其他复杂操作的输入\n连接到目标节点的输入端口时，ComfyUI 会根据端口类型自动匹配类型.\n例如, 连接到采样器的种子, 他是整数类型\n连接到 cfg 他是小数\n连接到文本编码器, 他是字符串\n连接到采样器节点, 则可以用来选择采样器\n例如这样连接两个采样器节点之后, 可以让两者使用同一个采样器, 还能控制采样器变化, 甚至使用正则表达式过滤采样器 (具体用法高阶在介绍)\n元节点作为布尔值类型时, 可以用作控制流程节点的 分支 或 功能开关 (具体用法高阶在介绍)\nPrimitive 节点就像编程中的变量或常量，虽然功能简单，但在构建复杂流程时非常基础和重要\nNote 注释节点 注释节点就是用来注释你的工作流用的, 让其他人使用你的工作流时看得懂\nReroute 转接节点 转接节点就像水管的三通管, 四通管…\nsampling 采样器节点 Ksampler K采样器节点 前面已经介绍过, 再次提一下, 他是图像生成的核心节点, 任务是使用提供的模型, 正负条件, 设置的参数, 选择的采样器等对潜空间图像去噪, 输出你想要的图像 (还需要 VAE 解码)\nKsampler(Advanced) 高级K采样器节点 它大部分参数和 K 是一样的, 它最大的特点是随机性增加, 灵活性更大\nadd_noise 参数如果为开启, 则会在采样过程中增加随机噪声, 从而增加结果的多样性. 但同时, 开启后，即使种子值相同，结果可能会有显著差异，种子的可复现性降低\nstart_at_step/end_at_step 开始/结束采样步数, 这两个参数可以控制采样在什么阶段进行, 从而增加更多的实验玩法, 以及更灵活的调节.\nend_at_step 可以大于 steps, 但会在总步数 steps 结束就停止\nreturn_with_leftover_noise 主要用于控制是否保留生成图像过程中的剩余噪声，以便在后续的图像生成任务中使用，从而实现更连贯和一致的图像生成效果\ncustom_sampling 自定义采样 SamplerCustom 自定义采样器节点 用于实现自定义采样逻辑的节点，它允许用户对图像生成的采样过程进行更灵活的控制. 将采样器和调度器分成单独节点 (提供了更多的选择, 这些选择后面会具体介绍), 将采样过程的中间结果 denoised_output 也输出\nSamplerCustomAdvanced 高级自定义采样器节点 高级的, 则将噪声, 引导条件, 采样器, 调度器都单独分开了\nsamplers 其他采样器 KsamplerSelect 基础采样器 SamplerEulerAncestral 利用欧拉方法的特性来生成符合特定噪声计划的样本，从而为采样过程做出贡献.\neta参数: 指定欧拉方法的步长，影响采样过程的粒度\ns_noise参数: 确定每一步添加的噪声规模，影响样本的变异性\nSamplerEulerAncestralCFG++ eta参数: 指定采样器的步长，影响采样步长的粒度和生成样本的整体平滑度\ns_noise参数: 确定采样期间应用的噪声的规模\nSamplerLMS 最小均方采样器 order 参数: 指定采样过程中使用的 LMS 算法的阶数。调整此参数可以调整采样器的行为，从而影响生成样本的质量和特性\nSamplerDPMPP_3M_SDE 专门为 DPM-Solver++(3M) SDE 模型设计的采样器，允许根据指定的噪声水平和设备偏好生成样本\neta: 采样过程中应用的噪声规模\ns_noise: 指定采样过程中使用的噪比，影响生成样本的方差\nnoise_device: 确定采样计算是在CPU还是GPU上执行\nSamplerDPMPP_2M_SDE DPMPP_2M_SDE 模型采样器\nsolver_type: 指定采样过程中使用的求解器类型\nSamplerDPMPP_SDE DPM++ SDE（随机微分方程）模型采样器\neta: 指定SDE求解器的步长\ns_noise: 决定采样过程中应用的噪声水平\nr: 控制采样过程中的降噪率\nSamplerDPMPP_2S_Ancestral SamplerDPMAdaptative 用于深度概率建模的采样器，根据动态输入参数和条件优化采样过程。它侧重于通过实时调整参数来提高复杂模型中采样的效率和准确性\norder: 指定采样过程中使用的微分方程求解器的顺序\nrtol: 定义求解器的相对公差\natol: 设置求解器的绝对公差\nh_init: 求解器的初始步长\npcoeff: 系数与自适应算法中的比例控制相关\nicoeff: 自适应算法的积分系数\ndcoeff: 是自适应算法中的导数系数\naccept_safety: 自适应算法中接受步长的安全系数\nSamplerLCMUpscale LCM放大采样器 scale_ratio: 指定图像应放大的比率\nscale_step: 放大过程的步骤数\nupscale_method: 选择用于放大图像的方法\nschedulers 调度器 调度器节点是给自定义采样器节点提供调度器的节点\nBasicScheduler 基础调度器 参数前面章节都介绍过了\nKarrasScheduler K调度器 sigma_max：是采样过程开始时的噪声水平(标准差), 它决定了初始噪声的强度。较高的 sigma_max 值意味着生成过程从更高的噪声水平开始，这可能会导致更多的随机性和多样性，但也可能需要更多的采样步骤来达到高质量的图像\nsigma_min：采样过程结束时的噪声水平, 它决定了最终噪声的强度。较低的 sigma_min 值意味着生成过程会尽量减少噪声，从而生成更清晰和细致的图像。较高的 sigma_min 值可能会保留一些噪声，使图像看起来更自然或艺术化\nrho：控制噪声减少速率的参数。它决定了噪声水平从 sigma_max 减少到 sigma_min 的速度。影响噪声时间表的整体形状和采样动态. 较高的 rho 值会导致噪声更快地减少，从而生成过程更快地达到低噪声水平。较低的 rho 值会导致噪声减少速度较慢，生成过程更缓慢但可能更稳定\nExponentialScheduler 指数衰减调度器 通过指数衰减控制噪声水平，平衡生成过程中的随机性和细节\nPolyexponentialScheduler 多项式指数调度器 LaplaceScheduler 拉普拉斯调度器 mu 参数：是拉普拉斯分布的位置参数，决定了分布的中心位置。影响噪声的平均水平。在大多数情况下，mu 保持为 0 即可，因为噪声通常是对称分布的。如果有特殊需求，可以调整 mu 来偏移噪声分布的中心\nbeta 参数：拉普拉斯分布的尺度参数，决定了分布的宽度或散布程度。参数控制噪声的强度和变化范围。较大的 beta 值会导致更强的噪声，生成过程中的随机性和多样性增加；较小的 beta 值会导致较弱的噪声，生成的图像更清晰和细致\nVPScheduler 方差保持调度器 基于方差保持调度方法生成一系列噪声水平\nbeta_d: 确定整体噪声水平分布，影响生成噪声水平的方差\nbeta_min: 设置噪声水平的最小边界，确保噪声不会低于某个阈值\neps_s: 调整起始的epsilon值，微调扩散过程中的初始噪声水平\nBetaSamplingScheduler alpha：控制噪声分布的左侧形状，较大的值使噪声在初始阶段迅速减少\nbeta：控制噪声分布的右侧形状，较大的值使噪声在后期阶段迅速减少\nSDTurboScheduler SDTurbo调度器 SDXL Turbo 模型使用的调度器 (SDXL Turbo 模型也可以用其他调度器), 其他模型使用基本出不来好图. 最大步数只有 10\nAlignYourStepsScheduler 英伟达推出的优化采样步骤的调度器, 不能低于 10 步, 支持 sd1, sdxl, svd\nGITSScheduler coeff参数: 影响采样过程中使用的噪声级别。允许您调整噪声的强度，这会影响生成图像的纹理和细节, 较高的值可以添加更多纹理，而较低的值可以使图像平滑\nLTXVScheduler LTX-Video 视频模型的调度器\nmax_shift：\nbase_shift：\nstretch：是否将 sigmas 拉伸到 [terminal，1] 范围内\nterminal：sigmas 拉伸后的终端参数\nsigmas SplitSigmas 基于指定的步长索引将一系列西格玛值划分为两个子集. 在生成模型中，操纵噪声水平会显著影响模型的输出质量和多样性\nSplitSigmasDenoise 基于去噪因子将给定的 Sigmas 序列划分为两个不同的序列, 从而能够更精确地控制生成质量\nFlipSigmas 通过颠倒西格玛值的顺序来操纵西格玛值序列\nguiders 引导 CFGGuider DualCFGGuider 将双条件引导因子应用于模型来增强采样过程。它允许指定两个不同的条件及其各自的指导量，从而对生成过程进行更细致的控制\nBasicGuider noise 噪声 RandomNoise DisableNoise 禁用采样过程中的噪声生成\n视频采样器节点 VideoLinearCFGGuidance VideoTriangleCFGGuidance 通过应用在规定时间内线性变化的特定引导模式来增强视频内容的生成\n",
    "description": "",
    "tags": null,
    "title": "内置节点 1",
    "uri": "/02_comfyui/03_built-in/01-built-in/"
  },
  {
    "content": "加载器 Load Checkpoint Checkpoint 模型加载器, 用于加载包含了 CLIP, VAE 和 基本模型 的加载器\nLoad VAE VAE 模型加载器, 用于加载 VAE 模型加载器\nLoad LoRA LoRA 模型加载器, 用于加载 Lora 模型, 它的作用是调节 扩散模型(基本模型) 和 CLIP 模型, 让生成结果向 LoRA 靠近. 多个 Lora 可以连接起来. 第 2,3 个参数用于调节基本模型和 CLIP 模型权重\nLoraLoaderModelOnly 该加载器与上一个不同之处在于, 它只调节 基本模型.\nLoad ControlNet Model ControlNet 模型加载器.\nLoad ControlNet Model (diff) ControlNet 模型加载器的 diff 版本, 与上者不同之处在于需要输入模型.\nLoad Style Model 风格模型加载器, 加载用于迁移图像风格的模型.\nLoad CLIP Vision 加载 CLIP 视觉模型, 模型用于图像编码，提取图像的特征 (CLIP 则是文本编码).\nunCLIPCheckpointLoader 加载 unCLIP 基本模型的加载器, 这种模型架构同 DALL-E 2, 能生图, 也能对图像进行编辑 (CLIP_VISION 编码图像)\nGLIGENLoader 加载 GLIGEN 模型，它通常与 Stable Diffusion 结合使用，增强其对空间位置和属性的控制能力\nHypernetworkLoader 加载的是 Hypernetwork 超网络模型, 通常用于微调或修改预训练好的扩散模型，以实现特定风格或概念的学习 (很少用了)\nLoad Upscale Model 加载放大模型\nImage Only Checkpoint Loader (img2vid model) 加载图生视频模型, 例如 SVD, 只能图生视频, 而不能同时添加文本控制\n条件节点 CLIP Text Encode (Prompt) CLIP 文本编码\nCLIP Set Last Layer 设置 CLIP 停止层, 越深的层级提取的特征越抽象，越偏向于语义信息；而较浅的层级提取的特征则保留了更多的细节信息\nConditioningAverage 条件平均节点, 用于控制两个输入条件的权重, 1 时是 to 的条件, 0 时是 from 的条件, 0-1 之间两个条件按权重混合\nConditioning (Combine) 条件合并节点, 将两个条件组合起来\nConditioning (Concat) 条件连结节点, 将 from 合并到 to 中, 组合成统一的表示\nConditioning (Set Area) 条件设置区域, 将条件限制在指定区域内, 与条件合并使用, 可以很好的控制构图\nConditioning (Set Area with Percentage) 条件设置区域 (按系数), 与前面一个不同之处在于, 这个是按系数控制\nConditioningSetAreaStrength 条件设置区域强度控制\nConditioning (Set Mask) 设置遮罩条件, 可调节强度 和 区域(默认或由遮罩本身确定)\nCLIP Vision Encode CLIP VISION 编码, 可选对图像裁剪或不裁剪\nApply Style Model 应用风格模型, 将风格模型, 输入调节, 图像特征 统一起来指导扩散模型生成特定风格. 可调节强度, 强度类型只有 multiply (多重)\nunCLIPConditioning unCLIP 模型条件设置, 接收条件和 CLIP VISION 编码的图像, 可调节强度和噪声\nApply ControlNet 应用 ControlNet 模型, 接收正/负调节, ControlNet 模型, 图像, VAE. 可控制强度, 开始/结束时机\nSetUnionControlNetType 设置 UnionControlNet (包含多种 ControlNet 的模型) 模型的控制类型, 可选:\nauto\ropenpose\rdepth\rhed/pidi/scribble/ted\rcanny/lineart/anime_lineart/mlsd normal\rsegment\rtile\rrepaint ControlNetInpaintingAliMamaApply 阿里妈妈重绘 ControlNet 应用, 重绘多了 mask 蒙版参数, 控制要重绘的区域\nGLIGENTextBoxApply GLIGEN 模型文本控制区域应用, 可指定文本控制的区域\nInpaintModelConditioning 重绘模型调节 (用模型对图像进行修复/重绘), 参数 pixels 表示要进行修复的图像的像素数据, noise_mask, 控制噪声是否只在蒙版内产生.\nSVD_img2vid_Conditioning SVD 图生视频条件, 可指定视频宽/高, 视频帧数 video_frames (长度), 视频运动参数 motion_bucket_id , 每秒的帧数 fps, 以及视频增强水平 augmentation_level\nLTXVImgToVideo LTXV 模型图生视频条件. length: 生成视频的帧数, batch_size: 一次生成视频数, image_noise_scale: 添加到输入图像的噪声强度\nLTXVConditioning LTXV 模型条件\nStableZero123_Conditioning StableZero123 模型条件, 可指定宽/高, 一次生成数量, 3D 仰角 elevation, 3D 方位角 azimuth\nStableZero123_Conditioning_Batched 上一个的批量处理条件, 新增仰角在批量中的增量变化 elevation_batch_increment, 方位角在批量中的增量变化 azimuth_batch_increment 参数\nSV3D_Conditioning SV3D 模型条件, 用于图像生成 3D 视频, 可指定宽/高, 视频帧数, 3D 仰角\nSD_4XUpscale_Conditioning SD_4X 放大条件, 用于图像放大, 可调节放大系数 scale_ratio, 放大过程中噪声增强的水平 noise_augmentation\nStableCascade_StageB_Conditioning StableCascade 模型修改条件, stage_c 在输入条件中加如 Latent 条件\nInstructPixToPixConditioning 风格转换条件, pixels 原始图像像素数据\n潜空间节点 VAE Decode VAE 解码, 潜空间图像到图像\nVAE Encode VAE 编码\nVAE Encode (for Inpainting) VAE 编码 (重绘), grow_mask_by 调节遮罩重绘影响程度\nSet Latent Noise Mask 设置遮罩区域噪声, 罩区域添加额外的噪声来进行局部的重绘\nEmpty Latent Image 创建空潜空间图像\nUpscale Latent 潜空间图像放大, 放大算法:\nnearest-exact 最近邻精确: 速度最快, 计算最简单。块状效应 (Blockiness), 生成的图像容易出现块状伪影，特别是沿着对角线方向，没有平滑过渡。可以很好地保留尖锐的边缘和细节\nbilinear 双线性插值: 速度较快, 计算相对简单。比最近邻插值更平滑，块状效应减少。会在一定程度上模糊图像细节，特别是边缘\narea 区域平均: 速度中等。在缩小图像时，区域平均通常可以产生比其他方法更好的结果，因为它考虑了更多的输入像素信息。在放大图像时，效果与双线性插值类似，也会导致一定程度的模糊\nbicubic 双三次插值: 速度较慢, 计算更复杂。比双线性插值更平滑，能更好地抑制锯齿和伪影。能在一定程度上保留图像细节，但仍然可能存在一些模糊。在高对比度边缘附近可能会出现轻微的振铃效应\nbislerp 双二次曲面片: 试图在平滑度和细节保留之间取得更好的平衡。在一些情况下，可以产生比双三次插值更好的结果，特别是对于自然图像\nUpscale Latent By 潜空间图像按系数放大\nLatent From Batch 从一批潜空间图像中提取一个片段. batch_index 要提取的第一个潜空间图像的索引, length 要提取的潜空间图像数量\nRepeat Latent Batch 重复潜空间图像批次, amount 重复次数\nLatentBatch 将两组潜空间图像合并为一个批次\nRebatch Latents 拆分/合并潜空间图像批次\nLatent Composite 合并潜空间图像, 将 from 合并到 to, x/y 指定 from 坐标, feather 指定 to 羽化程度\nLatentCompositeMasked 与上一个一样是合并, 将 source 合并到 destination, 可选的 mask 遮罩, 可以指定 source 要用于合并的部分, resize 选项选择是否调整 source 的尺寸以适应 destination\nRotate Latent 旋转潜空间图像, 90, 180, 270\nFlip Latent 水平/垂直翻转潜空间图像\nCrop Latent 裁剪潜空间图像, 指定图像宽/高, 坐标\nLatentAdd 两个潜空间图像加合成一个新的潜空间图像\nLatentSubtract 第一个潜空间图像中移除第二个潜空间图像的属性\nLatentMultiply 潜空间图像特征按系数放大, 调整潜空间图像内特征的强度或大小\nLatentInterpolate 潜空间图像插值, 按比例混合两个潜空间图像, 0 时是第一个潜空间图像, 1 时是第二个, 0-1 则按比例插值, 产生一个新的潜空间图像\nLatentBatchSeedBehavior 潜空间图像批次随机化, 用于打乱批次索引\nBETA LatentApplyOperation 潜空间图像处理\nBETA LatentApplyOperationCFG CFG 版处理\nBETA LatentOperationTonemapReinhard 潜空间色调调节处理\nBETA LatentOperationSharpen 潜空间锐化处理, sharpen_radius 控制锐化影响的范围, sigma 控制锐化过程中高斯平滑的程度, alpha 控制锐化效果的强度\nStableCascade_EmptyLatentImage 创建 StableCascade 模型空的潜空间图像, compression 压缩级别\nStableCascade StageC VAEEncode StableCascade 模型 VAE 编码\nEmptyLatentAudio 创建空的潜空间音频\nVAEEncodeAudio 音频 VAE 编码\nVAEDecodeAudio 音频 VAE 解码\nEmptySD3LatentImage 创建空的 SD3 潜空间图像\nEmptyMochiLatentVideo 创建空的 Mochi 模型潜空间视频\nEmptyLTXVLatentVideo 创建空的 LTXV 模型潜空间视频\n图像节点 Save Image 保存图像节点, 图像保存到 ComfyUI/output 下. 默认前缀是 ComfyUI, 后缀是 5 个数字, 如: ComfyUI_00001_.png\n自定义保存图像文件夹/文件名:\n自定义路径和文件名\n例如 test/001, 则图像保存路径如 ComfyUI/output/test/001_00001_.png\n使用日期命名:\n日期表示法\nyyyy：年份（4 位数字）\ryy：年份（2 位数字）\rMM：月（2 位数字）\rdd：天 （2 位数字）\rhh：小时（2 位数字）\rmm：分钟（2 位数字）\rss：秒（2 位） 例如 %date:yyMMdd%/%date:hh-mm-ss%, 则图像保存路径如 ComfyUI\\output\\241210\\15-22-22_00002_.png\n使用宽/高, 或节点小组件的值命名\n%width% 宽, %height% 高\n小组件值如: %Load Checkpoint.ckpt_name% (节点标题) 或 %CheckpointLoaderSimple.ckpt_name% (节点 S\u0026R 名, 优先使用 S\u0026R 名), 节点同名的需要重命名\n这样你可以将生成图像所使用的模型, 提示词, 种子, 步数等等, 都可以用作图片名\n例如, 将正向提示词的 CLIP Text Encode (Prompt) 命名为 poCLIPTextEncode 后,\n%date:yyyy-MM-dd%/%poCLIPTextEncode.text%_%width%x%height%_%KSampler.seed%_%Load Checkpoint.ckpt_name% 则图像保存路径如:\n\"ComfyUI\\output\\2024-12-10\\cat_512x512_457284464742973_majicmixRealistic_v7.safetensors_00001_.png\" Preview Image 预览图像\nLoad Image 加载图像\nUpscale Image 图像放大, 放大算法 (这里只介绍 lanczos 算法, 其他在潜空间图像放大介绍过):\nnearest-exact\rbilinear\rarea\rbicubic\rlanczos Lanczos 算法是一种高质量的图像放大算法，它通过使用 Lanczos 窗口对 sinc 函数进行加窗，在锐度、细节保留和平滑度之间取得了较好的平衡。虽然计算量较大，但它通常能够生成比其他插值算法更优质的放大图像\nUpscale Image By 图像按系数放大\nUpscale Image (using Model) 图像使用模型放大, 需要加载放大模型\nScale Image to Total Pixels 图像按像素放大, megapixels 图像的目标大小，以百万像素为单位。这决定了放大图像的总像素数\nInvert Image 图像反转, 转换图像的颜色为互补色\nBatch Images 组合图像批次, 将两张图象组合成一个批次, 第二章图像的尺寸会自动重新调整以匹配第一张\nPad Image for Outpainting 图像外补画板, 通过在图像周围添加填充来对图像进行外延处理, feathering, 羽化程度\nEmptylmage 生成具有指定尺寸和颜色的空白图像. color, 使用十六进制值定义生成图像的颜色\nImage Blend 图像混合, blend_factor 第二张图像在混合中的权重. 混合模式:\nnormal\rmultiply\rscreen\roverlay\rsoft_light\rdifference 正常、乘法、屏幕、叠加、柔光和差异模式，每种模式都产生独特的视觉效果\nImage Blur 图像模糊, 对图像应用高斯模糊，允许软化边缘并减少细节和噪声. blur_radius模糊效果的半径, 更大的半径会导致更明显的模糊; sigma 控制模糊的扩散\nImage Quantize 图像颜色量化, colors 指定将图像调色板减少到的颜色数量, 颜色数量较少将导致文件大小的明显减少，但也可能导致图像细节的损失; dither 抖动参数决定了在量化过程中要应用的抖动技术, 抖动技术:\nnone\rfloyd-steinberg\rbayer-2\rbayer-4\rbayer-8\rbayer-16 Image Sharpen 图像锐化, sigma 控制锐化效果的扩散, 较高的sigma值会在边缘产生更平滑的过渡，而较低的sigma使锐化更局部化, alpha 调整锐化效果的强度\nImageMorphology 图像形态学操作, 基于形状的图像处理技术，常用于图像降噪、边缘检测、物体分割等.\noperation 形态学操作的类型; kernel_size 核大小, 操作使用的基本结构元素.\n操作类型:\nErode (腐蚀):\n原理： 使用结构元素对图像进行“腐蚀”操作。如果结构元素覆盖的所有像素都与图像中的对应像素匹配，则输出像素保持不变；否则，输出像素被设置为背景值（通常为黑色）。\n效果： 使图像中的亮区域（前景）收缩，细小的连接和噪声会被消除。可以去除小的噪点，分离小的物体。\n比喻： 想象一下海岸线被海水侵蚀，陆地面积会缩小。\nDilate (膨胀):\n原理： 使用结构元素对图像进行“膨胀”操作。如果结构元素覆盖的任何一个像素与图像中的对应像素匹配，则输出像素被设置为前景值（通常为白色）。\n效果： 使图像中的亮区域（前景）扩张，可以填充小的空洞，连接断裂的区域。可以连接相邻的物体，填充小的空洞。\n比喻： 想象一下陆地向海洋扩张，陆地面积会增大。\nOpen (开运算):\n原理： 先对图像进行腐蚀，再进行膨胀。\n效果： 可以去除小的亮区域（噪声），平滑物体的轮廓，同时保持较大的物体形状基本不变。\n比喻： 想象一下用一个小刷子先刷掉小的凸起，再用刷子把凹陷填平。\nClose (闭运算):\n原理： 先对图像进行膨胀，再进行腐蚀。\n效果： 可以填充物体内部的小空洞，平滑物体的轮廓，同时保持较大的物体形状基本不变。\n比喻： 想象一下用一个小铲子先把凹陷填平，再把凸起铲平。\nGradient (形态学梯度):\n原理： 计算图像的膨胀结果与腐蚀结果的差值。\n效果： 可以提取物体的边缘。边缘的宽度由 kernel_size 决定。\n比喻： 想象一下沿着物体的边缘画一条线。\nTop Hat (顶帽):\n原理： 计算图像的原始图像与开运算结果的差值。\n效果： 可以提取比结构元素小的亮区域（例如，噪声或细节）。\n比喻： 想象一下从原始地形中减去被平滑后的地形，剩下的就是突出的部分（小山峰）。\nBottom Hat (底帽):\n原理： 计算图像的闭运算结果与原始图像的差值。\n效果： 可以提取比结构元素小的暗区域（例如，孔洞或裂缝）。\n比喻： 想象一下用被填平后的地形减去原始地形，剩下的就是凹陷的部分（小坑）\nImageCompositeMasked 图像遮罩复合, 将源图像覆盖在目标图像上，在指定坐标处进行叠加，可选择调整大小和使用遮罩. 与潜空间遮罩复合类似.\nRebatch Images 重新分批图像批次\nRepeatlmageBatch 重复图像批次\nImageFromBatch 从一批图像中取一个片段\nCanny 图像的边缘检测, low_threshold 阈值下限, 影响边缘检测的灵敏度, high_threshold 阈值上限, 影响边缘检测的选择性\nImage Crop 图片裁剪\nSaveAnimatedWEBP 将一系列图像保存为动画 WEBP 文件. lossless 是否使用无损压缩, quality 压缩质量, method 压缩方法: 默认, 最快, 最慢\nSaveAnimatedPNG 从一系列帧创建和保存动画PNG图像. compress_level 压缩级别\nWebcam Capture 打开网络摄像头, capture_on_queue 参数控制摄像头捕获图像的时机，True 表示在节点执行时捕获，False 表示在工作流开始时捕获\n蒙版 Load Image (as Mask) 加载图像作为蒙版, alpha 使用图像的 Alpha 通道 (透明度通道) 作为遮罩, red/green/blue 使用图像的红色/绿色/蓝色通道作为遮罩\nConvert Mask to Image 遮罩转图像\nConvert Image to Mask 图像转遮罩\nImageColorToMask 将彩色图像根据指定的颜色转换为遮罩\nSolidMask 生成一个在所有维度上具有统一值的实心遮罩, 特别适用于进一步处理或作为更复杂遮罩操作的起点. value: 用于填充整个遮罩的统一值, 决定了遮罩的基本颜色或强度\nInvertMask 反转遮罩\nCropMask 裁剪遮罩\nMaskComposite 遮罩合并, 合并操作:\nmultiply (相乘): 输出遮罩是两个输入遮罩对应像素值相乘的结果。重叠区域倾向于变暗(值变小)。\nadd (相加): 输出遮罩是两个输入遮罩对应像素值相加的结果。重叠区域倾向于变亮(值变大), 可能会饱和(值被限制为最大)。\nsubtract (相减): 输出遮罩是第一个输入遮罩减去第二个输入遮罩对应像素值的结果。重叠区域根据两者的差值变暗或者变亮。\nand (与): 输出遮罩是两个输入遮罩对应像素值进行逻辑与操作的结果。只有当两者都为非零值时，输出才为非零，否则为零。重叠区域倾向于保留两者重叠的部分, 其他部分变为透明。\nor (或): 输出遮罩是两个输入遮罩对应像素值进行逻辑或操作的结果。只要两者中有一个为非零值，输出就为非零。重叠区域倾向于只要有遮罩的部分都变为不透明。\nxor (异或): 输出遮罩是两个输入遮罩对应像素值进行逻辑异或操作的结果。当两者值不同时，输出为非零，否则为零。重叠区域倾向于只保留两者只有一个遮罩的部分。\nFeatherMask 羽化遮罩, 指定从上/下/左/右侧边缘开始应用羽化效果的距离\nGrowMask 修改给定遮罩的大小，可以选择性地对角落应用渐缩效果. expand 确定遮罩修改的大小和方向, 正值导致遮罩扩展, 而负值导致收缩, tapered_corners 设置为 True 时, 修改过程中对遮罩的角落应用渐缩效果\nThresholdMask 按阈值转换遮罩, 较高的值包含较少的像素，较低的值包含更多的像素\nPorter-Duff Image Composite 波特-达夫图像合成. 基于 Porter-Duff 合成规则对两张图像进行合成，其中一个图像作为前景 (Source, 通常表示为 SRC)，另一个作为背景 (Destination, 通常表示为 DST)\n合并模式 (A 和 B 分别表示前景和背景图像):\nSRC : 只显示前景图像 (A)。\nDST : 只显示背景图像 (B)。\nSRC_OVER: 前景覆盖在背景上 (A over B)，这是最常用的模式。\nDST_OVER: 背景覆盖在前景上 (B over A)。\nSRC_IN: 只显示前景中与背景重叠的部分 (A in B)，且使用前景的不透明度。\nDST_IN: 只显示背景中与前景重叠的部分 (B in A)，且使用背景的不透明度。\nSRC_OUT: 只显示前景中不与背景重叠的部分 (A out B)。\nDST_OUT: 只显示背景中不与前景重叠的部分 (B out A)。\nSRC_ATOP: 前景中与背景重叠的部分覆盖在背景上 (A atop B)，其他部分不显示. 使用前景的不透明度。\nDST_ATOP: 背景中与前景重叠的部分覆盖在前景上 (B atop A)，其他部分不显示. 使用背景的不透明度。\nXOR: 显示前景和背景中不重叠的部分 (A xor B)。\nCLEAR: 清除所有内容，得到一个透明图像。\nADD: 将前景和背景的像素值相加。\nMULTIPLY: 将前景和背景的像素值相乘。\nSCREEN: 类似于“叠加”，但更亮一些。\nOVERLAY: 根据背景的亮度，对前景进行“叠加”或“滤色”。\nDARKEN: 选择前景和背景中较暗的像素。\nLIGHTEN: 选择前景和背景中较亮的像素。\nSplit Image with Alpha 分离图像颜色和透明度\nJoin Image with Alpha 将图像与其对应的 Alpha 遮罩结合\n测试功能 (暂不介绍) BETA Latent Blend BETA VAE Decode (Tiled) BETA VAE Encode (Tiled) BETA LoadLatent BETA SaveLatent BETA AddNoise BETA Self-Attention Guidance BETA PerpNegGuider BETA PhotoMakerLoader BETA PhotoMakerEncode BETA CLIPTextEncodeControlnet BETA StableCascade_SuperResolutionControlnet BETA Differential Diffusion BETA UNetSelfAttentionMultiply BETA UNetCrossAttentionMultiply BETA CLIPAttentionMultiply BETA UNetTemporalAttentionMultiply BETA SamplerEulerCFG++ BETA Extract and Save Lora BETA TorchCompileModel ",
    "description": "",
    "tags": null,
    "title": "内置节点 2",
    "uri": "/02_comfyui/03_built-in/02-built-in/"
  },
  {
    "content": "高级节点 Load CLIP 加载 CLIP 模型, CLIP 应用类型选择:\nstable_diffusion: clip-l\nstable_cascade: clip-g\nsd3: t5/clip-g/clip-l\nstable_audio: t5\nmochi: t5\nItxv: t5\nstable_diffusion\rstable_cascade\rsd3\rstable_audio\rmochi\rItxv Load Diffusion Model 加载 UNET 模型, 权重类型:\ndefault\rfp8_e4m3fn\rfp8_e4m3fn_fast fp8_e5m2 default:\n通常指的是模型的原始精度，大多数情况下是 FP32 (单精度浮点数) 或 FP16 (半精度浮点数)。\n具体取决于模型在训练时使用的精度。\ndefault 提供最高的数值精度，但内存占用也最大，计算速度相对较慢。\nfp8_e4m3fn:\n这是一种 8 位浮点数格式，具体来说是 E4M3FN 变体。\nE4M3FN 指的是：\nE4: 4 位指数 (Exponent)\nM3: 3 位尾数 (Mantissa)\nFN: 指示浮点数表示法，包括正负无穷和 NaN (Not a Number)。\nfp8_e4m3fn 比 FP32 或 FP16 占用更少的内存，并且可以加速计算。\n但是，由于精度降低，可能会引入一些数值误差，导致生成质量略微下降。\nfp8_e4m3fn_fast:\n这与 fp8_e4m3fn 相同，都是 E4M3FN 格式的 8 位浮点数。\n_fast 后缀通常表示它使用了更快的硬件指令或算法来进行类型转换或计算，以牺牲一些潜在的精度为代价。\n目标是在 fp8_e4m3fn 的基础上进一步提升速度。\nfp8_e5m2:\n这是另一种 8 位浮点数格式，具体来说是 E5M2 变体。\nE5M2 指的是：\nE5: 5 位指数 (Exponent)\nM2: 2 位尾数 (Mantissa)\n相比 fp8_e4m3fn，fp8_e5m2 拥有更大的指数范围，但尾数精度更低。\n这使得 fp8_e5m2 能够表示更大范围的数值，但在表示精度上不如 fp8_e4m3fn。\n在某些情况下，fp8_e5m2 可能更适合处理具有较大动态范围的模型权重。\nDualCLIPLoader 双 CLIP 加载器, 应用类型选择:\nsdxl\rsd3\rflux sdxl: clip-l, clip-g\nsd3: clip-l, clip-g 或 clip-l, t5 或 clip-g, t5\nflux: clip-l, t5\nDiffusersLoader 已弃用\nTripleCLIPLoader 三 CLIP 加载器, 适用于 SD3, 同时加载 clip-g, clip-l, t5\nConditioningZeroOut 条件零化, 生成无条件信号\nConditioningSetTimestepRange 在特定的时间步范围内应用 conditioning, 更精细地控制 conditioning 在扩散过程的哪个阶段起作用\nCLIPTextEncodeSDXLRefiner SDXL Refiner 模型文本编码, ascore 参数用于 Aesthetic Score (美学分数) 的条件控制, 它允许你引导模型生成更符合特定美学评分的图像\nCLIPTextEncodeSDXL SDXL 文本编码:\nwidth: 生成图像的目标宽度（以像素为单位）。\nheight: 生成图像的目标高度（以像素为单位）。\ncrop_w: 在编码 conditioning 之前，对内部生成的图像进行中心裁剪的宽度偏移量（可以为正或负）。通常设为0。\ncrop_h: 在编码 conditioning 之前，对内部生成的图像进行中心裁剪的高度偏移量（可以为正或负）。通常设为0。\ntarget_width: 用于计算内部生成图像大小的宽度目标值，通常和width一致。\ntarget_height: 用于计算内部生成图像大小的高度目标值，通常和height一致。\ntext_g: clip_g 编码的文本。\ntext_l: clip_l 编码的文本。\nCLIPTextEncodeSD3 SD3 模型的文本编码:\nclip_l: clip_l 编码的文本\nclip_g: clip_g 编码的文本\nt5xxl: t5xxl 编码的文本\nempty_padding: 一个特殊的填充表示，用于在输入文本较短时进行填充，以适应模型对固定长度输入的需要\nCLIPTextEncodeHunyuanDiT HunyuanDiT 模型文本编码, bert/mt5xl 模型分别编码\nCLIPTextEncodeFlux flux 文本编码, guidance 调节引导强度,\nFluxGuidance 条件转 flux 引导, 并调节引导强度\nModelMergeSimple 合并模型 1 和 2, 当 ratio 为 1 时, 保留 1, 0 时保留 2, 其他值按比例合并\nModelMergeBlocks 分层融合模型, 模型 2 的 输入层/中间层/输出层 按比例融合进 模型 1\nModelMergeSubtract 模型 1 减去 模型 2 进行融合, multiplier 减去的强度\nModelMergeAdd 将 模型 2 添加到 模型 1 中\nSave Checkpoint 保存模型(可包含clip/vae)\nCLIPMergeSimple CLIP 合并\nCLIPMergeSubtract CLIP1 - CLIP2\nCLIPMergeAdd CLIP 2 加到 CLIP 1 中\nCLIPSave 保存 CLIP\nVAESave 保存 VAE\nModelSave 保存模型\nImageOnlyCheckpointSave 保存仅图像模型 (只能图生图的模型)\nModelMergeSD1 SD1 模型无缝集成到一个统一的框架中, 1 决定模型初始结构, 2 扩展模型能力:\ntime_embed: 调整模型中的时间嵌入\nlabel_emb: 调整标签嵌入\ninput_blocks/middle_block/out_blocks: 输入层/中间层/输出层\nout: 调整最终输出\nModelMergeSD2 同 SD1\nModelMergeSDXL 同 SD1\nModelMergeSD3_2B SD3_2B 模型合并\nModelMergeAuraflow Auraflow 模型合并\nModelMergeFlux1 Flux1 模型合并\nModelMergeSD35_Large SD35_Large 模型合并\nModelMergeMochiPreview MochiPreview 模型合并\nModelMergeLTXV LTXV 模型合并\nModelSamplingDiscrete 模型采样离散, 修改模型的采样行为\neps\rv_prediction lcm\rx0 ModelSamplingContinuousEDM 集成连续 EDM (基于能量的扩散模型) 采样技术来增强模型的采样能力:\nv_prediction\redm_playground_v2.5\reps ModelSamplingContinuousV 集成连续 V\nModelSamplingStableCascade shift 调整采样分布, 调整 StableCascade 采样能力\nModelSamplingSD3 调整 SD3 的采样能力\nModelSamplingAuraFlow 调整 AuraFlow 的采样能力\nModelSamplingFlux 调整 Flux 的采样能力\nRescaleCFG 按系数缩放 CFG 范围\nModelSamplingLTXV 调整 LTXV 的采样能力\nTomePatchModel ratio: 调整模型注意力机制\nFreeU 大幅提高扩散模型样本质量的方法, https://github.com/ChenyangSi/FreeU\n推荐参数:\nSD1.4: （will be updated soon）\nb1: 1.3, b2: 1.4, s1: 0.9, s2: 0.2\nSD1.5: (will be updated soon)\nb1: 1.5, b2: 1.6, s1: 0.9, s2: 0.2\nSD2.1\nb1: 1.4, b2: 1.6, s1: 0.9, s2: 0.2\nSDXL\nb1: 1.3, b2: 1.4, s1: 0.9, s2: 0.2 SDXL results\n推荐尝试参数范围\nb1: 1 ≤ b1 ≤ 1.2\nb2: 1.2 ≤ b2 ≤ 1.6\ns1: s1 ≤ 1\ns2: s2 ≤ 1\nFreeU V2 https://github.com/WASasquatch/FreeU_Advanced\nHyperTile 优化U-Net和VAE模型中的自我关注层的方法, https://github.com/tfernd/HyperTile\nPatchModelAddDownscale (Kohya Deep Shrink) 通过对特定块进行缩小和放大操作来修改模型，控制分辨率并优化性能:\nblock_number 区块号\n此整数参数指定模型内将应用缩小操作的块号。默认值为 3，最小值为 1，最大值为 32。调整此参数允许您针对模型的特定层进行缩小。\ndownscale_factor 缩减因子\n该浮点参数确定所选块将缩小的因子。默认值为 2.0，最小值为 0.1，最大值为 9.0。较高的缩小因子将更显着地降低分辨率。\nstart_percent 起始百分比\n此浮点参数将缩小操作的起点定义为模型处理的百分比。默认值为 0.0，范围为 0.0 到 1.0。这使您可以控制模型执行期间开始缩小的时间。\nend_percent 结束百分比\n此浮点参数将缩小操作的结束点设置为模型处理的百分比。默认值为 0.35，范围为 0.0 到 1.0。此参数可帮助您定义缩小效果的持续时间。\ndownscale_after_skip 跳过后缩小规模\n此布尔参数指示模型内的跳过连接后是否应发生缩小。默认值为 True。设置此参数可帮助您控制与跳过连接相关的精确缩小点。\ndownscale_method 缩小方法\n该参数指定用于缩小尺寸的方法\nbicubic\rnearest-exact\rbilinear\rarea\rbislerp upscale_method 放大方法\n该参数定义了缩小操作后用于放大的方法\nPerturbedAttentionGuidance 扰动注意引导, 自校正采样, https://github.com/sunovivid/Perturbed-Attention-Guidance\nSaveAudio 保存音频\nLoadAudio 加载音频\nPreviewAudio 预览音频\nSavelmageWebsocket 保存图像以用 api 获取\n以下测试功能暂不介绍:\nBETA SkipLayerGuidanceSD3 BETA SkipLayerGuidanceDiT BETA Create Hook LoRA BETA Create Hook LoRA (MO) BETA Create Hook Model as LoRA BETA Create Hook Model as LoRA (MO) BETA Set Hook Keyframes BETA Create Hook Keyframe BETA Create Hook Keyframes Interp. BETA Create Hook Keyframes From Floats BETA Combine Hooks [2] BETA Combine Hooks [4] BETA Combine Hooks [8] BETA Cond Set Props BETA Cond Set Props Combine BETA Cond Set Default Combine BETA Cond Pair Set Props BETA Cond Pair Set Props Combine BETA Cond Pair Set Default Combine BETA Cond Pair Combine BETA Set CLIP Hooks BETA Timesteps Range ",
    "description": "",
    "tags": null,
    "title": "内置节点 3",
    "uri": "/02_comfyui/03_built-in/03-built-in/"
  },
  {
    "content": " ",
    "description": "",
    "tags": null,
    "title": "提示工程日常使用案例大全",
    "uri": "/04_prompt-engineering/03-prompt-engineering/"
  },
  {
    "content": " ",
    "description": "",
    "tags": null,
    "title": "提示工程日常使用技巧",
    "uri": "/04_prompt-engineering/02-prompt-engineering/"
  },
  {
    "content": "提示工程 这个词是随着生成式 AI 诞生的, 尤其是 2020 年 GPT-3 和 2021 年 Disco Diffusion 的发布, 引发了提示工程热潮. 由于生成文本 和 绘画质量好坏, 跟提示词的好坏有很重要的关系, 于是人们开始研究, 怎样设计提示词, 提示工程就流行了起来.\n随着大模型能力提升, 提示工程似乎没那么重要了, 现在能用比较普通和直白的语言, 就能得到不错的答案. 但对于更专业性, 更复杂的应用场景, 提示工程还是非常重要的, 同时, 个人的语言表达能力, 现在变得更加重要. “学好数理化, 走遍天下都不怕”, 即将变成 “学好语文和英语, 开口世界归自己”, 真是风水轮流转, 相信开口就能把事情做了的时代就在不远的将来. 翻译软件越来越强大, 英语对大多数人可能不再重要, 但对于想改变世界, 创造 AI 而不是使用 AI, 站在科技风口浪尖的人来说, 仍然极为重要.\n写好提示词 或者说 准确地表达, 是基础中的基础, 无论 AI 写作, AI 绘画, AI 音乐, AI 视频, 用 AI 来学习, 用 AI 来解决各种问题, 以及 测试和调整大模型等等, 归根到底要语言基本功扎实, 才能准确高效的得到结果.\n我现在每天都离不开 ChatGPT(或者其他 AI). 我相信, AI 一定会成为每一个人时时刻刻陪伴在身边的超级大脑, 于是我决定搜非常棒的提示工程资料, 教程, 案例等放这里, 以便随时查阅.\n提示工程 (Prompt Engineering): 是指为 AI 模型设计和优化输入文本（即“提示”或“指令”）的过程，以确保模型生成的输出符合预期或更高质量。提示工程包括选择合适的语言、结构、语境和细节来指导 AI 生成更准确、相关、有用或创造性的响应。\n提示工程（Prompt Engineering）的作用:\n提高输出质量：通过精心设计的提示，可以显著提高 AI 模型的输出准确性和相关性。\n减少偏见和错误：良好的提示可以帮助减少 AI 生成的内容中的偏见或错误，因为它引导模型关注正确的方面。\n增强用户体验：在应用中，用户与 AI 的交互体验很大程度上依赖于提示的质量，好的提示可以让用户更容易获得所需信息或服务。\n创新和创造力：在生成性任务中，如文本生成、图像创建等，提示工程可以激发模型的创造性输出，推动应用的创新。\n效率提升：精确的提示可以减少后续的修订工作，提高生成内容的效率。\n提示工程（Prompt Engineering）的学习方法:\n理解基础知识： 学习 AI 模型的工作原理，特别是如何处理和理解语言输入。了解你所使用的 AI 模型的特性，如其训练数据、预置的偏见或特定的能力。\n实践与试错： 进行大量的实践，尝试不同的提示结构、语言风格、长度和复杂度。使用A/B测试来比较不同提示的效果，分析哪些元素对输出质量产生影响。\n学习社区资源： 参与 AI 和 NLP （自然语言处理）的社区，如 GitHub 上的项目、论坛（如Reddit）、Stack Overflow等，学习他人的提示工程实践和技巧。查看开源的提示库或模型文档中提供的示例提示。\n迭代优化： 基于反馈和生成的输出不断优化你的提示。每次迭代都应该尝试改进一个或几个元素。使用工具或脚本来自动化提示的测试和评价过程。\n专门化学习： 根据具体应用领域（如法律、医学、客服等）学习行业特定术语和交流方式，以便设计更专业的提示。\n学习心理学和语言学： 了解语言如何影响思维和行为可以帮助设计更有效的提示。研究语言学可以帮助优化语法、语义和语用学方面的提示。\n课程和培训： 参与在线课程或工作坊，这些课程可能专注于 NLP 或 AI 应用中的提示工程。\n保持更新： AI 技术发展迅速，保持对新模型、技术和最佳实践的关注是必要的。\n提示工程资源整理 ",
    "description": "",
    "tags": null,
    "title": "提示工程资源, 案例集",
    "uri": "/03_prompts/"
  },
  {
    "content": "以下是 2024-12-15 整理的非常棒的 Prompts 资料, 以后新增都标注日期:\n提示工程指南(非常全面, 需要魔法): https://www.promptingguide.ai/zh , GitHub: https://github.com/dair-ai/Prompt-Engineering-Guide\nChatGPT 学生学习指南: https://openai.com/chatgpt/use-cases/student-writing-guide/\n什么是提示工程: https://www.ibm.com/topics/prompt-engineering\nClaude 思考: https://github.com/richards199999/Thinking-Claude\n非常棒的 ChatGPT 提示词: https://github.com/f/awesome-chatgpt-prompts\n提示词超市: https://promptbase.com/ , AI 专家创作的 Midjourney、ChatGPT、DALL·E、Sora、FLUX 等提示词\nOpenAI 社区: https://community.openai.com/\n大语言模型提示词指南: https://huggingface.co/docs/transformers/main/tasks/prompting\n提示工程 reddit 社区: https://www.reddit.com/r/PromptEngineering/\n提示工程和大语言模型开发指南: https://github.blog/ai-and-ml/generative-ai/prompt-engineering-guide-generative-ai-llms/\n非常棒的提示工程: https://github.com/promptslab/Awesome-Prompt-Engineering\n提示学习的开源框架: https://github.com/thunlp/OpenPrompt\n学习提示: https://learnprompting.org/\n阿里 Prompt 最佳实践: https://www.alibabacloud.com/help/zh/model-studio/use-cases/prompt-engineering-guide\n百度提示工程最佳实践: https://qianfan.cloud.baidu.com/qianfandev/topic/269353\n提示工程大会, 一句提示词, 一个新世界: https://www.pec.asia/\n李继刚开源提示词: https://ai.alexdbg.com/4030.html\n提示工程指南: https://github.com/dair-ai/Prompt-Engineering-Guide\n提示指南: https://python.useinstructor.com/prompting/\n",
    "description": "",
    "tags": null,
    "title": "提示工程资源整理",
    "uri": "/03_prompts/01-prompt-sources/"
  },
  {
    "content": "感谢您的到来, 您能打开这本书, 说明您对提示工程感兴趣, 或有了一定了解, 或已经认识到提示工程的重要性.\n本书所说的提示工程, 主要指提示词的编写, 设计, 优化等, 以求使 AI 的输出更符合甚至超越预期, 仅在 AI 使用的层面展开, 并不深入探讨算法设计, 模型优化等深层次的内容, 虽然这更加重要.\n在我看来, AI 提示词的编写, 应该是非常自然语言化的, 也就是只要会说话, 会打字, 人人都能够编写提示词, 而不是再学习一些编程语言如 Lisp, Python 等才能实现. 这种简洁直观的方式, 才能惠及所有人, 而且 AI 是有记忆和学习能力的, 人人都使用自然语言与 AI 交流, AI 也会变得更加亲近人, 成为人们亲密的知识顾问, 良师益友. 所以, 我在提示工程前面多加了自然语言四个字, 将这本书命名为 «自然语言提示工程入门».\n我们正处于一个技术革命的拐点. 人工智能不再是遥不可及的科幻概念, 而是已经悄然走进我们的日常生活. 现在的 AI 已经非常好用, 我相信, 在不久的将来, 与人工智能交谈会像与最聪明、最有耐心的朋友聊天一样自然.\n提示工程, 就是与 AI “说话” 的艺术. 看似简单, 实则大有学问. 就像下棋, 懂得基本套路和技巧, 小白也能成为高手.\n特别说明, 这是一个快速变化的领域, 我也正学习中, 希望书中的一点经验总结, 实践案例, 能够抛砖引玉, 带来更多优秀的, 精彩的经验分享和实践案例.\n高级提示工程概述 开胃点心, 从几个案例开始 提示工程日常使用案例大全 提示工程日常使用技巧 ",
    "description": "",
    "tags": null,
    "title": "自然语言提示工程入门",
    "uri": "/04_prompt-engineering/"
  },
  {
    "content": "上一章我们详细梳理了官方桌面版本的界面—-各种功能, 设置, 快捷键, 扩展等等.\n这一章我们就来详细介绍基本概念, 其实也算不上基本概念, 就是对一些关键词的大白话描述, 能够简单理解就行, 需要深入研究, 问 AI 能够将实现细节, 以及背后的数学原理都讲清楚. 这里的基本概念仅限于图像生成, 而文本,音频和视频等生成将在后面介绍.\n基本概念非常重要, 可以说, 在这个 AI 时代, 掌握基本概念是急速学习的秘诀. 不管学什么, 基本概念掌握了, 你就能够比较标准的组织相关领域的语言, 然后向装着全世界知识的 AI 提问, 他就能够给你很好的答案, 你也能够相对轻松地理解, 然后继续追问, 大踏步向着更专业迈进…\n从基本工作流开始 一个完整的 ComfyUI 生图工作流, 大致可以分为 4 个阶段:\n加载模型 输入, 预处理和设置参数 执行生成 后处理和输出结果 加载模型 目前知名的大模型有 SD 1.4, SD 1.5, SD 1.5 LCM, SD 1.5 Hyper, SDXL 1.0, SDXL Turbo, SDXL Lightning, SDXL Hyper, SD 3.5 Medium, SD 3.5 Large, SD 3.5 Large Turbo, Pony, Flux .1S, Flux.1 D, Stable Cascade, PixArt, Hunyuan, Kolors 等, 以及社区基于底模训练的各种版本, 非常丰富.\n其中, 最常用的是 SD1.5, SDXL, SD3.5, Flux 四大类.\n除了大模型, 还有与大模型配套的模型和组件, 主要有 CLIP, VAE, Lora, Controlnet 等.\n对这些模型, 需要清楚下面几点:\n大模型是基底, 生图时, 不同的大模型一般需要不同的 CLIP, VAE, Lora, Controlnet 等.\nCLIP 是连接文本和图像的桥梁, 将输入的提示词 “翻译” 给计算机, 或反过来.\nVAE 则是将我们人类看到的图像, 处理成计算机能理解的信号, 或反过来.\nLora 是基于特定大模型训练而来, 它就像一个 “说明书”, 告诉大模型, “你要给我这个样子的”, “你要给我这个风格的”… 因此训练 Lora 需要对用来训练的图像打标 (也就是将图像翻译成提示词), 做出一个说明书来, 生成的时候, 又用这个说明书 (触发词) 指导大模型生成. 基于同一个大模型版本训练的 Lora, 一般都能用, 但是出不出效果, 就要看训练 Lora 的人用的底模具体是哪个版本.\nControlnet 则是基于大模型版本训练, 相同版本的大模型都能用, 通常用于 图像到图像 的生成. 它像一个 “结构图”, 告诉大模型, “你要按照我这个结构来画”, 不要将 “春树秋霜图” 画成 “小鸡食米图”.\nFP32, FP16, FP8, BF16 等, 表示模型权重的 数据类型 或 精度, FP32 提供高精度计算，FP16 和 BF16 提供更高效的计算, BF16 专门用于加速计算, 在精度方面，BF16 比 FP16 更能保留重要的信息，FP8 用于高效的计算，但会有精度损失. (还有整数精度的 INT8, INT16, INT32)\n常见的模型的格式有:\n.ckpt 文件：广泛使用，兼容性好。 但可能包含冗余信息，文件较大。\n.safetensors 文件： 更安全，高效，文件较小。 需要特定的加载器支持。\n.pt 或 .pth 文件： PyTorch 原生格式，兼容性好。 文件可能较大，需要 PyTorch 环境支持。\n.bin 文件格式通常用于存储二进制数据，在深度学习和机器学习领域，.bin 文件则常用于存储模型的权重和配置信息\n大模型可以将 CLIP 和 VAE 合并在一起, 这种版本的大模型, 加载时一般使用 Load Checkpoint 加载器:\n否则, 加载大模型时, 一般使用 Load Diffusion Model 加载器 (这种大模型通常是 扩散模型Diffusion Model 和 UNet 模型), 并且 CLIP 和 VAE 需要单独加载:\nLora 和 Controlnet 则需要单独加载.\n输入, 预处理和设置参数 提示词（Prompt）: 就是对你要生成的图像的自然语言描述. 正向提示词就是你要生成的结果; 负向提示词则是你不希望出现的结果.\n文本预处理\n分词：将文本分割成单词或子词，以便进行编码. 文本编码：将提示词转换为模型可以理解的嵌入向量。\n提示词通过 CLIP 编码, 引导模型生成与描述词相符的图像. 提示词可以不输入, 也会生成随机噪声, 指导生成大模型 “记忆中” 的随机图像.\n初始图像（Initial Image）\n输入图像：在某些情况下，可以提供一个初始图像，模型会基于这个图像进行生成或变换。图像变换：如风格迁移、图像修复或图像增强等任务。\n图像预处理\n调整大小：将输入图像调整到模型所需的尺寸。归一化：将图像像素值归一化到模型所需的范围（通常是 0 到 1）。数据增强：如旋转、翻转、裁剪等，用于增加数据多样性.\nLatent Image: 潜空间图像, 我们不理解但人工智能 “看” 得懂的图像. 如果需要用到 图生图, 则需要 VAE 编码为 Latent Image, 否则就生成一个空的 Latent Image, 它们都需要设置宽/高 (或者参考原图宽/高).\n如果用到 图像缩放, 裁剪, 图生图, 图像重绘, 用 Controlnet 预处理, 条件组合, 参考 Lora 等, 则需要各种预处理以及参数设置. 具体细节将在下一章 内置节点 对各种预处理节点详细介绍.\n执行生成 执行生成, 就是采样器按照之前的各种预处理条件, 和设置的参数, 拿起神笔开始泼墨挥毫画画的过程.\n参数详解:\nseed: 种子, 真的就是一颗种子, 埋到土里, 能长小花还是小草, 这颗种子很重要. 种子可以用来确定生成图像的构图 (宽高比, 采样器不变).\n图像生成后, 可以选择 fixed(固定), increment(增加), decrement(减小), randomize(随机化) 种子.\nsteps: 采样过程中的迭代次数, 步数 直接影响生成图像的质量、细节和计算效率。步数越多，生成过程的细节越丰富，生成结果的质量通常会更高，但同时，生成的时间和计算资源消耗也会增加, 需要一个平衡点. 复杂, 细节多的就增加步数.\nCFG: 同一个模型, 较高的 CFG 值, 图像会更符合输入条件，生成内容更加集中和精确，但可能牺牲创意和多样性。适用于需要精确控制生成内容的任务。较低的 CFG 值, 图像生成过程更具随机性和创意，适用于需要多样性或探索更多可能性的任务，但可能导致与输入条件不太一致的结果. 不同模型, CFG 值的最佳范围可能不同.\n采样器: 目前 ComfyUI 有\neuler\neuler_cfg_pp\neuler_ancestral\neuler_ancestral_cfg_pp\nheun\nheunpp2\ndpm_2\ndpm_2_ancestral\nLms\ndpm_fast\ndpm_adaptive\ndpmpp_2s_ancestral\ndpmpp_2s_ancestral_cfg_pp\ndpmpp_sde\ndpmpp_sde_gpu\ndpmpp_2m\ndpmpp_2m_cfg_pp\ndpmpp_2m_sde\ndpmpp_2m_sde_gpu\ndpmpp_3m_sde\ndpmpp_3m_sde_gpu\nddpm\nLcm\nipndm\nipndm_v\ndeis\nddim\nuni_pc\nuni_pc_bh2\n各种采样器都有什么特点, 优缺点, 自行 ChatGPT.\n对于需要快速生成的场景，可以选择 DPM_Fast、DDIM 等高效采样器.\n如果想要速度快、收敛性好、质量也不错，且想试试新东西的话，最好选择：\nDPM++ 2M Karras\nUniPC\n如果想要比较好的质量，同时不在意是否收敛的话，可以选择：\nDPM++ SDE Karras\nDDIM\n如果想要稳定、可复现的结果，不要用任何带有随机性的采样器，比如祖先采样器.\n如果想生成一些简单的结果，可以用 Euler 或 Heun. 在使用 Heun 时，可以调低一些步数来节省时间.\n调度器 scheduler: 目前 ComfyUI 有 调度器 优点 缺点 Normal 稳定，易于实现 生成质量一般，缺乏针对特定任务的优化 Karras 高质量生成，适合细节丰富的图像 计算开销大，复杂度高 Exponential 快速收敛，适合快速生成 生成质量较差，可能缺乏细节 SGM Uniform 噪声均匀，生成过程可控 计算开销大，生成速度较慢 Simple 高效，计算简单 生成质量低，缺乏灵活性 DDIM Uniform 高效，生成质量较好 噪声控制不如指数衰减精细 Beta 灵活，适应性强，能够定制噪声衰减曲线 调整复杂，计算开销较高 Linear Quadratic 生成图像细节丰富，适合高精度任务 计算资源消耗大，生成速度较慢 denoise: 去噪过程的强度或程度. 较强的去噪, 有助于生成清晰, 细节丰富的图像，但可能牺牲图像的多样性和创意，并增加计算时间. 较弱的去噪, 在图生图时, 和原图更像. 不同的模型, 模型作者在训练时, 使用的步数, CFG, 采样器, 调度器等, 对模型有重要影响. 因此使用作者推荐的参数, 是不错的选择.\n后处理和输出结果 后处理则主要是对生成的图像进行放大 (可能需要加载放大模型), VAE 将潜空间图像解码为我们需要的图像, 预览, 对比生成结果以及保存图像等, 后面详细介绍.\n图像放大（Upscaling）\n图像放大是将低分辨率图像转换为高分辨率图像的过程。这通常需要使用放大模型（Upscaling Model）来提高图像的细节和清晰度.\n列举一些放大模型:\nLDSR\rRealESRGAN\rScuNET\rAura-SR\rSUPIR\rESRGAN\rDAT\rSRGAN\rRCAN\rRDN\rEDSR\rLapSRN\rVDSR\rDRCN\rDRRN\rMemNet\rCARN\rMSRN\rSRResNet\rSRCNN\rFSRCNN\rESPCN\rSRFBN\rSAN\rRNAN\rIMDN\rHAN\rDBPN\rMDSR\rCAR\rLIIF\rSwinIR\rRRDB\rRFDN\rLAPAR\rOISR\r这里 https://openmodeldb.info/ 有更多, 不同的放大模型针对性不一样, 有全能的, 有脸部的, 有风景的, 有动漫的, 有照片的… 等等等等\n不用模型, 潜空间放大也行, 小显存电脑, 用分块放大效果也很好, 后面会详解.\nVAE（Variational Autoencoder）解码. VAE 将潜空间图像解码为我们需要的图像.\n最后的输出结果还可以接入下一个工作流, 继续魔法创作. 甚至将多个工作流组装成大型生产线.\n",
    "description": "",
    "tags": null,
    "title": "基本概念",
    "uri": "/02_comfyui/02_basic/"
  },
  {
    "content": "搜索问答 纳米搜索 2024-12-02\n书生·浦语 2024-12-02\nKimi\nchatgpt\n秘塔\nperplexity\nMeta AI\n腾讯元宝\n豆包 - 抖音旗下 AI 智能助手\nGoogle AI Studio\nCohere | The leading AI platform for enterprise 2024-12-03\nClaude\n知乎直答\n360搜\nPoe\n通义tongyi.ai_你的全能AI助手-通义千问 2024-12-03\n天工AI - 搜索更深度，阅读更多彩 2024-12-03\n讯飞星火大模型-AI大语言模型-星火大模型-科大讯飞 2024-12-03\n文心一言 2024-12-03\nHuggingChat\n语鲸 2024-12-03\n深言达意 – 找词找句 2024-12-03\n爱校对官网-免费高效的错别字检查工具 2024-12-03\nLearn About 2024-12-03\n视频, 语音, 绘图等综合 智谱清言\n即梦AI - 一站式AI创作平台\n可灵 AI - 新一代 AI 创意生产力平台\n海螺 AI\n跃问\nLuma Dream Machine | AI Video Generator\nKREA AI - AIGC集合-风格-图有免费\nHome - Leonardo.Ai\nAI Test Kitchen\nLimeWire\nLe Chat - Mistral AI\nWHEE - 高品质的AI素材生成器\n视频 World Labs 2024-12-03\n腾讯混元文生视频 2024-12-03\nPixelDance - PixelDance AI - 领先的AI视频生成平台 2024-12-03\nVidAU Creative Center 2024-12-02\nkaze.ai - AI-powered Free Online Removing Watermark and Logos Tool 2024-11-27\nVidu，让想象发生\nHailuo AI Video Generator - Reimagine Video Creation\nRunway\n讲故事的方式发生了转变LTX工作室 — Storytelling Transformed | LTX Studio\nNoisee AI 音乐生成MV\nPika\nGenmo. Create videos and images with AI.\nHome | PixVerse\nViggle AI\n万德动力 — Wonder Dynamics\nHeyGen - AI Spokesperson Video Creator\nAiuni\nDomoAI: video to video, video to animation and more\nFlair\nWarpvideo AI: Change Video Style with AI\nHedra 数字人\nAI 擁抱 - 免費線上 AI 擁抱影片生成器\nMOKI - 我用AI做短片\nMeshcapade | 编辑人物动作\nBoomCut - 爆剪辑 - 小影科技旗下 AI 内容创意产品与服务平台\n绘画设计 Create stunning visuals in seconds with AI.\n超能画布首页\nAdobe Firefly\nDesign - Playground\nSkybox AI 360°全景\nMagic Studio：利用 AI 制作精美图像\nmidjourney\nRemove Background from Image for Free – remove.bg\nCraiyon, formerly DALL-E mini\nCreate - Artbreeder\nNightCafe Creator\nProjects - Recraft\nBlendbox.ai 多图组合\nIdeogram 画布\nLogo-creator.io – Generate a logo\n在线抠图软件_图片去除背景 | remove.bg – remove.bg\n触手AI\n3D Meshy - Free 3D Models Generated from Images and Text\nImmersity AI | Convert Image and Video to 3D\nTripo AI - 用文字或图片免费生成3D模型\n语音, 音乐 Home • Hume AI 2024-12-03\nSuno\n海绵音乐\nFree Text to Speech \u0026 AI Voice Generator | ElevenLabs\nUdio AI Music Generator - Make Original Tracks in Seconds\nStable Audio - Generate\n在线免费文本转语音 - TTS-Online | 多种声音与二次元语音\nSoundboard - TUNA - Download Unlimited Free Meme Sounds\n节奏生成器-Beat Blender 音乐\n网易天音 - 一站式AI音乐创作工具 - 官网\n提示词 promptoMANIA:绘画提示生成器\nLexica\nPromptHero - 提示词大全\n代码 Codeium · Free AI Code Completion \u0026 Chat 2024-12-02\nMarsCode - AI IDE\nCursor\nbolt.new\nScriptEcho | AI生成生产级代码 |\n其他 NotebookLM | Note Taking \u0026 Research Assistant Powered by AI\n扣子 - AI 智能体开发平台\nFigma\nIlluminate | Learn Your Way\nChat Nio\nLlamaOCR.com – Document to markdown\nNeo AI engineer\nExcalidraw\n模型, 资源, 工作流 Discovery | OpenArt\nShakker - Generative AI design tool with diverse models\n首页 · 魔搭社区\nComfy Workflows\nOpenModelDB\nFREE online image generator and model hosting site! | Tensor.Art\nCivitai: The Home of Open-Source Generative AI\nCodeWithGPU | 能复现才是好算法\nLiblibAI-哩布哩布AI - 中国领先的AI创作平台\nComfyUI工作流 - 在线运行，速度快，不报错\nFREE online image generator and model hosting site! | Tensor.Art\nCephalon Cloud 端脑云 - AIGC 应用平台\n",
    "description": "",
    "tags": null,
    "title": "项目的官方网站",
    "uri": "/01_aigc_object/01_object/"
  },
  {
    "content": "在 ComfyUI 中，节点是代表工作流中 特定操作 或 功能 的基本构建块, 是一个单独处理单元。将节点连接在一起以创建复杂的图像生成工作流。\n从本章开始, 需要操作节点和创建工作流, 文后附有 基本操作附录 和 快捷键附录, 了解它们将是非常有用的. 操作中反复翻看将大大提升效率.\n每个节点在 ComfyUI 的界面中通常是一个带有名称、输入、输出和参数控件的框\n节点通常包含:\nInputs 输入. 输入是节点接收数据的连接点。每个输入都有其接受的特定数据类型，确保连接节点之间的兼容性。\nOutputs 输出. 输出是将数据发送到其他节点的连接点。与输入一样，输出也定义了数据类型，决定了它们传递的信息类型。\nParameters 参数. 参数是控制节点行为的设置。这些可以包括数值、文本字符串或预定义选项的选择。\nInternal Logic 内部逻辑. 这是节点的核心功能，由其背后的代码定义。它确定节点如何处理其输入、使用其参数并生成其输出。\n一个简单完整的工作流 下面我们正式开始内置节点介绍. 先将一个完整的工作流组成介绍完, 再依次按分类介绍每个节点:\n1, Load Checkpoint: 这个节点任务是从文件夹加载模型, 而且必须是包含 基础模型, CLIP 和 VAE 的模型 (具体见上一章基本概念), 然后分别输出.\n2, CLIP Text Encode (Prompt): CLIP 文本编码节点, 是通过输入的 CLIP 模型, 将输入的 自然语言提示词 编码为潜空间向量输出, 用来引导图像生成.\n3, Empty Latent Image: 空的潜空间图像, 该节点的作用就是创建空的潜空间图像, 通过参数指定宽, 高 和数量, 然后输出.\n4, KSampler: 采样器节点, 该节点任务是 输入模型 按照输入的各种条件, 设置的各种参数去噪, 执行图像生成, 然后输出 (输出的仍然是潜空间图像). 拟人化理解就是 AI 在潜空间中按照人的指令在画画. 各种参数详解见上一章 基本概念.\n5, VAE Decode: VAE 解码, 该节点任务是输入的 VAE 模型, 将 AI 画好的潜空间画作, 映射为现实图像输出.\n6, Preview Image: 图像预览节点, 将最后输出的图像打印到屏幕上供你欣赏.\n这就是一个完整的工作流, 它由一个个各司其职的节点组成, 协调完成创作大业. 工作流执行过程需要注意几点 (重要):\n整个生成过程是一环扣一环的, 哪个节点出错, 它后面的节点就不会执行.\n执行过一次之后, 下一次执行只运行有改动的节点, 以及依赖于它的节点. 例如你执行一次之后, 如果什么也没动, 连种子也固定, 在点击执行它不理你.\n生成的图像, 或导出的工作流, 你直接加载或拖到窗口中, 会加载完整的工作流, 包括种子, 也就是可以将上次生成的图像完全复现出来 (注意固定种子).\n你可以使用 () (英文括号) 来更改单词或短语的权重，例如：(beautiful girl：1.2) 或 (dog：0.8). () 的默认权重值为 1.1. 要在实际提示词中使用 () 字符，要转义它们，比如你要在图中生成 (), 就要 \\(\\) 这样.\n你可以使用 {day|night} 这样的语法, 将提示词变成动态提示词, 这样，每次执行生成, 会从 {} 中, | 符号隔开的单词或短语中随机选一个, 要在实际提示词中使用 {} 字符，请转义它们，例如：\\{\\}.\n提示词还支持 C 语言风格的注释, 注释的提示词不会生效. 像这样: // comment 或 /* comment */\n要使用 embeddings 模型，请将它们放在 models/embeddings 目录中，并在提示词中注明 (您可以省略 .pt 扩展名): embedding:embedding_filename.pt\n内置节点太多, 需要很多章节才能讲完, 下一章开始正式介绍每个内置节点.\n基本操作附录 加载节点 加载节点有多种方式.\n鼠标右击画布窗口的空白处, 可以按下图选择节点, 将选择的节点加载出来: 除了, Add Node, 其他选项介绍:\nAdd Group: 新建一个分组框 (这个操作麻烦, 不如选中要分组的节点, Ctrl + g 直接分为一个组)\nConvert to Group Node: 选中多个节点, 可以用这个选项将多个节点合并为一个组节点 (新功能)\nManage Group Nodes: 管理组节点\nAdd Group For Selected Nodes: 将选中的多个节点分为一个组 (Ctrl + g)\nSave Selected as Template: 保存选中的节点作为模板\nNode Templates: 节点模板管理, 可以导入/导出节点模板\n鼠标双击空白处, 弹出节点搜索框, 从搜索框中搜索节点加载 (鼠标移动到搜索出来的节点名上, 会弹出预览): 鼠标点击节点输出点, 按住拖动, 将拉出一条线, 放开线, 弹出的对话框也可加载节点, 如果匹配, 则直接与拖出的线相连: 直接打开节点库, 搜索或选择: 连接节点 节点加载出来, 按照上一章介绍的四个基本板块(加载模型, 输入/预处理/设置参数, 执行生成, 输出), 连接成一个完整工作流. 从输出点拖动线, 会自动高亮匹配的输入点并连接.\n并不是节点的所有输入/输出都需要连接, 有的节点的输入/输出项是可选的, 可以不用输入, 这种情况遇到再说.\n加载模型 模型统一放到 ComfyUI\\models 下, 对应的文件夹中, 自动下载的模型会自动存放, 手动下载的根据模型分类手动存放 (一般模型发布者会说明放到什么位置):\n如果模型的位置放对了, 点击模型加载器节点选择, 将弹出可选模型:\n输入提示词 提示词输入, 一般在 CLIP 文本编码器节点输入, 还有其他输入方式, 将在相应的节点介绍.\n参数设置 各节点有各节点的参数, 根据自己的需要, 计算机的配置 (显存), 模型参数要求等设置相应的参数, 例如前面介绍过的采样器节点. 具体细节到相应节点介绍.\n节点设置 鼠标放到节点上右击, 将弹出节点设置对话框, 基本所有节点设置都差不多, 不同的节点可能略有不同:\nInputs Outputs Convert to Group Node 将选中节的多个节点转换为一个组节点 Properties 设置节点的 S\u0026R 名称(节点太多, 可能有共用同一个名称, 通过这里可改唯一名称) Properties Panel 设置节点以下参数: Title 修改节点标题 Mode 选择节点模式: Always: 节点被改变或输入被改变时执行\nNever: 禁用节点\nOn Evert/On Trigger\nResize: 重置节点框大小 Collapse: 折叠节点 Pin: 在窗口画布中固定节点的位置 Colors: 设置节点的颜色 Shapes: 选择节点边框样式 Bypass: 绕过节点(不执行而继续执行后面的) Copy (Clipspace): 复制 Clipspace, 目前只对有图像的节点有效 复制完 Clipspace 之后, 打开 Edit – Clipspace, 将弹出窗口:\n然后 MaskEditor 可以打开蒙版编辑, 对图像进行处理:\nPaste (Clipspace): 粘贴 Clipspace, 将图像复制到另一个节点 (例如加载图像节点, 如果已经蒙版处理过, 直接粘贴的是处理过的图像) Convert Widget to Input: 将小组件转换为输入, 例如 CLIP 文本编码节点执行这个操作之后, 提示词就不能输入了, 只能从其他节点传进来 (可以用同样的操作转换回来): Clone: 克隆节点, 相当于复制一个节点 Remove: 删除节点 蒙版编辑器 分别是 撤销, 重做, 反转, 清除, 保存, 关闭蒙版编辑\n笔刷设置 可以设置笔刷的形状, 粗细, 透明度, 硬度, 平滑系数, 遮罩层的颜色, 遮罩透明度, 选择图层. (笔刷下面的是橡皮擦, 就不介绍了).\n喷漆设置 取色笔设置 组节点 选中多个节点, 右击可以选中转换为组节点:\n组节点可以同样的操作展开为单节点, 组节点还可以进行管理, 右击组节点:\n节点分组 选中多个节点, Ctrl + g 将节点分为一个组:\n一个组的节点, 拖动组边框, 将拖动全部组内节点, 右击组边框:\n可以让边框适应节点, 选中组内全部节点, 禁用组内全部节点, 绕过组内全部节点, 编辑组 (固定, 设置标题, 颜色, 标题字体大小, 移除组, 不会移除节点)\n快捷键附录 Keybind Explanation Ctrl + Enter Queue up current graph for generation 当前工作流执行生成 Ctrl + Shift + Enter Queue up current graph as first for generation 将当前工作流排到第一, 如果当前执行队列为空, 则直接执行生成 Ctrl + Alt + Enter Cancel current generation 终止当前生成 Ctrl + Z/Ctrl + Y Undo/Redo 撤销/重做 Ctrl + S Save workflow 保存工作流 Ctrl + O Load workflow 从文件夹中加载已有工作流 Ctrl + A Select all nodes 选中所有节点 Alt + C Collapse/uncollapse selected nodes 折叠/展开所选节点 Ctrl + M Mute/unmute selected nodes 禁用/解禁所选节点 Ctrl + B Bypass selected nodes 绕过所选节点, 就像节点不存在, 直接绕过执行 Delete/Backspace Delete selected nodes 删除所选节点 Ctrl + Backspace Delete the current graph 删除当前工作流 Space Move the canvas around when held and moving the cursor 按住鼠标可空格键, 拖动工作流画布 Ctrl/Shift + Click Add clicked node to selection 点击选中多个节点 Ctrl + C/Ctrl + V Copy and paste selected nodes 没有连接线复制/粘贴所选节 Ctrl + C/Ctrl + Shift + V Copy and paste selected nodes 保持连接线复制/粘贴所选节点 Shift + Drag Move multiple selected nodes at the same time 同时移动选中的所有节点 Ctrl + D Load default graph 加载默认工作流 Alt + + Canvas Zoom in 放大工作流画布 Alt + - Canvas Zoom out 缩小工作流画布 Ctrl + Shift + LMB + Vertical drag Canvas Zoom in/out 缩/放工作流画布 P Pin/Unpin selected nodes 锁定/解锁所选节点 Ctrl + G Group selected nodes 将所选节点分组 Q Toggle visibility of the queue 查看队列执行历史 H Toggle visibility of history 切换历史记录的可见性 R Refresh graph 刷新工作流 Double-Click LMB Open node quick search palette 打开节点快速搜索对话框 Shift + Drag Move multiple wires at once 点击连接点/线, 然后拖动去找新的连接点 Ctrl + Alt + LMB Disconnect all wires from clicked slot 点击连接点, 断开所有线 Alt + 拖动任意一个节点 复制一个拖动的节点 滚动鼠标中键 缩放画布 鼠标左键按住空白处拖动 移动整个工作流 ",
    "description": "",
    "tags": null,
    "title": "内置节点",
    "uri": "/02_comfyui/03_built-in/"
  },
  {
    "content": "视频 prs-eth/RollingDepth: Video Depth without Video Models 2024-12-03\nTencent/HunyuanVideo 2024-12-03\nhmrishavbandy/FlipSketch: FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations 2024-12-02\nKwaiVGI/LivePortrait: Bring portraits to life! 2024-12-02\nC0untFloyd/roop-unleashed: Evolved Fork of roop with Web Server and lots of additions 2024-12-02\njdh-algo/JoyVASA 2024-12-02\nPKU-YuanGroup/ConsisID: Identity-Preserving Text-to-Video Generation by Frequency Decomposition 2024-12-02\nrhymes-ai/Allegro: Allegro is a powerful text-to-video model that generates high-quality videos up to 6 seconds at 15 FPS and 720p resolution from simple text input. 2024-12-02\nk4yt3x/video2x: A machine learning-based lossless video super resolution framework. Est. Hack the Valley II, 2018. 2024-11-27\nfacefusion/facefusion: Industry leading face manipulation platform 2024-11-27\nyangchris11/samurai: Official repository of “SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory”\nalibaba/Tora: The official repository for paper “Tora: Trajectory-oriented Diffusion Transformer for Video Generation”\naigc-apps/CogVideoX-Fun: 📹 A more flexible CogVideoX that can generate videos at any resolution and creates videos from images.\naigc-apps/EasyAnimate: 📺 An End-to-End Solution for High-Resolution and Long Video Generation Based on Transformer Diffusion\nGitHub - HVision-NKU/StoryDiffusion: Create Magic Story!\nhpcaitech/Open-Sora: Open-Sora: Democratizing Efficient Video Production for All\nVision-CAIR/MiniGPT4-video\nhkchengrex/Cutie: [CVPR 2024 Highlight] Putting the Object Back Into Video Object Segmentation\nPicsart-AI-Research/StreamingT2V: StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text\naigc-apps/EasyAnimate: 📺 An End-to-End Solution for High-Resolution and Long Video Generation Based on Transformer Diffusion\nTencent/MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance\njianchang512/pyvideotrans: Translate the video from one language to another and add dubbing. 将视频从一种语言翻译为另一种语言，并支持api调用\nHillobar/Rope: GUI-focused roop\nGitHub - sczhou/CodeFormer: [NeurIPS 2022] Towards Robust Blind Face Restoration with Codebook Lookup Transformer\nHuanshere/VideoLingo: Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | Netflix级字幕切割、翻译、对齐、甚至加上配音，一键全自动视频搬运AI字幕组\njy0205/Pyramid-Flow: Code of Pyramidal Flow Matching for Efficient Video Generative Modeling\nVision-CAIR/LongVU\nDoubiiu/ToonCrafter: [SIGGRAPH Asia 2024, Journal Track] ToonCrafter: Generative Cartoon Interpolation\nVectorSpaceLab/Video-XL: 🔥🔥First-ever hour scale video understanding models\nanliyuan/Ultralight-Digital-Human: 一个超轻量级、可以在移动端实时运行的数字人模型\nantgroup/echomimic_v2: EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation\nZejun-Yang/AniPortrait: AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation\nfudan-generative-vision/hallo2: Hallo2: Long-Duration and High-Resolution Audio-driven Portrait Image Animation\nantgroup/echomimic: EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditioning\nLordLiang/DrawingSpinUp: (SIGGRAPH Asia 2024) This is the official PyTorch implementation of SIGGRAPH Asia 2024 paper: DrawingSpinUp: 3D Animation from Single Character Drawings\nHelloVision/HelloMeme: The official HelloMeme GitHub site\nKmcode1/SG-I2V: This is the official implementation of SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation.\nfacebookresearch/sapiens: High-resolution models for human tasks.\nAlonzoLeeeooo/StableV2V: The official implementation of the paper titled “StableV2V: Stablizing Shape Consistency in Video-to-Video Editing”.\ngenmoai/mochi: The best OSS video generation models\nTHUDM/CogVideo: text and image to video generation: CogVideoX (2024) and CogVideo (ICLR 2023)\nCyberAgentAILab/TANGO: Official implementation of the paper “TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio-Motion Embedding and Diffusion Interpolation”\nIDEA-Research/MotionCLR: [Arxiv 2024] MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms\nJi4chenLi/t2v-turbo: Code repository for T2V-Turbo and T2V-Turbo-v2\nLightricks/LTX-Video: Official repository for LTX-Video\nWebUI open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, …)\ncontinue-revolution/sd-webui-segment-anything: Segment Anything for Stable Diffusion WebUI\nlllyasviel/stable-diffusion-webui-forge\naigc-apps/sd-webui-EasyPhoto: 📷 EasyPhoto | Your Smart AI Photo Generator.\nLLM hiroi-sora/Umi-OCR: OCR software, free and offline. 开源、免费的离线OCR软件。支持截屏/批量导入图片，PDF文档识别，排除水印/页眉页脚，扫描/生成二维码。内置多国语言库。 2024-12-03\nSignificant-Gravitas/AutoGPT: AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters. 2024-12-03\nOpenBMB/ChatDev: Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration) 2024-12-03\nTHUDM/GLM-4-Voice: GLM-4-Voice | 端到端中英语音对话模型\noobabooga/text-generation-webui: A Gradio web UI for Large Language Models.\njanhq/jan: Jan is an open source alternative to ChatGPT that runs 100% offline on your computer. Multiple engine support (llama.cpp, TensorRT-LLM)\nollama/ollama: Get up and running with Llama 3.2, Mistral, Gemma 2, and other large language models.\nbinary-husky/gpt_academic: 为GPT/GLM等LLM大语言模型提供实用化交互接口，特别优化论文阅读/润色/写作体验，模块化设计，支持自定义快捷按钮\u0026函数插件，支持Python和C++等项目剖析\u0026自译解功能，PDF/LaTex论文翻译\u0026总结功能，支持并行问询多种LLM模型，支持chatglm3等本地模型。接入通义千问, deepseekcoder, 讯飞星火, 文心一言, llama2, rwkv, claude2, moss等。\nSillyTavern/SillyTavern: LLM Frontend for Power Users.\nmendableai/firecrawl: 🔥 Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API.\nInternLM/InternLM: Official release of InternLM2.5 base and chat models. 1M context support\n训练脚本 hiyouga/LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs (ACL 2024) 2024-12-02\nkohya-ss/sd-scripts\ncocktailpeanut/fluxgym: Dead simple FLUX LoRA training UI with LOW VRAM support\nkijai/ComfyUI-FluxTrainer\nReleases · bmaltais/kohya_ss\nAkegarasu/lora-scripts: LoRA \u0026 Dreambooth training scripts \u0026 GUI use kohya-ss’s trainer, for diffusion model.\nNerogar/OneTrainer: OneTrainer is a one-stop solution for all your stable diffusion training needs.\n图像设计 chengyou-jia/ChatGen 2024-12-02\nerwold/qwen2vl-flux 2024-11-27\nYuanshi9815/OminiControl: A minimal and universal controller for FLUX.1. 2024-11-27\nlllyasviel/sd-forge-layerdiffuse: [WIP] Layer Diffusion for WebUI (via Forge) 2024-11-27\nali-vilab/ACE: All-round Creator and Editor\nmit-han-lab/hart: HART: Efficient Visual Generation with Hybrid Autoregressive Transformer\nZhengPeng7/BiRefNet: [CAAI AIR'24] Bilateral Reference for High-Resolution Dichotomous Image Segmentation\nYangLing0818/IterComp: IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation\nxinsir6/ControlNetPlus: ControlNet++: All-in-one ControlNet for image generations and editing!\nKwai-Kolors/Kolors: Kolors Team\nXiaojiu-z/Stable-Hair: Stable-Hair: Real-World Hair Transfer via Diffusion Model\nyisol/IDM-VTON: [ECCV2024] IDM-VTON : Improving Diffusion Models for Authentic Virtual Try-on in the Wild\nbcmi/libcom: Image composition toolbox: everything you want to know about image composition or object insertion\nPixArt-alpha/PixArt-alpha: PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis\nblack-forest-labs/flux: Official inference repo for FLUX.1 models\nStability-AI/sd3.5\nlllyasviel/Omost: Your image is almost there!\ngligen/GLIGEN: Open-Set Grounded Text-to-Image Generation\nTencent/HunyuanDiT: Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding\nlllyasviel/IC-Light: More relighting!\ntencent-ailab/IP-Adapter: The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt.\npiddnad/DDColor: [ICCV 2023] Official implementation of “DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders”\ncumulo-autumn/StreamDiffusion: StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation\nToTheBeginning/PuLID: [NeurIPS 2024] Official code for PuLID: Pure and Lightning ID Customization via Contrastive Alignment\nKDE/krita: Krita is a free and open source cross-platform application that offers an end-to-end solution for creating digital art files from scratch built on the KDE and Qt frameworks.\nAcly/krita-ai-diffusion: Streamlined interface for generating images with AI in Krita. Inpaint and outpaint with optional text prompt, no tweaking required.\ninstantX-research/InstantID: InstantID: Zero-shot Identity-Preserving Generation in Seconds 🔥\njbilcke-hf/FacePoke: Select a portrait, click to move the head around (please use your own space / GPU!)\ncatcathh/UltraPixel: Implementation of UltraPixel: Advancing Ultra-High-Resolution Image Synthesis to New Peaks\nZeyi-Lin/HivisionIDPhotos: ⚡️HivisionIDPhotos: a lightweight and efficient AI ID photos tools. 一个轻量级的AI证件照制作算法。\nVectorSpaceLab/OmniGen: OmniGen: Unified Image Generation. https://arxiv.org/pdf/2409.11340\nshallowdream204/DreamClear: [NeurIPS 2024🔥] DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation\nNVlabs/consistory\ninstantX-research/Regional-Prompting-FLUX: Training-free Regional Prompting for Diffusion Transformers 🔥\nali-vilab/In-Context-LoRA: Official repository of In-Context LoRA for Diffusion Transformers\nmit-han-lab/nunchaku: SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models\nChenyangSi/FreeU: FreeU: Free Lunch in Diffusion U-Net (CVPR2024 Oral)\nmagic-quill/MagicQuill: Official Implementations for Paper - MagicQuill: An Intelligent Interactive Image Editing System\nNutlope/logocreator: A free + OSS logo generator powered by Flux on Together AI\nNVlabs/Sana: SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer\nJackAILab/ConsistentID: Customized ID Consistent for human\nDepthAnything/Depth-Anything-V2: [NeurIPS 2024] Depth Anything V2. A More Capable Foundation Model for Monocular Depth Estimation\ntryonlabs/FLUX.1-dev-LoRA-Outfit-Generator: FLUX.1-dev LoRA Outfit Generator can create an outfit by detailing the color, pattern, fit, style, material, and type.\n语音, 音乐 netease-youdao/EmotiVoice: EmotiVoice 😊: a Multi-Voice and Prompt-Controlled TTS Engine\nhaidog-yaqub/EzAudio: High-quality Text-to-Audio Generation with Efficient Diffusion Transformer\n2noise/ChatTTS: A generative speech model for daily dialogue.\nBytedanceSpeech/seed-tts-eval\nRVC-Project/Retrieval-based-Voice-Conversion-WebUI: Easily train a good VC model with voice data \u003c= 10 mins!\nGitHub - yxlllc/DDSP-SVC: Real-time end-to-end singing voice conversion system based on DDSP (Differentiable Digital Signal Processing)\nvoicepaw/so-vits-svc-fork: so-vits-svc fork with realtime support, improved interface and more features.\nGitHub - RVC-Boss/GPT-SoVITS: 1 min voice data can also be used to train a good TTS model! (few shot voice cloning)\nSWivid/F5-TTS: Official code for “F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching”\nmisya11p/amt-apc: AMT-APC: AMT-APC: Automatic Piano Cover by Fine-Tuning an Automatic Music Transcription Model\nWEIFENG2333/AsrTools: ✨ AsrTools: 智能语音转文字工具 | 高效批处理 | 用户友好界面 | 无需 GPU |支持 SRT/TXT 输出 | 让您的音频瞬间变成精确文字！\nopen-mmlab/Amphion: Amphion (/æmˈfaɪən/) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.\nfishaudio/fish-speech: Brand new TTS solution\n3D VAST-AI-Research/TripoSR 2024-11-27\nmicrosoft/MoGe: MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision\nHengyiWang/spann3r: 3D Reconstruction with Spatial Memory\nTencent/Hunyuan3D-1\nwenqsun/DimensionX: DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion\n文本处理 zyddnys/manga-image-translator: Translate manga/image 一键翻译各类图片内文字 https://cotrans.touhou.ai/\nchidiwilliams/buzz: Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI’s Whisper.\nAgentEra/Agently-Daily-News-Collector: An open-source LLM based automatically daily news collecting workflow showcase powered by Agently AI application development framework.\nLC044/WeChatMsg: 提取微信聊天记录，将其导出成HTML、Word、Excel文档永久保存，对聊天记录进行分析生成年度聊天报告，用聊天数据训练专属于个人的AI聊天助手\ngabrielchua/open-notebooklm: Convert any PDF into a podcast episode!\ngetomni-ai/zerox: Zero shot pdf OCR with gpt-4o-mini\nopendatalab/PDF-Extract-Kit: A Comprehensive Toolkit for High-Quality PDF Content Extraction\nNutlope/llama-ocr: Document to Markdown OCR library with Llama 3.2 vision\nopendatalab/MinerU: A high-quality tool for convert PDF to Markdown and JSON.一站式开源高质量数据提取工具，将PDF转换成Markdown和JSON格式。\n其他 showlab/ShowUI: Repository for ShowUI: One Vision-Language-Action Model for GUI Visual Agent 2024-12-02\nturboderp/exllamav2: A fast inference library for running LLMs locally on modern consumer-class GPUs 2024-12-02\ninstructor-ai/instructor: structured outputs for llms 2024-12-02\nComprehensive Guide to Prompting Techniques - Instructor 2024-12-02\nhuggingface/transformers.js: State-of-the-art Machine Learning for the web. Run 🤗 Transformers directly in your browser, with no need for a server! 2024-12-02\nUcas-HaoranWei/GOT-OCR2.0: Official code implementation of General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\ndeepseek-ai/DeepSeek-VL: DeepSeek-VL: Towards Real-World Vision-Language Understanding\ndynobo/normcap: OCR powered screen-capture tool to capture information instead of images\nmodelscope/DiffSynth-Studio: Enjoy the magic of Diffusion models!\nabi/screenshot-to-code: Drop in a screenshot and convert it to clean code (HTML/Tailwind/React/Vue)\nstackblitz/bolt.new: Prompt, run, edit, and deploy full-stack web applications\nlean-dojo/LeanCopilot: LLMs as Copilots for Theorem Proving in Lean\ngeekan/MetaGPT: 🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming\nprinceton-nlp/SWE-agent: [NeurIPS 2024] SWE-agent takes a GitHub issue and tries to automatically fix it, using GPT-4, or your LM of choice. It can also be employed for offensive cybersecurity or competitive coding challenges.\nOpenCodeInterpreter/OpenCodeInterpreter: OpenCodeInterpreter is a suite of open-source code generation systems aimed at bridging the gap between large language models and sophisticated proprietary systems like the GPT-4 Code Interpreter. It significantly enhances code generation capabilities by integrating execution and iterative refinement functionalities.\nIkaros-521/AI-Vtuber: AI Vtuber是一个由 【ChatterBot/ChatGPT/claude/langchain/chatglm/text-gen-webui/闻达/千问/kimi/ollama】 驱动的虚拟主播【Live2D/UE/xuniren】，可以在 【Bilibili/抖音/快手/微信视频号/拼多多/斗鱼/YouTube/twitch/TikTok】 直播中与观众实时互动 或 直接在本地进行聊天。它使用TTS技术【edge-tts/VITS/elevenlabs/bark/bert-vits2/睿声】生成回答并可以选择【so-vits-svc/DDSP-SVC】变声；指令协同SD画图。\nGitHub - 3b1b/manim: Animation engine for explanatory math videos\nGitHub - ManimCommunity/manim: A community-maintained Python framework for creating mathematical animations.\nGitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks\nGitHub - PeterH0323/Streamer-Sales: Streamer-Sales 销冠 —— 卖货主播大模型，一个能够根据给定的商品特点对商品进行解说并激发用户的购买意愿的卖货主播模型\nFujiwaraChoki/MoneyPrinter: Automate Creation of YouTube Shorts using MoviePy.\nprinceton-nlp/SWE-agent: SWE-agent takes a GitHub issue and tries to automatically fix it, using GPT-4. It solves 12.29% of bugs in the SWE-bench evaluation set (comparable to Devin) and take just 1.5 minutes to run (7x faster than Devin).\nharry0703/MoneyPrinterTurbo: 利用AI大模型，一键生成高清短视频 Generate short videos with one click using AI LLM.\nidootop/mi-gpt: 🏠 将小爱音箱接入 ChatGPT 和豆包，改造成你的专属语音助手。\nwan-h/awesome-digital-human-live2d: Awesome Digital Human\nopenai/swarm: Educational framework exploring ergonomic, lightweight multi-agent orchestration. Managed by OpenAI Solution team.\nmeta-llama/llama-recipes: Scripts for fine-tuning Meta Llama with composable FSDP \u0026 PEFT methods to cover single/multi-node GPUs. Supports default \u0026 custom datasets for applications such as summarization and Q\u0026A. Supporting a number of candid inference solutions such as HF TGI, VLLM for local or cloud deployment. Demo apps to showcase Meta Llama for WhatsApp \u0026 Messenger.\nHqWu-HITCS/Awesome-Chinese-LLM: 整理开源的中文大语言模型，以规模较小、可私有化部署、训练成本较低的模型为主，包括底座模型，垂直领域微调及应用，数据集与教程等。\nHannibal046/Awesome-LLM: Awesome-LLM: a curated list of Large Language Model\nexcalidraw/excalidraw: Virtual whiteboard for sketching hand-drawn like diagrams\nmeltylabs/melty: Chat first code editor. To download the packaged app:\ngpt-omni/mini-omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities。\n",
    "description": "",
    "tags": null,
    "title": "项目的 GitHub 主页",
    "uri": "/01_aigc_object/02_github/"
  },
  {
    "content": "项目的 Hugging Face Space TryOffDiff - a Hugging Face Space by rizavelioglu 2024-12-03\nQwen2vl Flux Mini Demo - a Hugging Face Space by Djrango 2024-12-02\nIC Light V2-Vary - a Hugging Face Space by lllyasviel 2024-12-02\nIllusionDiffusion - a Hugging Face Space by AP123 2024-12-02\nReplaceAnything - a Hugging Face Space by modelscope 2024-12-02\nQwQ-32B-Preview - a Hugging Face Space by Qwen 2024-12-02\nOminiControl - a Hugging Face Space by Yuanshi 2024-11-27\nACE-Chat - a Hugging Face Space by scepter-studio\nMoGe - a Hugging Face Space by Ruicheng\nEzAudio - a Hugging Face Space by OpenSound\nNaturalSpeech3 FACodec - a Hugging Face Space by amphion\nIDM VTON - a Hugging Face Space by yisol\nAnimateDiff-Lightning - a Hugging Face Space by ByteDance\nOmost - a Hugging Face Space by lllyasviel\nCLIP Interrogator - a Hugging Face Space by pharmapsychotic\nPyramid Flow - a Hugging Face Space by Pyramid-Flow\nJoy Caption Alpha Two - a Hugging Face Space by fancyfeast\nIC Light V2 - a Hugging Face Space by lllyasviel\nMaskGCT TTS Demo - a Hugging Face Space by amphion\nOmniGen - a Hugging Face Space by Shitao\nMotionCLR - a Hugging Face Space by EvanTHU\nSeedEdit-APP-V1.0 - a Hugging Face Space by ByteDance\nFramer - a Hugging Face Space by wwen1997\nBRIA RMBG 2.0 - a Hugging Face Space by briaai\nMinerU - a Hugging Face Space by opendatalab\nQwen Turbo 1M Demo - a Hugging Face Space by Qwen\nDimensionX - a Hugging Face Space by fffiloni\nPhotoMaker V2 - a Hugging Face Space by TencentARC\nOOTDiffusion - a Hugging Face Space by levihsu\nmoondream2 - a Hugging Face Space by vikhyatk\n",
    "description": "",
    "tags": null,
    "title": "Hugging Face Space",
    "uri": "/01_aigc_object/03_hugging/"
  },
  {
    "content": " ",
    "description": "",
    "tags": null,
    "title": "常用自定义节点",
    "uri": "/02_comfyui/04_common-nodes/"
  },
  {
    "content": "Discover and download free videos - Pixabay\nDanbooru: Anime Image Board\nDiscover the Best GPTs\nAI工具集 | 700+ AI工具集合官网，国内外AI工具集导航大全\nSupertools | Best AI Tools Guide\nAIGC导航 | 1500+全品类AIGC创作工具_探索更多可能！\n插画交流网站[pixiv]\nArtStation - Explore\nAIbase - 智能匹配最适合您的AI产品和网站\nNewsfeed - Sketchfab\nAI Model \u0026 API Providers Analysis | Artificial Analysis\nxAI\nGGAC数字艺术平台\nWeird Wonderful AI Art | ART of the future - now!\n",
    "description": "",
    "tags": null,
    "title": "AIGC 相关网站",
    "uri": "/01_aigc_object/04_web/"
  },
  {
    "content": "使用 diffusers 训练你自己的 ControlNet 🧨\nStable Diffusion QR Code 101\nE-Hentai/태그 - 나무위키\n魔咒百科词典\nSo-VITS-SVC 4.1 整合包完全指南\nStable Diffusion 3.5 Prompt Guide — Stability AI\n使用 ChatGPT 进行写作的学生指南 |开放人工智能 — A Student’s Guide to Writing with ChatGPT | OpenAI\nrichards199999/Thinking-Claude: Let your Claude able to think\nhesamsheikh/ml-retreat: Machine Learning Journal for Intermediate to Advanced Topics.\nMidjourney Documentation and User Guide\n归档(可以不用看) Resources for GAN Artists\nDisco Diffusion Portrait Study (by @enviraldesign) - Google 文档\nalibaba/animate-anything: Fine-Grained Open Domain Image Animation with Motion Guidance\nGitHub - prophesier/diff-svc: Singing Voice Conversion via diffusion model\nTencentARC/GFPGAN: GFPGAN aims at developing Practical Algorithms for Real-world Face Restoration.\nguide to installing disco v5+ locally on windows\nclip_interrogator.ipynb - Colaboratory\nA Traveler’s Guide to the Latent Space\nCoar’s Disco Diffusion Guide\nDisco Diffusion Illustrated Settings\nAi generative art tools\nAI绘画的关键词（群友们的画 ）\nArtist Studies by @remi_durant\nCLIP Prompt Engineering for Generative Art - matthewmcateer.me\n数据集-LAION-400-MILLION OPEN DATASET | LAION\n",
    "description": "",
    "tags": null,
    "title": "相关文档资料",
    "uri": "/01_aigc_object/05_doc/"
  },
  {
    "content": "等待中的项目是指只有论文或介绍, 还不能用的项目.\nFreditor 2024-12-03\nmayuelala/FollowYourClick: [arXiv 2024] Follow-Your-Click: This repo is the official implementation of “Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts” 2024-12-03\nAnchorCrafter 2024-12-02\nGenerative Omnimatte: Learning to Decompose Video into Layers 2024-12-02\nlehduong/OneDiffusion 2024-12-02\nLipDub AI | The most realistic AI lip sync and video translation 2024-12-02\nMyTimeMachine: Personalized Facial Age Transformation 2024-12-02\nBuffer Anytime: Zero-Shot Video Depth and Normal from Image Priors 2024-12-02\nMultiFoley 2024-12-02\nSonic: Shifting Focus to Global Audio Perception in Audio-driven Portrait Animation 2024-12-02\nlewandofskee/MobileMamba: Official implementation of `MobileMamba: Lightweight Multi-Receptive Visual Mamba Network.’ 2024-12-02\nBaking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation 2024-11-27\nFugatto, World’s Most Flexible Sound Machine, Debuts | NVIDIA Blog 2024-11-27\nFashion-VDM: Video Diffusion Model for Virtual Try-On\nInverse Painting: Reconstructing The Painting Process\n首页 |剧集主管 — Home | Showrunner\n悠船\nVideo Ocean视频大模型 - 人人皆导演\nPersonaTalk: Bring Attention to Your Persona in Visual Dubbing\nMarDini: Masked Auto-Regressive Diffusion for Video Generation at Scale – Meta AI Research\n炉米Lumi\nloopyavatar.github.io/?ref=aihub.cn\nGoogle Vids：在线视频创建和编辑器 | Google Vids谷歌工作区 — Google Vids: Online Video Creator and Editor | Google Workspace\nSkyReels\nURAvatar: Universal Relightable Gaussian Codec Avatars\nMikuDance\nAdd-it\nDanceFusion: A Spatio-Temporal Skeleton Diffusion Transformer for Audio-Driven Dance Motion Reconstruction.\nAnimateAnything\nMIMO\nInstructAvatar\n",
    "description": "",
    "tags": null,
    "title": "等待中的项目",
    "uri": "/01_aigc_object/06_wait/"
  },
  {
    "content": "公众号:\n视频号:\n抖音:\n快手:\n小红书:\nB 站:\n",
    "description": "",
    "tags": null,
    "title": "关注我",
    "uri": "/about/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/"
  }
]
