<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.101.0">
    <meta name="generator" content="Relearn 5.2.1+tip">
    <meta name="description" content="">
    <meta name="author" content="å´æ˜æ–‡">
    <title>é¡¹ç›®çš„ GitHub ä¸»é¡µ - aiart.website</title>
    <link href="/01_aigc_object/02_github/index.xml" rel="alternate" type="application/rss+xml" title="aiart.website"><link rel="icon" href="/favicon.png" type="image/png" />
    <!-- https://github.com/filamentgroup/loadCSS/blob/master/README.md#how-to-use -->
    <link href="/css/fontawesome-all.min.css?1734882723" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fontawesome-all.min.css?1734882723" rel="stylesheet"></noscript>
    <link href="/css/featherlight.min.css?1734882723" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/featherlight.min.css?1734882723" rel="stylesheet"></noscript>
    <link href="/css/auto-complete.css?1734882723" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/auto-complete.css?1734882723" rel="stylesheet"></noscript>
    <link href="/css/perfect-scrollbar.min.css?1734882723" rel="stylesheet">
    <link href="/css/nucleus.css?1734882723" rel="stylesheet">
    <link href="/css/fonts.css?1734882723" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fonts.css?1734882723" rel="stylesheet"></noscript>
    <link href="/css/theme.css?1734882723" rel="stylesheet">
    <link href="/css/theme-black.css?1734882723" rel="stylesheet" id="variant-style">
    <link href="/css/ie.css?1734882723" rel="stylesheet">
    <link href="/css/variant.css?1734882723" rel="stylesheet">
    <link href="/css/print.css?1734882723" rel="stylesheet" media="print">
    <script src="/js/variant.js?1734882723"></script>
    <script>
      // hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic:
      // https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72
      var index_url="/index.json";
      var root_url="/";
      var baseUri=root_url.replace(/\/$/, '');
      // translations
      window.T_Copy_to_clipboard = 'å¤åˆ¶åˆ°å‰ªè´´æ¿';
      window.T_Copied_to_clipboard = 'å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼';
      window.T_Copy_link_to_clipboard = 'å°†é“¾æ¥å¤åˆ¶åˆ°å‰ªè´´æ¿';
      window.T_Link_copied_to_clipboard = 'é“¾æ¥å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼';
      // some further base stuff
      var baseUriFull='/';
      window.variants && variants.init( [ 'black' ] );
    </script>
    <script src="/js/jquery.min.js?1734882723" defer></script>
  </head>
  <body class="mobile-support html" data-url="/01_aigc_object/02_github/">
    <div id="body" class="default-animation">
      <div id="sidebar-overlay"></div>
      <div id="toc-overlay"></div>
      <nav id="topbar" class="highlightable">
        <div>
          <div id="breadcrumbs">
            <span id="sidebar-toggle-span">
              <a href="#" id="sidebar-toggle" title='å¯¼èˆª (CTRL+ALT+m)'><i class="fas fa-bars fa-fw"></i></a>
            </span>
            <span id="toc-menu" title='ç›®å½• (CTRL+ALT+t)'><i class="fas fa-list-alt fa-fw"></i></span>
            <ol class="links" itemscope itemtype="http://schema.org/BreadcrumbList">
              <meta itemprop="itemListOrder" content="Descending" />
              <li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><meta itemprop="position" content="3" /><a itemprop="item" href="/"><span itemprop="name">aiart.website</span></a> > </li>
              <li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><meta itemprop="position" content="2" /><a itemprop="item" href="/01_aigc_object/"><span itemprop="name">ACGC é¡¹ç›®èµ„æºé›†</span></a> > </li>
              <li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><meta itemprop="position" content="1" /><a itemprop="item" href="/01_aigc_object/02_github/" aria-disabled="true"><span itemprop="name">é¡¹ç›®çš„ GitHub ä¸»é¡µ</span></a></li>
            </ol>
          </div>
          <div class="default-animation progress">
            <div class="wrapper">
<nav id="TableOfContents">
  <ul>
    <li><a href="#è§†é¢‘">è§†é¢‘</a></li>
    <li><a href="#webui">WebUI</a></li>
    <li><a href="#llm">LLM</a></li>
    <li><a href="#è®­ç»ƒè„šæœ¬">è®­ç»ƒè„šæœ¬</a></li>
    <li><a href="#å›¾åƒè®¾è®¡">å›¾åƒè®¾è®¡</a></li>
    <li><a href="#è¯­éŸ³-éŸ³ä¹">è¯­éŸ³, éŸ³ä¹</a></li>
    <li><a href="#3d">3D</a></li>
    <li><a href="#æ–‡æœ¬å¤„ç†">æ–‡æœ¬å¤„ç†</a></li>
    <li><a href="#å…¶ä»–">å…¶ä»–</a></li>
  </ul>
</nav>
            </div>
          </div>
        </div>
      </nav>
      <main id="body-inner" class="highlightable default" tabindex="-1">
        <div class="flex-block-wrapper">
          <div id="head-tags">
          </div>
          <article class="default">
<h1>é¡¹ç›®çš„ GitHub ä¸»é¡µ</h1>

<h2 id="è§†é¢‘">è§†é¢‘</h2>
<p><a href="https://github.com/prs-eth/rollingdepth">prs-eth/RollingDepth: Video Depth without Video Models</a> 2024-12-03</p>
<p><a href="https://github.com/Tencent/HunyuanVideo">Tencent/HunyuanVideo</a> 2024-12-03</p>
<p><a href="https://github.com/hmrishavbandy/FlipSketch">hmrishavbandy/FlipSketch: FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations</a> 2024-12-02</p>
<p><a href="https://github.com/KwaiVGI/LivePortrait">KwaiVGI/LivePortrait: Bring portraits to life!</a> 2024-12-02</p>
<p><a href="https://github.com/C0untFloyd/roop-unleashed">C0untFloyd/roop-unleashed: Evolved Fork of roop with Web Server and lots of additions</a> 2024-12-02</p>
<p><a href="https://github.com/jdh-algo/JoyVASA">jdh-algo/JoyVASA</a> 2024-12-02</p>
<p><a href="https://github.com/PKU-YuanGroup/ConsisID">PKU-YuanGroup/ConsisID: Identity-Preserving Text-to-Video Generation by Frequency Decomposition</a> 2024-12-02</p>
<p><a href="https://github.com/rhymes-ai/Allegro">rhymes-ai/Allegro: Allegro is a powerful text-to-video model that generates high-quality videos up to 6 seconds at 15 FPS and 720p resolution from simple text input.</a> 2024-12-02</p>
<p><a href="https://github.com/k4yt3x/video2x">k4yt3x/video2x: A machine learning-based lossless video super resolution framework. Est. Hack the Valley II, 2018.</a> 2024-11-27</p>
<p><a href="https://github.com/facefusion/facefusion">facefusion/facefusion: Industry leading face manipulation platform</a> 2024-11-27</p>
<p><a href="https://github.com/yangchris11/samurai">yangchris11/samurai: Official repository of &ldquo;SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory&rdquo;</a></p>
<p><a href="https://github.com/alibaba/Tora?tab=readme-ov-file">alibaba/Tora: The official repository for paper &ldquo;Tora: Trajectory-oriented Diffusion Transformer for Video Generation&rdquo;</a></p>
<p><a href="https://github.com/aigc-apps/CogVideoX-Fun">aigc-apps/CogVideoX-Fun: ğŸ“¹ A more flexible CogVideoX that can generate videos at any resolution and creates videos from images.</a></p>
<p><a href="https://github.com/aigc-apps/EasyAnimate">aigc-apps/EasyAnimate: ğŸ“º An End-to-End Solution for High-Resolution and Long Video Generation Based on Transformer Diffusion</a></p>
<p><a href="https://github.com/HVision-NKU/StoryDiffusion">GitHub - HVision-NKU/StoryDiffusion: Create Magic Story!</a></p>
<p><a href="https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file">hpcaitech/Open-Sora: Open-Sora: Democratizing Efficient Video Production for All</a></p>
<p><a href="https://github.com/Vision-CAIR/MiniGPT4-video">Vision-CAIR/MiniGPT4-video</a></p>
<p><a href="https://github.com/hkchengrex/Cutie">hkchengrex/Cutie: [CVPR 2024 Highlight] Putting the Object Back Into Video Object Segmentation</a></p>
<p><a href="https://github.com/Picsart-AI-Research/StreamingT2V">Picsart-AI-Research/StreamingT2V: StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</a></p>
<p><a href="https://github.com/aigc-apps/EasyAnimate/">aigc-apps/EasyAnimate: ğŸ“º An End-to-End Solution for High-Resolution and Long Video Generation Based on Transformer Diffusion</a></p>
<p><a href="https://github.com/Tencent/MimicMotion">Tencent/MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance</a></p>
<p><a href="https://github.com/jianchang512/pyvideotrans">jianchang512/pyvideotrans: Translate the video from one language to another and add dubbing. å°†è§†é¢‘ä»ä¸€ç§è¯­è¨€ç¿»è¯‘ä¸ºå¦ä¸€ç§è¯­è¨€ï¼Œå¹¶æ”¯æŒapiè°ƒç”¨</a></p>
<p><a href="https://github.com/Hillobar/Rope">Hillobar/Rope: GUI-focused roop</a></p>
<p><a href="https://github.com/sczhou/CodeFormer">GitHub - sczhou/CodeFormer: [NeurIPS 2022] Towards Robust Blind Face Restoration with Codebook Lookup Transformer</a></p>
<p><a href="https://github.com/Huanshere/VideoLingo">Huanshere/VideoLingo: Netflix-level subtitle cutting, translation, alignment, and even dubbing - one-click fully automated AI video subtitle team | Netflixçº§å­—å¹•åˆ‡å‰²ã€ç¿»è¯‘ã€å¯¹é½ã€ç”šè‡³åŠ ä¸Šé…éŸ³ï¼Œä¸€é”®å…¨è‡ªåŠ¨è§†é¢‘æ¬è¿AIå­—å¹•ç»„</a></p>
<p><a href="https://github.com/jy0205/Pyramid-Flow">jy0205/Pyramid-Flow: Code of Pyramidal Flow Matching for Efficient Video Generative Modeling</a></p>
<p><a href="https://github.com/Vision-CAIR/LongVU">Vision-CAIR/LongVU</a></p>
<p><a href="https://github.com/Doubiiu/ToonCrafter">Doubiiu/ToonCrafter: [SIGGRAPH Asia 2024, Journal Track] ToonCrafter: Generative Cartoon Interpolation</a></p>
<p><a href="https://github.com/VectorSpaceLab/Video-XL">VectorSpaceLab/Video-XL: ğŸ”¥ğŸ”¥First-ever hour scale video understanding models</a></p>
<p><a href="https://github.com/anliyuan/Ultralight-Digital-Human">anliyuan/Ultralight-Digital-Human: ä¸€ä¸ªè¶…è½»é‡çº§ã€å¯ä»¥åœ¨ç§»åŠ¨ç«¯å®æ—¶è¿è¡Œçš„æ•°å­—äººæ¨¡å‹</a></p>
<p><a href="https://github.com/antgroup/echomimic_v2">antgroup/echomimic_v2: EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation</a></p>
<p><a href="https://github.com/Zejun-Yang/AniPortrait">Zejun-Yang/AniPortrait: AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</a></p>
<p><a href="https://github.com/fudan-generative-vision/hallo2">fudan-generative-vision/hallo2: Hallo2: Long-Duration and High-Resolution Audio-driven Portrait Image Animation</a></p>
<p><a href="https://github.com/antgroup/echomimic">antgroup/echomimic: EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditioning</a></p>
<p><a href="https://github.com/LordLiang/DrawingSpinUp">LordLiang/DrawingSpinUp: (SIGGRAPH Asia 2024) This is the official PyTorch implementation of SIGGRAPH Asia 2024 paper: DrawingSpinUp: 3D Animation from Single Character Drawings</a></p>
<p><a href="https://github.com/HelloVision/HelloMeme">HelloVision/HelloMeme: The official HelloMeme GitHub site</a></p>
<p><a href="https://github.com/Kmcode1/SG-I2V">Kmcode1/SG-I2V: This is the official implementation of SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation.</a></p>
<p><a href="https://github.com/facebookresearch/sapiens">facebookresearch/sapiens: High-resolution models for human tasks.</a></p>
<p><a href="https://github.com/AlonzoLeeeooo/StableV2V">AlonzoLeeeooo/StableV2V: The official implementation of the paper titled &ldquo;StableV2V: Stablizing Shape Consistency in Video-to-Video Editing&rdquo;.</a></p>
<p><a href="https://github.com/genmoai/mochi">genmoai/mochi: The best OSS video generation models</a></p>
<p><a href="https://github.com/THUDM/CogVideo?tab=readme-ov-file">THUDM/CogVideo: text and image to video generation: CogVideoX (2024) and CogVideo (ICLR 2023)</a></p>
<p><a href="https://github.com/CyberAgentAILab/TANGO">CyberAgentAILab/TANGO: Official implementation of the paper &ldquo;TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio-Motion Embedding and Diffusion Interpolation&rdquo;</a></p>
<p><a href="https://github.com/IDEA-Research/MotionCLR">IDEA-Research/MotionCLR: [Arxiv 2024] MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms</a></p>
<p><a href="https://github.com/Ji4chenLi/t2v-turbo">Ji4chenLi/t2v-turbo: Code repository for T2V-Turbo and T2V-Turbo-v2</a></p>
<p><a href="https://github.com/Lightricks/LTX-Video">Lightricks/LTX-Video: Official repository for LTX-Video</a></p>
<h2 id="webui">WebUI</h2>
<p><a href="https://github.com/open-webui/open-webui">open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, &hellip;)</a></p>
<p><a href="https://github.com/continue-revolution/sd-webui-segment-anything">continue-revolution/sd-webui-segment-anything: Segment Anything for Stable Diffusion WebUI</a></p>
<p><a href="https://github.com/lllyasviel/stable-diffusion-webui-forge">lllyasviel/stable-diffusion-webui-forge</a></p>
<p><a href="https://github.com/aigc-apps/sd-webui-EasyPhoto">aigc-apps/sd-webui-EasyPhoto: ğŸ“· EasyPhoto | Your Smart AI Photo Generator.</a></p>
<h2 id="llm">LLM</h2>
<p><a href="https://github.com/hiroi-sora/Umi-OCR">hiroi-sora/Umi-OCR: OCR software, free and offline. å¼€æºã€å…è´¹çš„ç¦»çº¿OCRè½¯ä»¶ã€‚æ”¯æŒæˆªå±/æ‰¹é‡å¯¼å…¥å›¾ç‰‡ï¼ŒPDFæ–‡æ¡£è¯†åˆ«ï¼Œæ’é™¤æ°´å°/é¡µçœ‰é¡µè„šï¼Œæ‰«æ/ç”ŸæˆäºŒç»´ç ã€‚å†…ç½®å¤šå›½è¯­è¨€åº“ã€‚</a> 2024-12-03</p>
<p><a href="https://github.com/Significant-Gravitas/AutoGPT">Significant-Gravitas/AutoGPT: AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.</a> 2024-12-03</p>
<p><a href="https://github.com/OpenBMB/ChatDev">OpenBMB/ChatDev: Create Customized Software using Natural Language Idea (through LLM-powered Multi-Agent Collaboration)</a> 2024-12-03</p>
<p><a href="https://github.com/THUDM/GLM-4-Voice">THUDM/GLM-4-Voice: GLM-4-Voice | ç«¯åˆ°ç«¯ä¸­è‹±è¯­éŸ³å¯¹è¯æ¨¡å‹</a></p>
<p><a href="https://github.com/oobabooga/text-generation-webui">oobabooga/text-generation-webui: A Gradio web UI for Large Language Models.</a></p>
<p><a href="https://github.com/janhq/jan">janhq/jan: Jan is an open source alternative to ChatGPT that runs 100% offline on your computer. Multiple engine support (llama.cpp, TensorRT-LLM)</a></p>
<p><a href="https://github.com/ollama/ollama">ollama/ollama: Get up and running with Llama 3.2, Mistral, Gemma 2, and other large language models.</a></p>
<p><a href="https://github.com/binary-husky/gpt_academic">binary-husky/gpt_academic: ä¸ºGPT/GLMç­‰LLMå¤§è¯­è¨€æ¨¡å‹æä¾›å®ç”¨åŒ–äº¤äº’æ¥å£ï¼Œç‰¹åˆ«ä¼˜åŒ–è®ºæ–‡é˜…è¯»/æ¶¦è‰²/å†™ä½œä½“éªŒï¼Œæ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒè‡ªå®šä¹‰å¿«æ·æŒ‰é’®&amp;å‡½æ•°æ’ä»¶ï¼Œæ”¯æŒPythonå’ŒC++ç­‰é¡¹ç›®å‰–æ&amp;è‡ªè¯‘è§£åŠŸèƒ½ï¼ŒPDF/LaTexè®ºæ–‡ç¿»è¯‘&amp;æ€»ç»“åŠŸèƒ½ï¼Œæ”¯æŒå¹¶è¡Œé—®è¯¢å¤šç§LLMæ¨¡å‹ï¼Œæ”¯æŒchatglm3ç­‰æœ¬åœ°æ¨¡å‹ã€‚æ¥å…¥é€šä¹‰åƒé—®, deepseekcoder, è®¯é£æ˜Ÿç«, æ–‡å¿ƒä¸€è¨€, llama2, rwkv, claude2, mossç­‰ã€‚</a></p>
<p><a href="https://github.com/SillyTavern/SillyTavern">SillyTavern/SillyTavern: LLM Frontend for Power Users.</a></p>
<p><a href="https://github.com/mendableai/firecrawl">mendableai/firecrawl: ğŸ”¥ Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API.</a></p>
<p><a href="https://github.com/InternLM/InternLM">InternLM/InternLM: Official release of InternLM2.5 base and chat models. 1M context support</a></p>
<h2 id="è®­ç»ƒè„šæœ¬">è®­ç»ƒè„šæœ¬</h2>
<p><a href="https://github.com/hiyouga/LLaMA-Factory">hiyouga/LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs (ACL 2024)</a> 2024-12-02</p>
<p><a href="https://github.com/kohya-ss/sd-scripts">kohya-ss/sd-scripts</a></p>
<p><a href="https://github.com/cocktailpeanut/fluxgym">cocktailpeanut/fluxgym: Dead simple FLUX LoRA training UI with LOW VRAM support</a></p>
<p><a href="https://github.com/kijai/ComfyUI-FluxTrainer">kijai/ComfyUI-FluxTrainer</a></p>
<p><a href="https://github.com/bmaltais/kohya_ss">Releases Â· bmaltais/kohya_ss</a></p>
<p><a href="https://github.com/Akegarasu/lora-scripts">Akegarasu/lora-scripts: LoRA &amp; Dreambooth training scripts &amp; GUI use kohya-ss&rsquo;s trainer, for diffusion model.</a></p>
<p><a href="https://github.com/Nerogar/OneTrainer">Nerogar/OneTrainer: OneTrainer is a one-stop solution for all your stable diffusion training needs.</a></p>
<h2 id="å›¾åƒè®¾è®¡">å›¾åƒè®¾è®¡</h2>
<p><a href="https://github.com/chengyou-jia/ChatGen?tab=readme-ov-file">chengyou-jia/ChatGen</a> 2024-12-02</p>
<p><a href="https://github.com/erwold/qwen2vl-flux">erwold/qwen2vl-flux</a> 2024-11-27</p>
<p><a href="https://github.com/Yuanshi9815/OminiControl">Yuanshi9815/OminiControl: A minimal and universal controller for FLUX.1.</a> 2024-11-27</p>
<p><a href="https://github.com/lllyasviel/sd-forge-layerdiffuse">lllyasviel/sd-forge-layerdiffuse: [WIP] Layer Diffusion for WebUI (via Forge)</a> 2024-11-27</p>
<p><a href="https://github.com/ali-vilab/ACE">ali-vilab/ACE: All-round Creator and Editor</a></p>
<p><a href="https://github.com/mit-han-lab/hart">mit-han-lab/hart: HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</a></p>
<p><a href="https://github.com/ZhengPeng7/BiRefNet">ZhengPeng7/BiRefNet: [CAAI AIR'24] Bilateral Reference for High-Resolution Dichotomous Image Segmentation</a></p>
<p><a href="https://github.com/YangLing0818/IterComp">YangLing0818/IterComp: IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation</a></p>
<p><a href="https://github.com/xinsir6/ControlNetPlus">xinsir6/ControlNetPlus: ControlNet++: All-in-one ControlNet for image generations and editing!</a></p>
<p><a href="https://github.com/Kwai-Kolors/Kolors">Kwai-Kolors/Kolors: Kolors Team</a></p>
<p><a href="https://github.com/Xiaojiu-z/Stable-Hair">Xiaojiu-z/Stable-Hair: Stable-Hair: Real-World Hair Transfer via Diffusion Model</a></p>
<p><a href="https://github.com/yisol/IDM-VTON">yisol/IDM-VTON: [ECCV2024] IDM-VTON : Improving Diffusion Models for Authentic Virtual Try-on in the Wild</a></p>
<p><a href="https://github.com/bcmi/libcom">bcmi/libcom: Image composition toolbox: everything you want to know about image composition or object insertion</a></p>
<p><a href="https://github.com/PixArt-alpha/PixArt-alpha">PixArt-alpha/PixArt-alpha: PixArt-Î±: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</a></p>
<p><a href="https://github.com/black-forest-labs/flux">black-forest-labs/flux: Official inference repo for FLUX.1 models</a></p>
<p><a href="https://github.com/Stability-AI/sd3.5">Stability-AI/sd3.5</a></p>
<p><a href="https://github.com/lllyasviel/Omost">lllyasviel/Omost: Your image is almost there!</a></p>
<p><a href="https://github.com/gligen/GLIGEN">gligen/GLIGEN: Open-Set Grounded Text-to-Image Generation</a></p>
<p><a href="https://github.com/Tencent/HunyuanDiT">Tencent/HunyuanDiT: Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding</a></p>
<p><a href="https://github.com/lllyasviel/IC-Light">lllyasviel/IC-Light: More relighting!</a></p>
<p><a href="https://github.com/tencent-ailab/IP-Adapter">tencent-ailab/IP-Adapter: The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt.</a></p>
<p><a href="https://github.com/piddnad/DDColor">piddnad/DDColor: [ICCV 2023] Official implementation of &ldquo;DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders&rdquo;</a></p>
<p><a href="https://github.com/cumulo-autumn/StreamDiffusion">cumulo-autumn/StreamDiffusion: StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation</a></p>
<p><a href="https://github.com/ToTheBeginning/PuLID">ToTheBeginning/PuLID: [NeurIPS 2024] Official code for PuLID: Pure and Lightning ID Customization via Contrastive Alignment</a></p>
<p><a href="https://github.com/KDE/krita">KDE/krita: Krita is a free and open source cross-platform application that offers an end-to-end solution for creating digital art files from scratch built on the KDE and Qt frameworks.</a></p>
<p><a href="https://github.com/Acly/krita-ai-diffusion">Acly/krita-ai-diffusion: Streamlined interface for generating images with AI in Krita. Inpaint and outpaint with optional text prompt, no tweaking required.</a></p>
<p><a href="https://github.com/instantX-research/InstantID">instantX-research/InstantID: InstantID: Zero-shot Identity-Preserving Generation in Seconds ğŸ”¥</a></p>
<p><a href="https://github.com/jbilcke-hf/FacePoke">jbilcke-hf/FacePoke: Select a portrait, click to move the head around (please use your own space / GPU!)</a></p>
<p><a href="https://github.com/catcathh/UltraPixel">catcathh/UltraPixel: Implementation of UltraPixel: Advancing Ultra-High-Resolution Image Synthesis to New Peaks</a></p>
<p><a href="https://github.com/Zeyi-Lin/HivisionIDPhotos">Zeyi-Lin/HivisionIDPhotos: âš¡ï¸HivisionIDPhotos: a lightweight and efficient AI ID photos tools. ä¸€ä¸ªè½»é‡çº§çš„AIè¯ä»¶ç…§åˆ¶ä½œç®—æ³•ã€‚</a></p>
<p><a href="https://github.com/VectorSpaceLab/OmniGen">VectorSpaceLab/OmniGen: OmniGen: Unified Image Generation. https://arxiv.org/pdf/2409.11340</a></p>
<p><a href="https://github.com/shallowdream204/DreamClear">shallowdream204/DreamClear: [NeurIPS 2024ğŸ”¥] DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation</a></p>
<p><a href="https://github.com/NVlabs/consistory">NVlabs/consistory</a></p>
<p><a href="https://github.com/instantX-research/Regional-Prompting-FLUX">instantX-research/Regional-Prompting-FLUX: Training-free Regional Prompting for Diffusion Transformers ğŸ”¥</a></p>
<p><a href="https://github.com/ali-vilab/In-Context-LoRA">ali-vilab/In-Context-LoRA: Official repository of In-Context LoRA for Diffusion Transformers</a></p>
<p><a href="https://github.com/mit-han-lab/nunchaku">mit-han-lab/nunchaku: SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</a></p>
<p><a href="https://github.com/ChenyangSi/FreeU">ChenyangSi/FreeU: FreeU: Free Lunch in Diffusion U-Net (CVPR2024 Oral)</a></p>
<p><a href="https://github.com/magic-quill/MagicQuill">magic-quill/MagicQuill: Official Implementations for Paper - MagicQuill: An Intelligent Interactive Image Editing System</a></p>
<p><a href="https://github.com/Nutlope/logocreator">Nutlope/logocreator: A free + OSS logo generator powered by Flux on Together AI</a></p>
<p><a href="https://github.com/NVlabs/Sana">NVlabs/Sana: SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer</a></p>
<p><a href="https://github.com/JackAILab/ConsistentID">JackAILab/ConsistentID: Customized ID Consistent for human</a></p>
<p><a href="https://github.com/DepthAnything/Depth-Anything-V2">DepthAnything/Depth-Anything-V2: [NeurIPS 2024] Depth Anything V2. A More Capable Foundation Model for Monocular Depth Estimation</a></p>
<p><a href="https://github.com/tryonlabs/FLUX.1-dev-LoRA-Outfit-Generator">tryonlabs/FLUX.1-dev-LoRA-Outfit-Generator: FLUX.1-dev LoRA Outfit Generator can create an outfit by detailing the color, pattern, fit, style, material, and type.</a></p>
<h2 id="è¯­éŸ³-éŸ³ä¹">è¯­éŸ³, éŸ³ä¹</h2>
<p><a href="https://github.com/netease-youdao/EmotiVoice?tab=readme-ov-file">netease-youdao/EmotiVoice: EmotiVoice ğŸ˜Š: a Multi-Voice and Prompt-Controlled TTS Engine</a></p>
<p><a href="https://github.com/haidog-yaqub/EzAudio">haidog-yaqub/EzAudio: High-quality Text-to-Audio Generation with Efficient Diffusion Transformer</a></p>
<p><a href="https://github.com/2noise/ChatTTS">2noise/ChatTTS: A generative speech model for daily dialogue.</a></p>
<p><a href="https://github.com/BytedanceSpeech/seed-tts-eval">BytedanceSpeech/seed-tts-eval</a></p>
<p><a href="https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI">RVC-Project/Retrieval-based-Voice-Conversion-WebUI: Easily train a good VC model with voice data &lt;= 10 mins!</a></p>
<p><a href="https://github.com/yxlllc/DDSP-SVC">GitHub - yxlllc/DDSP-SVC: Real-time end-to-end singing voice conversion system based on DDSP (Differentiable Digital Signal Processing)</a></p>
<p><a href="https://github.com/voicepaw/so-vits-svc-fork">voicepaw/so-vits-svc-fork: so-vits-svc fork with realtime support, improved interface and more features.</a></p>
<p><a href="https://github.com/RVC-Boss/GPT-SoVITS">GitHub - RVC-Boss/GPT-SoVITS: 1 min voice data can also be used to train a good TTS model! (few shot voice cloning)</a></p>
<p><a href="https://github.com/SWivid/F5-TTS">SWivid/F5-TTS: Official code for &ldquo;F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching&rdquo;</a></p>
<p><a href="https://github.com/misya11p/amt-apc">misya11p/amt-apc: AMT-APC: AMT-APC: Automatic Piano Cover by Fine-Tuning an Automatic Music Transcription Model</a></p>
<p><a href="https://github.com/WEIFENG2333/AsrTools">WEIFENG2333/AsrTools: âœ¨ AsrTools: æ™ºèƒ½è¯­éŸ³è½¬æ–‡å­—å·¥å…· | é«˜æ•ˆæ‰¹å¤„ç† | ç”¨æˆ·å‹å¥½ç•Œé¢ | æ— éœ€ GPU |æ”¯æŒ SRT/TXT è¾“å‡º | è®©æ‚¨çš„éŸ³é¢‘ç¬é—´å˜æˆç²¾ç¡®æ–‡å­—ï¼</a></p>
<p><a href="https://github.com/open-mmlab/Amphion">open-mmlab/Amphion: Amphion (/Ã¦mËˆfaÉªÉ™n/) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.</a></p>
<p><a href="https://github.com/fishaudio/fish-speech">fishaudio/fish-speech: Brand new TTS solution</a></p>
<h2 id="3d">3D</h2>
<p><a href="https://github.com/VAST-AI-Research/TripoSR">VAST-AI-Research/TripoSR</a> 2024-11-27</p>
<p><a href="https://github.com/microsoft/MoGe?tab=readme-ov-file">microsoft/MoGe: MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision</a></p>
<p><a href="https://github.com/HengyiWang/spann3r">HengyiWang/spann3r: 3D Reconstruction with Spatial Memory</a></p>
<p><a href="https://github.com/Tencent/Hunyuan3D-1">Tencent/Hunyuan3D-1</a></p>
<p><a href="https://github.com/wenqsun/DimensionX">wenqsun/DimensionX: DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion</a></p>
<h2 id="æ–‡æœ¬å¤„ç†">æ–‡æœ¬å¤„ç†</h2>
<p><a href="https://github.com/zyddnys/manga-image-translator">zyddnys/manga-image-translator: Translate manga/image ä¸€é”®ç¿»è¯‘å„ç±»å›¾ç‰‡å†…æ–‡å­— https://cotrans.touhou.ai/</a></p>
<p><a href="https://github.com/chidiwilliams/buzz">chidiwilliams/buzz: Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI&rsquo;s Whisper.</a></p>
<p><a href="https://github.com/AgentEra/Agently-Daily-News-Collector">AgentEra/Agently-Daily-News-Collector: An open-source LLM based automatically daily news collecting workflow showcase powered by Agently AI application development framework.</a></p>
<p><a href="https://github.com/LC044/WeChatMsg">LC044/WeChatMsg: æå–å¾®ä¿¡èŠå¤©è®°å½•ï¼Œå°†å…¶å¯¼å‡ºæˆHTMLã€Wordã€Excelæ–‡æ¡£æ°¸ä¹…ä¿å­˜ï¼Œå¯¹èŠå¤©è®°å½•è¿›è¡Œåˆ†æç”Ÿæˆå¹´åº¦èŠå¤©æŠ¥å‘Šï¼Œç”¨èŠå¤©æ•°æ®è®­ç»ƒä¸“å±äºä¸ªäººçš„AIèŠå¤©åŠ©æ‰‹</a></p>
<p><a href="https://github.com/gabrielchua/open-notebooklm">gabrielchua/open-notebooklm: Convert any PDF into a podcast episode!</a></p>
<p><a href="https://github.com/getomni-ai/zerox">getomni-ai/zerox: Zero shot pdf OCR with gpt-4o-mini</a></p>
<p><a href="https://github.com/opendatalab/PDF-Extract-Kit">opendatalab/PDF-Extract-Kit: A Comprehensive Toolkit for High-Quality PDF Content Extraction</a></p>
<p><a href="https://github.com/Nutlope/llama-ocr">Nutlope/llama-ocr: Document to Markdown OCR library with Llama 3.2 vision</a></p>
<p><a href="https://github.com/opendatalab/MinerU">opendatalab/MinerU: A high-quality tool for convert PDF to Markdown and JSON.ä¸€ç«™å¼å¼€æºé«˜è´¨é‡æ•°æ®æå–å·¥å…·ï¼Œå°†PDFè½¬æ¢æˆMarkdownå’ŒJSONæ ¼å¼ã€‚</a></p>
<h2 id="å…¶ä»–">å…¶ä»–</h2>
<p><a href="https://github.com/showlab/ShowUI">showlab/ShowUI: Repository for ShowUI: One Vision-Language-Action Model for GUI Visual Agent</a> 2024-12-02</p>
<p><a href="https://github.com/turboderp/exllamav2">turboderp/exllamav2: A fast inference library for running LLMs locally on modern consumer-class GPUs</a> 2024-12-02</p>
<p><a href="https://github.com/instructor-ai/instructor">instructor-ai/instructor: structured outputs for llms</a> 2024-12-02</p>
<p><a href="https://python.useinstructor.com/prompting/">Comprehensive Guide to Prompting Techniques - Instructor</a> 2024-12-02</p>
<p><a href="https://github.com/huggingface/transformers.js">huggingface/transformers.js: State-of-the-art Machine Learning for the web. Run ğŸ¤— Transformers directly in your browser, with no need for a server!</a> 2024-12-02</p>
<p><a href="https://github.com/Ucas-HaoranWei/GOT-OCR2.0">Ucas-HaoranWei/GOT-OCR2.0: Official code implementation of General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model</a></p>
<p><a href="https://github.com/deepseek-ai/DeepSeek-VL">deepseek-ai/DeepSeek-VL: DeepSeek-VL: Towards Real-World Vision-Language Understanding</a></p>
<p><a href="https://github.com/dynobo/normcap">dynobo/normcap: OCR powered screen-capture tool to capture information instead of images</a></p>
<p><a href="https://github.com/modelscope/DiffSynth-Studio?tab=readme-ov-file#usage-in-webui">modelscope/DiffSynth-Studio: Enjoy the magic of Diffusion models!</a></p>
<p><a href="https://github.com/abi/screenshot-to-code">abi/screenshot-to-code: Drop in a screenshot and convert it to clean code (HTML/Tailwind/React/Vue)</a></p>
<p><a href="https://github.com/stackblitz/bolt.new">stackblitz/bolt.new: Prompt, run, edit, and deploy full-stack web applications</a></p>
<p><a href="https://github.com/lean-dojo/LeanCopilot">lean-dojo/LeanCopilot: LLMs as Copilots for Theorem Proving in Lean</a></p>
<p><a href="https://github.com/geekan/MetaGPT">geekan/MetaGPT: ğŸŒŸ The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming</a></p>
<p><a href="https://github.com/princeton-nlp/SWE-agent">princeton-nlp/SWE-agent: [NeurIPS 2024] SWE-agent takes a GitHub issue and tries to automatically fix it, using GPT-4, or your LM of choice. It can also be employed for offensive cybersecurity or competitive coding challenges.</a></p>
<p><a href="https://github.com/OpenCodeInterpreter/OpenCodeInterpreter">OpenCodeInterpreter/OpenCodeInterpreter: OpenCodeInterpreter is a suite of open-source code generation systems aimed at bridging the gap between large language models and sophisticated proprietary systems like the GPT-4 Code Interpreter. It significantly enhances code generation capabilities by integrating execution and iterative refinement functionalities.</a></p>
<p><a href="https://github.com/Ikaros-521/AI-Vtuber">Ikaros-521/AI-Vtuber: AI Vtuberæ˜¯ä¸€ä¸ªç”± ã€ChatterBot/ChatGPT/claude/langchain/chatglm/text-gen-webui/é—»è¾¾/åƒé—®/kimi/ollamaã€‘ é©±åŠ¨çš„è™šæ‹Ÿä¸»æ’­ã€Live2D/UE/xunirenã€‘ï¼Œå¯ä»¥åœ¨ ã€Bilibili/æŠ–éŸ³/å¿«æ‰‹/å¾®ä¿¡è§†é¢‘å·/æ‹¼å¤šå¤š/æ–—é±¼/YouTube/twitch/TikTokã€‘ ç›´æ’­ä¸­ä¸è§‚ä¼—å®æ—¶äº’åŠ¨ æˆ– ç›´æ¥åœ¨æœ¬åœ°è¿›è¡ŒèŠå¤©ã€‚å®ƒä½¿ç”¨TTSæŠ€æœ¯ã€edge-tts/VITS/elevenlabs/bark/bert-vits2/ç¿å£°ã€‘ç”Ÿæˆå›ç­”å¹¶å¯ä»¥é€‰æ‹©ã€so-vits-svc/DDSP-SVCã€‘å˜å£°ï¼›æŒ‡ä»¤ååŒSDç”»å›¾ã€‚</a></p>
<p><a href="https://github.com/3b1b/manim">GitHub - 3b1b/manim: Animation engine for explanatory math videos</a></p>
<p><a href="https://github.com/manimCommunity/manim">GitHub - ManimCommunity/manim: A community-maintained Python framework for creating mathematical animations.</a></p>
<p><a href="https://github.com/KindXiaoming/pykan">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a></p>
<p><a href="https://github.com/PeterH0323/Streamer-Sales">GitHub - PeterH0323/Streamer-Sales: Streamer-Sales é”€å†  â€”â€” å–è´§ä¸»æ’­å¤§æ¨¡å‹ï¼Œä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®ç»™å®šçš„å•†å“ç‰¹ç‚¹å¯¹å•†å“è¿›è¡Œè§£è¯´å¹¶æ¿€å‘ç”¨æˆ·çš„è´­ä¹°æ„æ„¿çš„å–è´§ä¸»æ’­æ¨¡å‹</a></p>
<p><a href="https://github.com/FujiwaraChoki/MoneyPrinter">FujiwaraChoki/MoneyPrinter: Automate Creation of YouTube Shorts using MoviePy.</a></p>
<p><a href="https://github.com/princeton-nlp/swe-agent">princeton-nlp/SWE-agent: SWE-agent takes a GitHub issue and tries to automatically fix it, using GPT-4. It solves 12.29% of bugs in the SWE-bench evaluation set (comparable to Devin) and take just 1.5 minutes to run (7x faster than Devin).</a></p>
<p><a href="https://github.com/harry0703/MoneyPrinterTurbo">harry0703/MoneyPrinterTurbo: åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.</a></p>
<p><a href="https://github.com/idootop/mi-gpt">idootop/mi-gpt: ğŸ  å°†å°çˆ±éŸ³ç®±æ¥å…¥ ChatGPT å’Œè±†åŒ…ï¼Œæ”¹é€ æˆä½ çš„ä¸“å±è¯­éŸ³åŠ©æ‰‹ã€‚</a></p>
<p><a href="https://github.com/wan-h/awesome-digital-human-live2d">wan-h/awesome-digital-human-live2d: Awesome Digital Human</a></p>
<p><a href="https://github.com/openai/swarm">openai/swarm: Educational framework exploring ergonomic, lightweight multi-agent orchestration. Managed by OpenAI Solution team.</a></p>
<p><a href="https://github.com/meta-llama/llama-recipes">meta-llama/llama-recipes: Scripts for fine-tuning Meta Llama with composable FSDP &amp; PEFT methods to cover single/multi-node GPUs. Supports default &amp; custom datasets for applications such as summarization and Q&amp;A. Supporting a number of candid inference solutions such as HF TGI, VLLM for local or cloud deployment. Demo apps to showcase Meta Llama for WhatsApp &amp; Messenger.</a></p>
<p><a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM">HqWu-HITCS/Awesome-Chinese-LLM: æ•´ç†å¼€æºçš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼Œä»¥è§„æ¨¡è¾ƒå°ã€å¯ç§æœ‰åŒ–éƒ¨ç½²ã€è®­ç»ƒæˆæœ¬è¾ƒä½çš„æ¨¡å‹ä¸ºä¸»ï¼ŒåŒ…æ‹¬åº•åº§æ¨¡å‹ï¼Œå‚ç›´é¢†åŸŸå¾®è°ƒåŠåº”ç”¨ï¼Œæ•°æ®é›†ä¸æ•™ç¨‹ç­‰ã€‚</a></p>
<p><a href="https://github.com/Hannibal046/Awesome-LLM">Hannibal046/Awesome-LLM: Awesome-LLM: a curated list of Large Language Model</a></p>
<p><a href="https://github.com/excalidraw/excalidraw">excalidraw/excalidraw: Virtual whiteboard for sketching hand-drawn like diagrams</a></p>
<p><a href="https://github.com/meltylabs/melty">meltylabs/melty: Chat first code editor. To download the packaged app:</a></p>
<p><a href="https://github.com/gpt-omni/mini-omni2">gpt-omni/mini-omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilitiesã€‚</a></p>

            <footer class="footline">
            </footer>
          </article>
        </div>
      </main>
    </div>
    <aside id="sidebar" class="default-animation">
      <div id="header-wrapper" class="default-animation">
        <div id="header" class="default-animation">
          <a id="logo" href="/">
            <img width="280px" src="/hb.png"></img>
            <br><font face="serif" size="5" color="#FF0000"><b>æ˜ æ–‡ è§† ç•Œ</b></font>
          </a>
        </div>
        <div class="searchbox default-animation">
          <label for="search-by"><i class="fas fa-search"></i></label>
          <input data-search-input id="search-by" type="search" placeholder="æœç´¢...">
          <span data-search-clear=""><i class="fas fa-times"></i></span>
        </div>
        <script>
          var contentLangs=['zh'];
        </script>
        <script src="/js/auto-complete.js?1734882723" defer></script>
        <script src="/js/lunr.min.js?1734882723" defer></script>
        <script src="/js/lunr.stemmer.support.min.js?1734882723" defer></script>
        <script src="/js/lunr.multi.min.js?1734882723" defer></script>
        <script src="/js/lunr.zh.min.js?1734882723" defer></script>
        <script src="/js/search.js?1734882723" defer></script>
      </div>
      <div id="content-wrapper" class="highlightable">
        <ul class="topics">
          <li data-nav-id="/04_prompt-engineering/" title="&lt;&lt;è‡ªç„¶è¯­è¨€æç¤ºå·¥ç¨‹å…¥é—¨&gt;&gt;" class="dd-item"><input type="checkbox" id="section-e5b35d3b038fb5ec64923f071f9232c3" class="toggle"/><label for="section-e5b35d3b038fb5ec64923f071f9232c3" ></label><a href="/04_prompt-engineering/">&lt;&lt;è‡ªç„¶è¯­è¨€æç¤ºå·¥ç¨‹å…¥é—¨&gt;&gt;</a><ul>
          <li data-nav-id="/04_prompt-engineering/01-prompt-engineering/" title="1. å¼€èƒƒç‚¹å¿ƒ, ä»å‡ ä¸ªæ¡ˆä¾‹å¼€å§‹" class="dd-item"><a href="/04_prompt-engineering/01-prompt-engineering/">1. å¼€èƒƒç‚¹å¿ƒ, ä»å‡ ä¸ªæ¡ˆä¾‹å¼€å§‹</a></li>
          <li data-nav-id="/04_prompt-engineering/02-prompt-engineering/" title="2. æç¤ºå·¥ç¨‹æ—¥å¸¸ä½¿ç”¨æŠ€å·§" class="dd-item"><input type="checkbox" id="section-85b0cf03ed8d259501be9914547856fd" class="toggle"/><label for="section-85b0cf03ed8d259501be9914547856fd" ></label><a href="/04_prompt-engineering/02-prompt-engineering/">2. æç¤ºå·¥ç¨‹æ—¥å¸¸ä½¿ç”¨æŠ€å·§</a><ul>
          <li data-nav-id="/04_prompt-engineering/02-prompt-engineering/daily-skills01/" title="2.1. çƒ­èº«, ç†Ÿæ‚‰åŸºæœ¬æ¦‚å¿µå’Œç¼–å†™è¦ç‚¹" class="dd-item"><a href="/04_prompt-engineering/02-prompt-engineering/daily-skills01/">2.1. çƒ­èº«, ç†Ÿæ‚‰åŸºæœ¬æ¦‚å¿µå’Œç¼–å†™è¦ç‚¹</a></li>
          <li data-nav-id="/04_prompt-engineering/02-prompt-engineering/daily-skills02/" title="2.2. é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤º" class="dd-item"><a href="/04_prompt-engineering/02-prompt-engineering/daily-skills02/">2.2. é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤º</a></li>
          <li data-nav-id="/04_prompt-engineering/02-prompt-engineering/daily-skills03/" title="2.3. æ€ç»´é“¾ä¸å·¦å³äº’æ" class="dd-item"><a href="/04_prompt-engineering/02-prompt-engineering/daily-skills03/">2.3. æ€ç»´é“¾ä¸å·¦å³äº’æ</a></li>
          <li data-nav-id="/04_prompt-engineering/02-prompt-engineering/daily-skills04/" title="2.4. è§’è‰²æ‰®æ¼”æç¤º" class="dd-item"><a href="/04_prompt-engineering/02-prompt-engineering/daily-skills04/">2.4. è§’è‰²æ‰®æ¼”æç¤º</a></li>
          <li data-nav-id="/04_prompt-engineering/02-prompt-engineering/daily-skills05/" title="2.5. å°†æç¤ºæ‹†åˆ†ä¸ºæç¤ºé“¾" class="dd-item"><a href="/04_prompt-engineering/02-prompt-engineering/daily-skills05/">2.5. å°†æç¤ºæ‹†åˆ†ä¸ºæç¤ºé“¾</a></li>
          <li data-nav-id="/04_prompt-engineering/02-prompt-engineering/daily-skills06/" title="2.6. æ•…äº‹æç¤º" class="dd-item"><a href="/04_prompt-engineering/02-prompt-engineering/daily-skills06/">2.6. æ•…äº‹æç¤º</a></li></ul></li>
          <li data-nav-id="/04_prompt-engineering/03-prompt-engineering/" title="3. æç¤ºå·¥ç¨‹æ—¥å¸¸ä½¿ç”¨æ¡ˆä¾‹å¤§å…¨" class="dd-item"><a href="/04_prompt-engineering/03-prompt-engineering/">3. æç¤ºå·¥ç¨‹æ—¥å¸¸ä½¿ç”¨æ¡ˆä¾‹å¤§å…¨</a></li>
          <li data-nav-id="/04_prompt-engineering/04-prompt-engineering/" title="4. é«˜çº§æç¤ºå·¥ç¨‹æ¦‚è¿°" class="dd-item"><a href="/04_prompt-engineering/04-prompt-engineering/">4. é«˜çº§æç¤ºå·¥ç¨‹æ¦‚è¿°</a></li></ul></li>
          <li data-nav-id="/01_aigc_object/" title="ACGC é¡¹ç›®èµ„æºé›†" class="dd-item parent"><input type="checkbox" id="section-33ed1d2a346ed20d642e8be7edffce65" class="toggle" checked/><label for="section-33ed1d2a346ed20d642e8be7edffce65" ></label><a href="/01_aigc_object/">ACGC é¡¹ç›®èµ„æºé›†</a><ul>
          <li data-nav-id="/01_aigc_object/01_object/" title="é¡¹ç›®çš„å®˜æ–¹ç½‘ç«™" class="dd-item"><a href="/01_aigc_object/01_object/">é¡¹ç›®çš„å®˜æ–¹ç½‘ç«™</a></li>
          <li data-nav-id="/01_aigc_object/02_github/" title="é¡¹ç›®çš„ GitHub ä¸»é¡µ" class="dd-item active"><a href="/01_aigc_object/02_github/">é¡¹ç›®çš„ GitHub ä¸»é¡µ</a></li>
          <li data-nav-id="/01_aigc_object/03_hugging/" title="Hugging Face Space" class="dd-item"><a href="/01_aigc_object/03_hugging/">Hugging Face Space</a></li>
          <li data-nav-id="/01_aigc_object/04_web/" title="AIGC ç›¸å…³ç½‘ç«™" class="dd-item"><a href="/01_aigc_object/04_web/">AIGC ç›¸å…³ç½‘ç«™</a></li>
          <li data-nav-id="/01_aigc_object/05_doc/" title="ç›¸å…³æ–‡æ¡£èµ„æ–™" class="dd-item"><a href="/01_aigc_object/05_doc/">ç›¸å…³æ–‡æ¡£èµ„æ–™</a></li>
          <li data-nav-id="/01_aigc_object/06_wait/" title="ç­‰å¾…ä¸­çš„é¡¹ç›®" class="dd-item"><a href="/01_aigc_object/06_wait/">ç­‰å¾…ä¸­çš„é¡¹ç›®</a></li></ul></li>
          <li data-nav-id="/02_comfyui/" title="ComfyUI èµ„æº, æ•™ç¨‹" class="dd-item"><input type="checkbox" id="section-b139ed5ac8d7ecf0bdd93cad7cc2ee6f" class="toggle"/><label for="section-b139ed5ac8d7ecf0bdd93cad7cc2ee6f" ></label><a href="/02_comfyui/">ComfyUI èµ„æº, æ•™ç¨‹</a><ul>
          <li data-nav-id="/02_comfyui/01_interface/" title="ç•Œé¢ä»‹ç»" class="dd-item"><a href="/02_comfyui/01_interface/">ç•Œé¢ä»‹ç»</a></li>
          <li data-nav-id="/02_comfyui/02_basic/" title="åŸºæœ¬æ¦‚å¿µ" class="dd-item"><a href="/02_comfyui/02_basic/">åŸºæœ¬æ¦‚å¿µ</a></li>
          <li data-nav-id="/02_comfyui/03_built-in/" title="å†…ç½®èŠ‚ç‚¹" class="dd-item"><input type="checkbox" id="section-a56a67957678d144c2719c346d47faa5" class="toggle"/><label for="section-a56a67957678d144c2719c346d47faa5" ></label><a href="/02_comfyui/03_built-in/">å†…ç½®èŠ‚ç‚¹</a><ul>
          <li data-nav-id="/02_comfyui/03_built-in/01-built-in/" title="å†…ç½®èŠ‚ç‚¹ 1" class="dd-item"><a href="/02_comfyui/03_built-in/01-built-in/">å†…ç½®èŠ‚ç‚¹ 1</a></li>
          <li data-nav-id="/02_comfyui/03_built-in/02-built-in/" title="å†…ç½®èŠ‚ç‚¹ 2" class="dd-item"><a href="/02_comfyui/03_built-in/02-built-in/">å†…ç½®èŠ‚ç‚¹ 2</a></li>
          <li data-nav-id="/02_comfyui/03_built-in/03-built-in/" title="å†…ç½®èŠ‚ç‚¹ 3" class="dd-item"><a href="/02_comfyui/03_built-in/03-built-in/">å†…ç½®èŠ‚ç‚¹ 3</a></li></ul></li>
          <li data-nav-id="/02_comfyui/04_common-nodes/" title="å¸¸ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹" class="dd-item"><a href="/02_comfyui/04_common-nodes/">å¸¸ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹</a></li></ul></li>
          <li data-nav-id="/03_prompts/" title="æç¤ºå·¥ç¨‹èµ„æº, æ¡ˆä¾‹é›†" class="dd-item"><input type="checkbox" id="section-019abe92d153661cf1e3ed2ecad53b65" class="toggle"/><label for="section-019abe92d153661cf1e3ed2ecad53b65" ></label><a href="/03_prompts/">æç¤ºå·¥ç¨‹èµ„æº, æ¡ˆä¾‹é›†</a><ul>
          <li data-nav-id="/03_prompts/01-prompt-sources/" title="æç¤ºå·¥ç¨‹èµ„æºæ•´ç†" class="dd-item"><a href="/03_prompts/01-prompt-sources/">æç¤ºå·¥ç¨‹èµ„æºæ•´ç†</a></li></ul></li>
        </ul>
        <div id="shortcuts">
          <div class="nav-title"></div>
          <ul>
            <li><a class="padding" href="/about">å…³æ³¨æˆ‘</a></li>
          </ul>
        </div>
        <div class="footermargin footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showFooter"></div>
        <hr class="default-animation footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showFooter"/>
        <div id="prefooter" class="footerLangSwitch footerVariantSwitch footerVisitedLinks">
          <ul>
            <li id="select-language-container" class="footerLangSwitch">
              <a class="padding select-container">
                <i class="fas fa-language fa-fw"></i>
                <span>&nbsp;</span>
                <div class="select-style">
                  <select id="select-language" onchange="location = baseUri + this.value;">
                  </select>
                </div>
                <div class="select-clear"></div>
              </a>
            </li>
            <li id="select-variant-container" class="footerVariantSwitch">
              <a class="padding select-container">
                <i class="fas fa-paint-brush fa-fw"></i>
                <span>&nbsp;</span>
                <div class="select-style">
                  <select id="select-variant" onchange="window.variants && variants.changeVariant( this.value );">
                    <option id="black" value="black" selected>Black</option>
                  </select>
                </div>
                <div class="select-clear"></div>
              </a>
              <script>window.variants && variants.markSelectedVariant();</script>
            </li>
            <li class="footerVisitedLinks"><a class="padding" onclick="clearHistory();"><i class="fas fa-history fa-fw"></i> æ¸…ç†å†å²è®°å½•</a></li>
          </ul>
        </div>
        <div id="footer" class="footerFooter showFooter">      <p>ç²¤ICPå¤‡ <a href="https://beian.miit.gov.cn/#/Integrated/index">20041225å·</a></p>
      <p></a><img src="/beian.png" >ç²¤å…¬ç½‘å®‰å¤‡<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44030702003010"> 44030702003010å·</a>
      <p>Â©2022 <a href="https://jupyter.fun/">ğŸ Jupyter.fun</a></p>
        </div>
      </div>
    </aside>
    <script src="/js/clipboard.min.js?1734882723" defer></script>
    <script src="/js/perfect-scrollbar.min.js?1734882723" defer></script>
    <script src="/js/featherlight.min.js?1734882723" defer></script>
    <script>
      function useMathJax( config ){
        if( !Object.assign ){
          
          return;
        }
        window.MathJax = Object.assign( window.MathJax || {}, {
          loader: {
            load: ['[tex]/mhchem']
          },
          startup: {
            elements: [
              '.math'
            ]
          },
          tex: {
            inlineMath: [
              ['$', '$'], 
              ['\\(', '\\)']
            ]
          },
          options: {
            enableMenu: false 
          }
        }, config );
      }
      useMathJax( JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/js/jquery.svg.pan.zoom.js?1734882723" defer></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js" defer></script>
    <script>
      window.themeUseMermaid = JSON.parse("{ \"theme\": \"default\" }");
    </script>
    <script src="https://unpkg.com/rapidoc/dist/rapidoc-min.js" defer></script>
    <script>
      window.themeUseSwagger = JSON.parse("{ \"theme\": \"light\" }");
    </script>
    <script src="/js/theme.js?1734882723" defer></script>
  </body>
</html>
